# http://docs.firecrawl.dev llms-full.txt

## Firecrawl API Documentation
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Quickstart

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

![Hero Light](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/hero.png)

## [‚Äã](https://docs.firecrawl.dev/introduction\#welcome-to-firecrawl)  Welcome to Firecrawl

[Firecrawl](https://firecrawl.dev/?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.

## [‚Äã](https://docs.firecrawl.dev/introduction\#how-to-use-it%3F)  How to use it?

We provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you‚Äôd like.

Check out the following resources to get started:

- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)
- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)
- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)
- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)
- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)
- [ ]  Want an SDK or Integration? Let us know by opening an issue.

**Self-host:** To self-host refer to guide [here](https://docs.firecrawl.dev/contributing/self-host).

### [‚Äã](https://docs.firecrawl.dev/introduction\#api-key)  API Key

To use the API, you need to sign up on [Firecrawl](https://firecrawl.dev/) and get an API key.

### [‚Äã](https://docs.firecrawl.dev/introduction\#features)  Features

- [**Scrape**](https://docs.firecrawl.dev/introduction#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](https://docs.firecrawl.dev/introduction#extraction), screenshot, html)
- [**Crawl**](https://docs.firecrawl.dev/introduction#crawling): scrapes all the URLs of a web page and return content in LLM-ready format
- [**Map**](https://docs.firecrawl.dev/features/map): input a website and get all the website urls - extremely fast
- [**Extract**](https://docs.firecrawl.dev/features/extract): get structured data from single page, multiple pages or entire websites with AI.

### [‚Äã](https://docs.firecrawl.dev/introduction\#powerful-capabilities)  Powerful Capabilities

- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata
- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration
- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc‚Ä¶
- **Media parsing**: pdfs, docx, images.
- **Reliability first**: designed to get the data you need - no matter how hard it is.
- **Actions**: click, scroll, input, wait and more before extracting data

You can find all of Firecrawl‚Äôs capabilities and how to use them in our [documentation](https://docs.firecrawl.dev/)

## [‚Äã](https://docs.firecrawl.dev/introduction\#crawling)  Crawling

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

### [‚Äã](https://docs.firecrawl.dev/introduction\#installation)  Installation

Python

Node

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/introduction\#usage)  Usage

Python

Node

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)

```

If you‚Äôre using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.

Copy

```json
{
  "success": true,
  "id": "123-456-789",
  "url": "https://api.firecrawl.dev/v1/crawl/123-456-789"
}

```

### [‚Äã](https://docs.firecrawl.dev/introduction\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Python

Node

Go

Rust

cURL

Copy

```python
crawl_status = app.check_crawl_status("<crawl_id>")
print(crawl_status)

```

#### [‚Äã](https://docs.firecrawl.dev/introduction\#response)  Response

The response will be different depending on the status of the crawl. For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.

Scraping

Completed

Copy

```json
{
  "status": "scraping",
  "total": 36,
  "completed": 10,
  "creditsUsed": 10,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "next": "https://api.firecrawl.dev/v1/crawl/123-456-789?skip=10",
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#scraping)  Scraping\
\
To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\
\
Python\
\
Node\
\
Go\
\
Rust\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#response-2)  Response\
\
SDKs will return the data object directly. cURL will return the payload exactly as shown below.\
\
Copy\
\
```json\
{\
  "success": true,\
  "data" : {\
    "markdown": "Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...",\
    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "https://firecrawl.dev",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#extraction)  Extraction\
\
With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\
\
v1 is only supported on node, python and cURL at this time.\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
from pydantic import BaseModel, Field\
\
# Initialize the FirecrawlApp with your API key\
app = FirecrawlApp(api_key='your_api_key')\
\
class ExtractSchema(BaseModel):\
    company_mission: str\
    supports_sso: bool\
    is_open_source: bool\
    is_in_yc: bool\
\
data = app.scrape_url('https://docs.firecrawl.dev/', {\
    'formats': ['json'],\
    'jsonOptions': {\
        'schema': ExtractSchema.model_json_schema(),\
    }\
})\
print(data["json"])\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
        "supports_sso": true,\
        "is_open_source": false,\
        "is_in_yc": true\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#extracting-without-schema-new)  Extracting without schema (New)\
\
You can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\
\
cURL\
\
Copy\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/scrape \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev/",\
      "formats": ["json"],\
      "jsonOptions": {\
        "prompt": "Extract the company mission from the page."\
      }\
    }'\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#extraction-v0)  Extraction (v0)\
\
Python\
\
JavaScript\
\
Go\
\
Rust\
\
cURL\
\
Copy\
\
```python\
\
app = FirecrawlApp(version="v0")\
\
class ArticleSchema(BaseModel):\
    title: str\
    points: int\
    by: str\
    commentsURL: str\
\
class TopArticlesSchema(BaseModel):\
top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")\
\
data = app.scrape_url('https://news.ycombinator.com', {\
'extractorOptions': {\
'extractionSchema': TopArticlesSchema.model_json_schema(),\
'mode': 'llm-extraction'\
},\
'pageOptions':{\
'onlyMainContent': True\
}\
})\
print(data["llm_extraction"])\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#interacting-with-the-page-with-actions)  Interacting with the page with Actions\
\
Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\
\
Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\
\
It is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#example)  Example\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev',\
    params={\
        'formats': ['markdown', 'html'],\
        'actions': [\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "click", "selector": "textarea[title=\"Search\"]"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "write", "text": "firecrawl"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "press", "key": "ENTER"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "click", "selector": "h3"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "scrape"},\
            {"type": "screenshot"}\
        ]\
    }\
)\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#output)  Output\
\
JSON\
\
Copy\
\
```json\
{\
  "success": true,\
  "data": {\
    "markdown": "Our first Launch Week is over! [See the recap üöÄ](blog/firecrawl-launch-week-1-recap)...",\
    "actions": {\
      "screenshots": [\
        "https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png"\
      ],\
      "scrapes": [\
        {\
          "url": "https://www.firecrawl.dev/",\
          "html": "<html><body><h1>Firecrawl</h1></body></html>"\
        }\
      ]\
    },\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "http://google.com",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#open-source-vs-cloud)  Open Source vs Cloud\
\
Firecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE).\
\
To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\
\
Firecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev/) and offers a range of features that are not available in the open source version:\
\
![Firecrawl Cloud vs Open Source](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/open-source-cloud.png)\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#contributing)  Contributing\
\
We love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/introduction)\
\
[Launch Week II (New)](https://docs.firecrawl.dev/launch-week)\
\
On this page\
\
- [Welcome to Firecrawl](https://docs.firecrawl.dev/introduction#welcome-to-firecrawl)\
- [How to use it?](https://docs.firecrawl.dev/introduction#how-to-use-it%3F)\
- [API Key](https://docs.firecrawl.dev/introduction#api-key)\
- [Features](https://docs.firecrawl.dev/introduction#features)\
- [Powerful Capabilities](https://docs.firecrawl.dev/introduction#powerful-capabilities)\
- [Crawling](https://docs.firecrawl.dev/introduction#crawling)\
- [Installation](https://docs.firecrawl.dev/introduction#installation)\
- [Usage](https://docs.firecrawl.dev/introduction#usage)\
- [Check Crawl Job](https://docs.firecrawl.dev/introduction#check-crawl-job)\
- [Response](https://docs.firecrawl.dev/introduction#response)\
- [Scraping](https://docs.firecrawl.dev/introduction#scraping)\
- [Response](https://docs.firecrawl.dev/introduction#response-2)\
- [Extraction](https://docs.firecrawl.dev/introduction#extraction)\
- [Extracting without schema (New)](https://docs.firecrawl.dev/introduction#extracting-without-schema-new)\
- [Extraction (v0)](https://docs.firecrawl.dev/introduction#extraction-v0)\
- [Interacting with the page with Actions](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions)\
- [Example](https://docs.firecrawl.dev/introduction#example)\
- [Output](https://docs.firecrawl.dev/introduction#output)\
- [Open Source vs Cloud](https://docs.firecrawl.dev/introduction#open-source-vs-cloud)\
- [Contributing](https://docs.firecrawl.dev/introduction#contributing)\
\
![Hero Light](https://docs.firecrawl.dev/introduction)\
\
![Firecrawl Cloud vs Open Source](https://docs.firecrawl.dev/introduction)

## Web Data Integrations
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Integrations

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

[![Firecrawl Document Loader](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/langchain.png)\\
\\
**Langchain** \\
\\
Check out Firecrawl Document Loader](https://docs.firecrawl.dev/integrations/langchain) [![Firecrawl Reader](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/llamaindex.jpeg)\\
\\
**LlamaIndex** \\
\\
Check out Firecrawl Reader](https://docs.firecrawl.dev/integrations/llamaindex) [![Dify](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/dify.jpeg)\\
\\
**Dify** \\
\\
Extract structured data from web pages](https://docs.firecrawl.dev/integrations/dify) [![Flowise](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/flowise.png)\\
\\
**Flowise** \\
\\
Sync data directly from websites](https://docs.firecrawl.dev/integrations/flowise) [![Crew AI](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/crewai.png)\\
\\
**CrewAI** \\
\\
Coordinate AI agents for web scraping tasks](https://docs.firecrawl.dev/integrations/crewai) [![Langflow](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/langflow.webp)\\
\\
**Langflow** \\
\\
Design visual web data pipelines](https://docs.firecrawl.dev/integrations/langflow) [![CamelAI](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/camelai.jpg)\\
\\
**Camel AI** \\
\\
Design visual web data pipelines](https://docs.firecrawl.dev/integrations/camelai) [![SourceSync.ai](https://raw.githubusercontent.com/hellofirecrawl/docs/main/images/integrations/sourcesyncai.png)\\
\\
**SourceSync.ai** \\
\\
Build RAG applications with web data](https://docs.firecrawl.dev/integrations/sourcesyncai)

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations)

[Rate Limits](https://docs.firecrawl.dev/rate-limits) [Advanced Scraping Guide](https://docs.firecrawl.dev/advanced-scraping-guide)

![Firecrawl Document Loader](https://docs.firecrawl.dev/integrations)

![Firecrawl Reader](https://docs.firecrawl.dev/integrations)

![Dify](https://docs.firecrawl.dev/integrations)

![Flowise](https://docs.firecrawl.dev/integrations)

![Crew AI](https://docs.firecrawl.dev/integrations)

![CamelAI](https://docs.firecrawl.dev/integrations)

![SourceSync.ai](https://docs.firecrawl.dev/integrations)

## Firecrawl API Documentation
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Quickstart

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

![Hero Light](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/hero.png)

## [‚Äã](https://docs.firecrawl.dev/introduction\#welcome-to-firecrawl)  Welcome to Firecrawl

[Firecrawl](https://firecrawl.dev/?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.

## [‚Äã](https://docs.firecrawl.dev/introduction\#how-to-use-it%3F)  How to use it?

We provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you‚Äôd like.

Check out the following resources to get started:

- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)
- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)
- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)
- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)
- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)
- [ ]  Want an SDK or Integration? Let us know by opening an issue.

**Self-host:** To self-host refer to guide [here](https://docs.firecrawl.dev/contributing/self-host).

### [‚Äã](https://docs.firecrawl.dev/introduction\#api-key)  API Key

To use the API, you need to sign up on [Firecrawl](https://firecrawl.dev/) and get an API key.

### [‚Äã](https://docs.firecrawl.dev/introduction\#features)  Features

- [**Scrape**](https://docs.firecrawl.dev/introduction#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](https://docs.firecrawl.dev/introduction#extraction), screenshot, html)
- [**Crawl**](https://docs.firecrawl.dev/introduction#crawling): scrapes all the URLs of a web page and return content in LLM-ready format
- [**Map**](https://docs.firecrawl.dev/features/map): input a website and get all the website urls - extremely fast
- [**Extract**](https://docs.firecrawl.dev/features/extract): get structured data from single page, multiple pages or entire websites with AI.

### [‚Äã](https://docs.firecrawl.dev/introduction\#powerful-capabilities)  Powerful Capabilities

- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata
- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration
- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc‚Ä¶
- **Media parsing**: pdfs, docx, images.
- **Reliability first**: designed to get the data you need - no matter how hard it is.
- **Actions**: click, scroll, input, wait and more before extracting data

You can find all of Firecrawl‚Äôs capabilities and how to use them in our [documentation](https://docs.firecrawl.dev/)

## [‚Äã](https://docs.firecrawl.dev/introduction\#crawling)  Crawling

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

### [‚Äã](https://docs.firecrawl.dev/introduction\#installation)  Installation

Python

Node

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/introduction\#usage)  Usage

Python

Node

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)

```

If you‚Äôre using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.

Copy

```json
{
  "success": true,
  "id": "123-456-789",
  "url": "https://api.firecrawl.dev/v1/crawl/123-456-789"
}

```

### [‚Äã](https://docs.firecrawl.dev/introduction\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Python

Node

Go

Rust

cURL

Copy

```python
crawl_status = app.check_crawl_status("<crawl_id>")
print(crawl_status)

```

#### [‚Äã](https://docs.firecrawl.dev/introduction\#response)  Response

The response will be different depending on the status of the crawl. For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.

Scraping

Completed

Copy

```json
{
  "status": "scraping",
  "total": 36,
  "completed": 10,
  "creditsUsed": 10,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "next": "https://api.firecrawl.dev/v1/crawl/123-456-789?skip=10",
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#scraping)  Scraping\
\
To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\
\
Python\
\
Node\
\
Go\
\
Rust\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#response-2)  Response\
\
SDKs will return the data object directly. cURL will return the payload exactly as shown below.\
\
Copy\
\
```json\
{\
  "success": true,\
  "data" : {\
    "markdown": "Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...",\
    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "https://firecrawl.dev",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#extraction)  Extraction\
\
With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\
\
v1 is only supported on node, python and cURL at this time.\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
from pydantic import BaseModel, Field\
\
# Initialize the FirecrawlApp with your API key\
app = FirecrawlApp(api_key='your_api_key')\
\
class ExtractSchema(BaseModel):\
    company_mission: str\
    supports_sso: bool\
    is_open_source: bool\
    is_in_yc: bool\
\
data = app.scrape_url('https://docs.firecrawl.dev/', {\
    'formats': ['json'],\
    'jsonOptions': {\
        'schema': ExtractSchema.model_json_schema(),\
    }\
})\
print(data["json"])\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
        "supports_sso": true,\
        "is_open_source": false,\
        "is_in_yc": true\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#extracting-without-schema-new)  Extracting without schema (New)\
\
You can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\
\
cURL\
\
Copy\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/scrape \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev/",\
      "formats": ["json"],\
      "jsonOptions": {\
        "prompt": "Extract the company mission from the page."\
      }\
    }'\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#extraction-v0)  Extraction (v0)\
\
Python\
\
JavaScript\
\
Go\
\
Rust\
\
cURL\
\
Copy\
\
```python\
\
app = FirecrawlApp(version="v0")\
\
class ArticleSchema(BaseModel):\
    title: str\
    points: int\
    by: str\
    commentsURL: str\
\
class TopArticlesSchema(BaseModel):\
top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")\
\
data = app.scrape_url('https://news.ycombinator.com', {\
'extractorOptions': {\
'extractionSchema': TopArticlesSchema.model_json_schema(),\
'mode': 'llm-extraction'\
},\
'pageOptions':{\
'onlyMainContent': True\
}\
})\
print(data["llm_extraction"])\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#interacting-with-the-page-with-actions)  Interacting with the page with Actions\
\
Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\
\
Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\
\
It is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#example)  Example\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev',\
    params={\
        'formats': ['markdown', 'html'],\
        'actions': [\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "click", "selector": "textarea[title=\"Search\"]"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "write", "text": "firecrawl"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "press", "key": "ENTER"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "click", "selector": "h3"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "scrape"},\
            {"type": "screenshot"}\
        ]\
    }\
)\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#output)  Output\
\
JSON\
\
Copy\
\
```json\
{\
  "success": true,\
  "data": {\
    "markdown": "Our first Launch Week is over! [See the recap üöÄ](blog/firecrawl-launch-week-1-recap)...",\
    "actions": {\
      "screenshots": [\
        "https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png"\
      ],\
      "scrapes": [\
        {\
          "url": "https://www.firecrawl.dev/",\
          "html": "<html><body><h1>Firecrawl</h1></body></html>"\
        }\
      ]\
    },\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "http://google.com",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#open-source-vs-cloud)  Open Source vs Cloud\
\
Firecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE).\
\
To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\
\
Firecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev/) and offers a range of features that are not available in the open source version:\
\
![Firecrawl Cloud vs Open Source](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/open-source-cloud.png)\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#contributing)  Contributing\
\
We love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/introduction)\
\
[Launch Week II (New)](https://docs.firecrawl.dev/launch-week)\
\
On this page\
\
- [Welcome to Firecrawl](https://docs.firecrawl.dev/introduction#welcome-to-firecrawl)\
- [How to use it?](https://docs.firecrawl.dev/introduction#how-to-use-it%3F)\
- [API Key](https://docs.firecrawl.dev/introduction#api-key)\
- [Features](https://docs.firecrawl.dev/introduction#features)\
- [Powerful Capabilities](https://docs.firecrawl.dev/introduction#powerful-capabilities)\
- [Crawling](https://docs.firecrawl.dev/introduction#crawling)\
- [Installation](https://docs.firecrawl.dev/introduction#installation)\
- [Usage](https://docs.firecrawl.dev/introduction#usage)\
- [Check Crawl Job](https://docs.firecrawl.dev/introduction#check-crawl-job)\
- [Response](https://docs.firecrawl.dev/introduction#response)\
- [Scraping](https://docs.firecrawl.dev/introduction#scraping)\
- [Response](https://docs.firecrawl.dev/introduction#response-2)\
- [Extraction](https://docs.firecrawl.dev/introduction#extraction)\
- [Extracting without schema (New)](https://docs.firecrawl.dev/introduction#extracting-without-schema-new)\
- [Extraction (v0)](https://docs.firecrawl.dev/introduction#extraction-v0)\
- [Interacting with the page with Actions](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions)\
- [Example](https://docs.firecrawl.dev/introduction#example)\
- [Output](https://docs.firecrawl.dev/introduction#output)\
- [Open Source vs Cloud](https://docs.firecrawl.dev/introduction#open-source-vs-cloud)\
- [Contributing](https://docs.firecrawl.dev/introduction#contributing)\
\
![Hero Light](https://docs.firecrawl.dev/introduction)

## Data Extraction Tool
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Features

Extract

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/features/extract\#introducing-%2Fextract-open-beta)  Introducing `/extract` (Open Beta)

The `/extract` endpoint simplifies collecting structured data from any number of URLs or entire domains. Provide a list of URLs, optionally with wildcards (e.g., `example.com/*`), and a prompt or schema describing the information you want. Firecrawl handles the details of crawling, parsing, and collating large or small datasets.

## [‚Äã](https://docs.firecrawl.dev/features/extract\#using-%2Fextract)  Using `/extract`

You can extract structured data from one or multiple URLs, including wildcards:

- **Single Page**


Example: `https://firecrawl.dev/some-page`
- **Multiple Pages / Full Domain**


Example: `https://firecrawl.dev/*`

When you use `/*`, Firecrawl will automatically crawl and parse all URLs it can discover in that domain, then extract the requested data. This feature is experimental; email [help@firecrawl.dev](mailto:help@firecrawl.dev) if you have issues.

### [‚Äã](https://docs.firecrawl.dev/features/extract\#example-usage)  Example Usage

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp
from pydantic import BaseModel, Field

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key='your_api_key')

class ExtractSchema(BaseModel):
    company_mission: str
    supports_sso: bool
    is_open_source: bool
    is_in_yc: bool

data = app.extract([\
  'https://docs.firecrawl.dev/*',\
  'https://firecrawl.dev/',\
  'https://www.ycombinator.com/companies/'\
], {
    'prompt': 'Extract the company mission, whether it supports SSO, whether it is open source, and whether it is in Y Combinator from the page.',
    'schema': ExtractSchema.model_json_schema(),
})
print(data)

```

**Key Parameters:**

- **urls**: An array of one or more URLs. Supports wildcards ( `/*`) for broader crawling.
- **prompt** (Optional unless no schema): A natural language prompt describing the data you want or specifying how you want that data structured.
- **schema** (Optional unless no prompt): A more rigid structure if you already know the JSON layout.
- **enableWebSearch** (Optional): When `true`, extraction can follow links outside the specified domain.

See [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/extract) for more details.

### [‚Äã](https://docs.firecrawl.dev/features/extract\#response-sdks)  Response (sdks)

JSON

Copy

```json
{
  "success": true,
  "data": {
    "company_mission": "Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.",
    "supports_sso": false,
    "is_open_source": true,
    "is_in_yc": true
  }
}

```

## [‚Äã](https://docs.firecrawl.dev/features/extract\#asynchronous-extraction-%26-status-checking)  Asynchronous Extraction & Status Checking

When you submit an extraction job‚Äîeither directly via the API or through the SDK‚Äôs asynchronous methods‚Äîyou‚Äôll receive a Job ID. You can use this ID to:

- Check Job Status: Send a request to the /extract/ endpoint to see if the job is still running or has finished.
- Automatically Poll (Default SDK Behavior): If you use the default extract method (Python/Node), the SDK automatically polls this endpoint for you and returns the final results once the job completes.
- Manually Poll (Async SDK Methods): If you use the asynchronous methods‚Äîasync\_extract (Python) or asyncExtract (Node)‚Äîthe SDK immediately returns a Job ID that you can track. Use get\_extract\_status (Python) or getExtractStatus (Node) to check the job‚Äôs progress on your own schedule.

This endpoint only works for jobs in progress or recently completed (within 24
hours).

Below are code examples for checking an extraction job‚Äôs status using Python, Node.js, and cURL:

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(
    api_key="fc-YOUR_API_KEY"
)

# Start an extraction job first
extract_job = app.async_extract([\
    'https://docs.firecrawl.dev/*',\
    'https://firecrawl.dev/'\
], prompt="Extract the company mission and features from these pages.")

# Get the status of the extraction job
job_status = app.get_extract_status(extract_job.job_id)

print(job_status)
# Example output:
# {
#     "status": "completed",
#     "progress": 100,
#     "results": [{\
#         "url": "https://docs.firecrawl.dev",\
#         "data": { ... }\
#     }]
# }

```

### [‚Äã](https://docs.firecrawl.dev/features/extract\#possible-states)  Possible States

- **completed**: The extraction finished successfully.
- **pending**: Firecrawl is still processing your request.
- **failed**: An error occurred; data was not fully extracted.
- **cancelled**: The job was cancelled by the user.

#### [‚Äã](https://docs.firecrawl.dev/features/extract\#pending-example)  Pending Example

JSON

Copy

```json
{
  "success": true,
  "data": [],
  "status": "processing",
  "expiresAt": "2025-01-08T20:58:12.000Z"
}

```

#### [‚Äã](https://docs.firecrawl.dev/features/extract\#completed-example)  Completed Example

JSON

Copy

```json
{
  "success": true,
  "data": {
      "company_mission": "Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.",
      "supports_sso": false,
      "is_open_source": true,
      "is_in_yc": true
    },
  "status": "completed",
  "expiresAt": "2025-01-08T20:58:12.000Z"
}

```

## [‚Äã](https://docs.firecrawl.dev/features/extract\#extracting-without-a-schema)  Extracting without a Schema

If you prefer not to define a strict structure, you can simply provide a `prompt`. The underlying model will choose a structure for you, which can be useful for more exploratory or flexible requests.

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key='your_api_key')

data = app.extract([\
  'https://docs.firecrawl.dev/',\
  'https://firecrawl.dev/'\
], {
  'prompt': "Extract Firecrawl's mission from the page."
})
print(data)

```

JSON

Copy

```json
{
  "success": true,
  "data": {
    "company_mission": "Turn websites into LLM-ready data. Power your AI apps with clean data crawled from any website."
  }
}

```

## [‚Äã](https://docs.firecrawl.dev/features/extract\#improving-results-with-web-search)  Improving Results with Web Search

Setting `enableWebSearch = true` in your request will expand the crawl beyond the provided URL set. This can capture supporting or related information from linked pages.

Here‚Äôs an example that extracts information about dash cams, enriching the results with data from related pages:

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

# Initialize the FirecrawlApp with your API key

app = FirecrawlApp(api_key='your_api_key')

data = app.extract([\
'https://nextbase.com/dash-cams/622gw-dash-cam'\
], {
'prompt': "Extract details about the best dash cams including prices, features, pros/cons and reviews.",
'enableWebSearch': True # Enable web search for better context
})
print(data)

```

### [‚Äã](https://docs.firecrawl.dev/features/extract\#example-response-with-web-search)  Example Response with Web Search

JSON

Copy

```json
{
  "success": true,
  "data": {
    "dash_cams": [\
      {\
        "name": "Nextbase 622GW",\
        "price": "$399.99",\
        "features": [\
          "4K video recording",\
          "Image stabilization",\
          "Alexa built-in",\
          "What3Words integration"\
        ],\
        /* Information below enriched with other websites like\
        https://www.techradar.com/best/best-dash-cam found\
        via enableWebSearch parameter */\
        "pros": [\
          "Excellent video quality",\
          "Great night vision",\
          "Built-in GPS"\
        ],\
        "cons": ["Premium price point", "App can be finicky"]\
      }\
    ],
  }

```

The response includes additional context gathered from related pages, providing more comprehensive and accurate information.

## [‚Äã](https://docs.firecrawl.dev/features/extract\#known-limitations-beta)  Known Limitations (Beta)

1. **Large-Scale Site Coverage**


Full coverage of massive sites (e.g., ‚Äúall products on Amazon‚Äù) in a single request is not yet supported.

2. **Complex Logical Queries**


Requests like ‚Äúfind every post from 2025‚Äù may not reliably return all expected data. More advanced query capabilities are in progress.

3. **Occasional Inconsistencies**


Results might differ across runs, particularly for very large or dynamic sites. Usually it captures core details, but some variation is possible.

4. **Beta State**


Since `/extract` is still in Beta, features and performance will continue to evolve. We welcome bug reports and feedback to help us improve.


## [‚Äã](https://docs.firecrawl.dev/features/extract\#billing-and-usage-tracking)  Billing and Usage Tracking

You can check our the pricing for /extract on the [Extract landing page pricing page](https://www.firecrawl.dev/extract#pricing) and monitor usage via the [Extract page on the dashboard](https://www.firecrawl.dev/app/extract).

Have feedback or need help? Email [help@firecrawl.dev](mailto:help@firecrawl.dev).

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/extract)

[Map](https://docs.firecrawl.dev/features/map) [Langchain](https://docs.firecrawl.dev/integrations/langchain)

On this page

- [Introducing /extract (Open Beta)](https://docs.firecrawl.dev/features/extract#introducing-%2Fextract-open-beta)
- [Using /extract](https://docs.firecrawl.dev/features/extract#using-%2Fextract)
- [Example Usage](https://docs.firecrawl.dev/features/extract#example-usage)
- [Response (sdks)](https://docs.firecrawl.dev/features/extract#response-sdks)
- [Asynchronous Extraction & Status Checking](https://docs.firecrawl.dev/features/extract#asynchronous-extraction-%26-status-checking)
- [Possible States](https://docs.firecrawl.dev/features/extract#possible-states)
- [Pending Example](https://docs.firecrawl.dev/features/extract#pending-example)
- [Completed Example](https://docs.firecrawl.dev/features/extract#completed-example)
- [Extracting without a Schema](https://docs.firecrawl.dev/features/extract#extracting-without-a-schema)
- [Improving Results with Web Search](https://docs.firecrawl.dev/features/extract#improving-results-with-web-search)
- [Example Response with Web Search](https://docs.firecrawl.dev/features/extract#example-response-with-web-search)
- [Known Limitations (Beta)](https://docs.firecrawl.dev/features/extract#known-limitations-beta)
- [Billing and Usage Tracking](https://docs.firecrawl.dev/features/extract#billing-and-usage-tracking)

## Firecrawl and CrewAI Integration
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

CrewAI

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#using-firecrawl-with-crewai)  Using Firecrawl with CrewAI

Firecrawl is integrated with [CrewAI, the framework for orchestrating AI agents](https://www.crewai.com/). This page introduces all of the Firecrawl tools added to the framework.

### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#installing-firecrawl-tools-inside-of-crewai)  Installing Firecrawl Tools inside of CrewAI

- Get an API key from your [firecrawl.dev dashboard](https://firecrawl.dev/) and set it in environment variables ( `FIRECRAWL_API_KEY`).
- Install the [Firecrawl SDK](https://github.com/mendableai/firecrawl) along with `crewai[tools]` package:

Copy

```
pip install firecrawl-py 'crewai[tools]'

```

## [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#tools)  Tools

### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#firecrawlcrawlwebsitetool)  FirecrawlCrawlWebsiteTool

#### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#example)  Example

Utilize the FirecrawlScrapeFromWebsiteTool as follows to allow your agent to load websites:

Copy

```python
from crewai_tools import FirecrawlCrawlWebsiteTool

tool = FirecrawlCrawlWebsiteTool(url='firecrawl.dev')

```

#### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#arguments)  Arguments

- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.
- `url`: The base URL to start crawling from.
- `page_options`: Optional.

  - `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.
  - `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.
- `crawler_options`: Optional. Options for controlling the crawling behavior.

  - `includes`: Optional. URL patterns to include in the crawl.
  - `exclude`: Optional. URL patterns to exclude from the crawl.
  - `generateImgAltText`: Optional. Generate alt text for images using LLMs (requires a paid plan).
  - `returnOnlyUrls`: Optional. If true, returns only the URLs as a list in the crawl status. Note: the response will be a list of URLs inside the data, not a list of documents.
  - `maxDepth`: Optional. Maximum depth to crawl. Depth 1 is the base URL, depth 2 includes the base URL and its direct children, and so on.
  - `mode`: Optional. The crawling mode to use. Fast mode crawls 4x faster on websites without a sitemap but may not be as accurate and shouldn‚Äôt be used on heavily JavaScript-rendered websites.
  - `limit`: Optional. Maximum number of pages to crawl.
  - `timeout`: Optional. Timeout in milliseconds for the crawling operation.

### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#firecrawlscrapewebsitetool)  FirecrawlScrapeWebsiteTool

#### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#example-2)  Example

Utilize the FirecrawlScrapeWebsiteTool as follows to allow your agent to load websites:

Copy

```python
from crewai_tools import FirecrawlScrapeWebsiteTool

tool = FirecrawlScrapeWebsiteTool(url='firecrawl.dev')

```

#### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#arguments-2)  Arguments

- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.
- `url`: The URL to scrape.
- `page_options`: Optional.

  - `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.
  - `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.
- `extractor_options`: Optional. Options for LLM-based extraction of structured information from the page content

  - `mode`: The extraction mode to use, currently supports ‚Äòllm-extraction‚Äô
  - `extractionPrompt`: Optional. A prompt describing what information to extract from the page
  - `extractionSchema`: Optional. The schema for the data to be extracted
- `timeout`: Optional. Timeout in milliseconds for the request

### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#firecrawlsearchtool)  FirecrawlSearchTool

#### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#example-3)  Example

Utilize the FirecrawlSearchTool as follows to allow your agent to load websites:

Copy

```python
from crewai_tools import FirecrawlSearchTool

tool = FirecrawlSearchTool(query='what is firecrawl?')

```

#### [‚Äã](https://docs.firecrawl.dev/integrations/crewai\#arguments-3)  Arguments

- `api_key`: Optional. Specifies Firecrawl API key. Defaults is the `FIRECRAWL_API_KEY` environment variable.
- `query`: The search query string to be used for searching.
- `page_options`: Optional. Options for result formatting.

  - `onlyMainContent`: Optional. Only return the main content of the page excluding headers, navs, footers, etc.
  - `includeHtml`: Optional. Include the raw HTML content of the page. Will output a html key in the response.
  - `fetchPageContent`: Optional. Fetch the full content of the page.
- `search_options`: Optional. Options for controlling the crawling behavior.

  - `limit`: Optional. Maximum number of pages to crawl.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/crewai.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/crewai)

[Llamaindex](https://docs.firecrawl.dev/integrations/llamaindex) [Dify](https://docs.firecrawl.dev/integrations/dify)

On this page

- [Using Firecrawl with CrewAI](https://docs.firecrawl.dev/integrations/crewai#using-firecrawl-with-crewai)
- [Installing Firecrawl Tools inside of CrewAI](https://docs.firecrawl.dev/integrations/crewai#installing-firecrawl-tools-inside-of-crewai)
- [Tools](https://docs.firecrawl.dev/integrations/crewai#tools)
- [FirecrawlCrawlWebsiteTool](https://docs.firecrawl.dev/integrations/crewai#firecrawlcrawlwebsitetool)
- [Example](https://docs.firecrawl.dev/integrations/crewai#example)
- [Arguments](https://docs.firecrawl.dev/integrations/crewai#arguments)
- [FirecrawlScrapeWebsiteTool](https://docs.firecrawl.dev/integrations/crewai#firecrawlscrapewebsitetool)
- [Example](https://docs.firecrawl.dev/integrations/crewai#example-2)
- [Arguments](https://docs.firecrawl.dev/integrations/crewai#arguments-2)
- [FirecrawlSearchTool](https://docs.firecrawl.dev/integrations/crewai#firecrawlsearchtool)
- [Example](https://docs.firecrawl.dev/integrations/crewai#example-3)
- [Arguments](https://docs.firecrawl.dev/integrations/crewai#arguments-3)

## Firecrawl API Documentation
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Quickstart

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

![Hero Light](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/hero.png)

## [‚Äã](https://docs.firecrawl.dev/introduction\#welcome-to-firecrawl)  Welcome to Firecrawl

[Firecrawl](https://firecrawl.dev/?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.

## [‚Äã](https://docs.firecrawl.dev/introduction\#how-to-use-it%3F)  How to use it?

We provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you‚Äôd like.

Check out the following resources to get started:

- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)
- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)
- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)
- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)
- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)
- [ ]  Want an SDK or Integration? Let us know by opening an issue.

**Self-host:** To self-host refer to guide [here](https://docs.firecrawl.dev/contributing/self-host).

### [‚Äã](https://docs.firecrawl.dev/introduction\#api-key)  API Key

To use the API, you need to sign up on [Firecrawl](https://firecrawl.dev/) and get an API key.

### [‚Äã](https://docs.firecrawl.dev/introduction\#features)  Features

- [**Scrape**](https://docs.firecrawl.dev/introduction#scraping): scrapes a URL and get its content in LLM-ready format (markdown, structured data via [LLM Extract](https://docs.firecrawl.dev/introduction#extraction), screenshot, html)
- [**Crawl**](https://docs.firecrawl.dev/introduction#crawling): scrapes all the URLs of a web page and return content in LLM-ready format
- [**Map**](https://docs.firecrawl.dev/features/map): input a website and get all the website urls - extremely fast
- [**Extract**](https://docs.firecrawl.dev/features/extract): get structured data from single page, multiple pages or entire websites with AI.

### [‚Äã](https://docs.firecrawl.dev/introduction\#powerful-capabilities)  Powerful Capabilities

- **LLM-ready formats**: markdown, structured data, screenshot, HTML, links, metadata
- **The hard stuff**: proxies, anti-bot mechanisms, dynamic content (js-rendered), output parsing, orchestration
- **Customizability**: exclude tags, crawl behind auth walls with custom headers, max crawl depth, etc‚Ä¶
- **Media parsing**: pdfs, docx, images.
- **Reliability first**: designed to get the data you need - no matter how hard it is.
- **Actions**: click, scroll, input, wait and more before extracting data

You can find all of Firecrawl‚Äôs capabilities and how to use them in our [documentation](https://docs.firecrawl.dev/)

## [‚Äã](https://docs.firecrawl.dev/introduction\#crawling)  Crawling

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

### [‚Äã](https://docs.firecrawl.dev/introduction\#installation)  Installation

Python

Node

Go

Rust

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/introduction\#usage)  Usage

Python

Node

Go

Rust

cURL

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)

```

If you‚Äôre using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.

```json
{
  "success": true,
  "id": "123-456-789",
  "url": "https://api.firecrawl.dev/v1/crawl/123-456-789"
}

```

### [‚Äã](https://docs.firecrawl.dev/introduction\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Python

Node

Go

Rust

cURL

```python
crawl_status = app.check_crawl_status("<crawl_id>")
print(crawl_status)

```

#### [‚Äã](https://docs.firecrawl.dev/introduction\#response)  Response

The response will be different depending on the status of the crawl. For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.

Scraping

Completed

```json
{
  "status": "scraping",
  "total": 36,
  "completed": 10,
  "creditsUsed": 10,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "next": "https://api.firecrawl.dev/v1/crawl/123-456-789?skip=10",
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#scraping)  Scraping\
\
To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.\
\
Python\
\
Node\
\
Go\
\
Rust\
\
cURL\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#response-2)  Response\
\
SDKs will return the data object directly. cURL will return the payload exactly as shown below.\
\
```json\
{\
  "success": true,\
  "data" : {\
    "markdown": "Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...",\
    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "https://firecrawl.dev",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#extraction)  Extraction\
\
With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:\
\
v1 is only supported on node, python and cURL at this time.\
\
Python\
\
Node\
\
cURL\
\
```python\
from firecrawl import FirecrawlApp\
from pydantic import BaseModel, Field\
\
# Initialize the FirecrawlApp with your API key\
app = FirecrawlApp(api_key='your_api_key')\
\
class ExtractSchema(BaseModel):\
    company_mission: str\
    supports_sso: bool\
    is_open_source: bool\
    is_in_yc: bool\
\
data = app.scrape_url('https://docs.firecrawl.dev/', {\
    'formats': ['json'],\
    'jsonOptions': {\
        'schema': ExtractSchema.model_json_schema(),\
    }\
})\
print(data["json"])\
\
```\
\
Output:\
\
JSON\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
        "supports_sso": true,\
        "is_open_source": false,\
        "is_in_yc": true\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#extracting-without-schema-new)  Extracting without schema (New)\
\
You can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\
\
cURL\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/scrape \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev/",\
      "formats": ["json"],\
      "jsonOptions": {\
        "prompt": "Extract the company mission from the page."\
      }\
    }'\
\
```\
\
Output:\
\
JSON\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#extraction-v0)  Extraction (v0)\
\
Python\
\
JavaScript\
\
Go\
\
Rust\
\
cURL\
\
```python\
\
app = FirecrawlApp(version="v0")\
\
class ArticleSchema(BaseModel):\
    title: str\
    points: int\
    by: str\
    commentsURL: str\
\
class TopArticlesSchema(BaseModel):\
top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")\
\
data = app.scrape_url('https://news.ycombinator.com', {\
'extractorOptions': {\
'extractionSchema': TopArticlesSchema.model_json_schema(),\
'mode': 'llm-extraction'\
},\
'pageOptions':{\
'onlyMainContent': True\
}\
})\
print(data["llm_extraction"])\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#interacting-with-the-page-with-actions)  Interacting with the page with Actions\
\
Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\
\
Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\
\
It is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#example)  Example\
\
Python\
\
Node\
\
cURL\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev',\
    params={\
        'formats': ['markdown', 'html'],\
        'actions': [\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "click", "selector": "textarea[title=\"Search\"]"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "write", "text": "firecrawl"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "press", "key": "ENTER"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "click", "selector": "h3"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "scrape"},\
            {"type": "screenshot"}\
        ]\
    }\
)\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/introduction\#output)  Output\
\
JSON\
\
```json\
{\
  "success": true,\
  "data": {\
    "markdown": "Our first Launch Week is over! [See the recap üöÄ](blog/firecrawl-launch-week-1-recap)...",\
    "actions": {\
      "screenshots": [\
        "https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png"\
      ],\
      "scrapes": [\
        {\
          "url": "https://www.firecrawl.dev/",\
          "html": "<html><body><h1>Firecrawl</h1></body></html>"\
        }\
      ]\
    },\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "http://google.com",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#open-source-vs-cloud)  Open Source vs Cloud\
\
Firecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE).\
\
To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.\
\
Firecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev/) and offers a range of features that are not available in the open source version:\
\
![Firecrawl Cloud vs Open Source](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/open-source-cloud.png)\
\
## [‚Äã](https://docs.firecrawl.dev/introduction\#contributing)  Contributing\
\
We love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/introduction)\
\
[Launch Week II (New)](https://docs.firecrawl.dev/launch-week)\
\
On this page\
\
- [Welcome to Firecrawl](https://docs.firecrawl.dev/introduction#welcome-to-firecrawl)\
- [How to use it?](https://docs.firecrawl.dev/introduction#how-to-use-it%3F)\
- [API Key](https://docs.firecrawl.dev/introduction#api-key)\
- [Features](https://docs.firecrawl.dev/introduction#features)\
- [Powerful Capabilities](https://docs.firecrawl.dev/introduction#powerful-capabilities)\
- [Crawling](https://docs.firecrawl.dev/introduction#crawling)\
- [Installation](https://docs.firecrawl.dev/introduction#installation)\
- [Usage](https://docs.firecrawl.dev/introduction#usage)\
- [Check Crawl Job](https://docs.firecrawl.dev/introduction#check-crawl-job)\
- [Response](https://docs.firecrawl.dev/introduction#response)\
- [Scraping](https://docs.firecrawl.dev/introduction#scraping)\
- [Response](https://docs.firecrawl.dev/introduction#response-2)\
- [Extraction](https://docs.firecrawl.dev/introduction#extraction)\
- [Extracting without schema (New)](https://docs.firecrawl.dev/introduction#extracting-without-schema-new)\
- [Extraction (v0)](https://docs.firecrawl.dev/introduction#extraction-v0)\
- [Interacting with the page with Actions](https://docs.firecrawl.dev/introduction#interacting-with-the-page-with-actions)\
- [Example](https://docs.firecrawl.dev/introduction#example)\
- [Output](https://docs.firecrawl.dev/introduction#output)\
- [Open Source vs Cloud](https://docs.firecrawl.dev/introduction#open-source-vs-cloud)\
- [Contributing](https://docs.firecrawl.dev/introduction#contributing)

## Firecrawl Langchain Integration
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

Langchain

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

> Note: this integration is still using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction). You can install the 0.0.20 version for the Python SDK or the 0.0.36 for the Node SDK.

## [‚Äã](https://docs.firecrawl.dev/integrations/langchain\#installation)  Installation

Copy

```bash
pip install firecrawl-py==0.0.20

```

## [‚Äã](https://docs.firecrawl.dev/integrations/langchain\#usage)  Usage

You will need to get your own API key. See [https://firecrawl.dev](https://firecrawl.dev/)

Copy

```python
from langchain_community.document_loaders import FireCrawlLoader

loader = FireCrawlLoader(
    api_key="YOUR_API_KEY", url="https://firecrawl.dev", mode="crawl"
)

docs = loader.load()

```

### [‚Äã](https://docs.firecrawl.dev/integrations/langchain\#modes)  Modes

Scrape: Scrape single url and return the markdown.
Crawl: Crawl the url and all accessible sub pages and return the markdown for each one.

Copy

```python
loader = FireCrawlLoader(
    api_key="YOUR_API_KEY",
    url="https://firecrawl.dev",
    mode="scrape",
)

data = loader.load()

```

### [‚Äã](https://docs.firecrawl.dev/integrations/langchain\#crawler-options)  Crawler Options

You can also pass params to the loader. This is a dictionary of options to pass to the crawler. See the FireCrawl API documentation for more information.

## [‚Äã](https://docs.firecrawl.dev/integrations/langchain\#langchain-js)  Langchain JS

To use it in Langchain JS, you can install it via npm:

Copy

```bash
npm install @mendableai/firecrawl-js

```

Then, you can use it like this:

Copy

```typescript
import { FireCrawlLoader } from "langchain/document_loaders/web/firecrawl";

const loader = new FireCrawlLoader({
  url: "https://firecrawl.dev", // The URL to scrape
  apiKey: process.env.FIRECRAWL_API_KEY, // Optional, defaults to `FIRECRAWL_API_KEY` in your env.
  mode: "scrape", // The mode to run the crawler in. Can be "scrape" for single urls or "crawl" for all accessible subpages
  params: {
    // optional parameters based on Firecrawl API docs
    // For API documentation, visit https://docs.firecrawl.dev
  },
});

const docs = await loader.load();

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/langchain.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/langchain)

[Extract (New)](https://docs.firecrawl.dev/features/extract) [Llamaindex](https://docs.firecrawl.dev/integrations/llamaindex)

On this page

- [Installation](https://docs.firecrawl.dev/integrations/langchain#installation)
- [Usage](https://docs.firecrawl.dev/integrations/langchain#usage)
- [Modes](https://docs.firecrawl.dev/integrations/langchain#modes)
- [Crawler Options](https://docs.firecrawl.dev/integrations/langchain#crawler-options)
- [Langchain JS](https://docs.firecrawl.dev/integrations/langchain#langchain-js)

## Firecrawl Go SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Go

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/sdks/go\#installation)  Installation

To install the Firecrawl Go SDK, you can use go get:

Go

Copy

```bash
go get github.com/mendableai/firecrawl-go

```

## [‚Äã](https://docs.firecrawl.dev/sdks/go\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the `API key` as a parameter to the `FirecrawlApp` struct.
3. Set the `API URL` and/or pass it as a parameter to the `FirecrawlApp` struct. Defaults to `https://api.firecrawl.dev`.
4. Set the `version` and/or pass it as a parameter to the `FirecrawlApp` struct. Defaults to `v1`.

Here‚Äôs an example of how to use the SDK with error handling:

Go

Copy

```go
import (
	"fmt"
	"log"
	"github.com/google/uuid"
	"github.com/mendableai/firecrawl-go"
)

func ptr[T any](v T) *T {
	return &v
}

func main() {
	// Initialize the FirecrawlApp with your API key
	apiKey := "fc-YOUR_API_KEY"
	apiUrl := "https://api.firecrawl.dev"
	version := "v1"

	app, err := firecrawl.NewFirecrawlApp(apiKey, apiUrl, version)
	if err != nil {
		log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
	}

  // Scrape a website
  scrapeStatus, err := app.ScrapeUrl("https://firecrawl.dev", firecrawl.ScrapeParams{
    Formats: []string{"markdown", "html"},
  })
  if err != nil {
    log.Fatalf("Failed to send scrape request: %v", err)
  }

  fmt.Println(scrapeStatus)

	// Crawl a website
  idempotencyKey := uuid.New().String() // optional idempotency key
  crawlParams := &firecrawl.CrawlParams{
		ExcludePaths: []string{"blog/*"},
		MaxDepth:     ptr(2),
	}

	crawlStatus, err := app.CrawlUrl("https://firecrawl.dev", crawlParams, &idempotencyKey)
	if err != nil {
		log.Fatalf("Failed to send crawl request: %v", err)
	}

	fmt.Println(crawlStatus)
}

```

### [‚Äã](https://docs.firecrawl.dev/sdks/go\#scraping-a-url)  Scraping a URL

To scrape a single URL with error handling, use the `ScrapeURL` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Go

Copy

```go
// Scrape a website
scrapeResult, err := app.ScrapeUrl("https://firecrawl.dev", map[string]any{
  "formats": []string{"markdown", "html"},
})
if err != nil {
  log.Fatalf("Failed to scrape URL: %v", err)
}

fmt.Println(scrapeResult)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/go\#crawling-a-website)  Crawling a Website

To crawl a website, use the `CrawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Go

Copy

```go
crawlStatus, err := app.CrawlUrl("https://firecrawl.dev", map[string]any{
  "limit": 100,
  "scrapeOptions": map[string]any{
    "formats": []string{"markdown", "html"},
  },
})
if err != nil {
  log.Fatalf("Failed to send crawl request: %v", err)
}

fmt.Println(crawlStatus)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/go\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job, use the `CheckCrawlStatus` method. It takes the job ID as a parameter and returns the current status of the crawl job.

Go

Copy

```go
// Get crawl status
crawlStatus, err := app.CheckCrawlStatus("<crawl_id>")

if err != nil {
  log.Fatalf("Failed to get crawl status: %v", err)
}

fmt.Println(crawlStatus)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/go\#map-a-website)  Map a Website

Use `MapUrl` to generate a list of URLs from a website. The `params` argument let you customize the mapping process, including options to exclude subdomains or to utilize the sitemap.

Go

Copy

```go
// Map a website
mapResult, err := app.MapUrl("https://firecrawl.dev", nil)
if err != nil {
  log.Fatalf("Failed to map URL: %v", err)
}

fmt.Println(mapResult)

```

## [‚Äã](https://docs.firecrawl.dev/sdks/go\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/go.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/go)

[Node](https://docs.firecrawl.dev/sdks/node) [Rust](https://docs.firecrawl.dev/sdks/rust)

On this page

- [Installation](https://docs.firecrawl.dev/sdks/go#installation)
- [Usage](https://docs.firecrawl.dev/sdks/go#usage)
- [Scraping a URL](https://docs.firecrawl.dev/sdks/go#scraping-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/sdks/go#crawling-a-website)
- [Checking Crawl Status](https://docs.firecrawl.dev/sdks/go#checking-crawl-status)
- [Map a Website](https://docs.firecrawl.dev/sdks/go#map-a-website)
- [Error Handling](https://docs.firecrawl.dev/sdks/go#error-handling)

## Firecrawl Contribution Guide
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Contributing

Running locally

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

Welcome to [Firecrawl](https://firecrawl.dev/) üî•! Here are some instructions on how to get the project locally so you can run it on your own and contribute.

If you‚Äôre contributing, note that the process is similar to other open-source repos, i.e., fork Firecrawl, make changes, run tests, PR.

If you have any questions or would like help getting on board, join our Discord community [here](https://discord.gg/gSmWdAkdwd) for more information or submit an issue on Github [here](https://github.com/mendableai/firecrawl/issues/new/choose)!

## [‚Äã](https://docs.firecrawl.dev/contributing/guide\#running-the-project-locally)  Running the project locally

First, start by installing dependencies:

1. node.js [instructions](https://nodejs.org/en/learn/getting-started/how-to-install-nodejs)
2. pnpm [instructions](https://pnpm.io/installation)
3. redis [instructions](https://redis.io/docs/latest/operate/oss_and_stack/install/install-redis/)

Set environment variables in a `.env` file in the `/apps/api/` directory. You can copy over the template in `.env.example`.

To start, we won‚Äôt set up authentication, or any optional sub services (pdf parsing, JS blocking support, AI features)

Copy

```
# ./apps/api/.env

# ===== Required ENVS ======
NUM_WORKERS_PER_QUEUE=8
PORT=3002
HOST=0.0.0.0

#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379
REDIS_URL=redis://localhost:6379

#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379
REDIS_RATE_LIMIT_URL=redis://localhost:6379
PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/html

## To turn on DB authentication, you need to set up supabase.
USE_DB_AUTHENTICATION=false

# ===== Optional ENVS ======

# Supabase Setup (used to support DB authentication, advanced logging, etc.)
SUPABASE_ANON_TOKEN=
SUPABASE_URL=
SUPABASE_SERVICE_TOKEN=

# Other Optionals
# use if you've set up authentication and want to test with a real API key
TEST_API_KEY=
# set if you'd like to test the scraping rate limit
RATE_LIMIT_TEST_API_KEY_SCRAPE=
# set if you'd like to test the crawling rate limit
RATE_LIMIT_TEST_API_KEY_CRAWL=
# set if you'd like to use scraping Be to handle JS blocking
SCRAPING_BEE_API_KEY=
# add for LLM dependednt features (image alt generation, etc.)
OPENAI_API_KEY=
BULL_AUTH_KEY=@
# use if you're configuring basic logging with logtail
LOGTAIL_KEY=
# set if you have a llamaparse key you'd like to use to parse pdfs
LLAMAPARSE_API_KEY=
# set if you'd like to send slack server health status messages
SLACK_WEBHOOK_URL=
# set if you'd like to send posthog events like job logs
POSTHOG_API_KEY=
# set if you'd like to send posthog events like job logs
POSTHOG_HOST=

# set if you'd like to use the fire engine closed beta
FIRE_ENGINE_BETA_URL=

# Proxy Settings for Playwright (Alternative you can can use a proxy service like oxylabs, which rotates IPs for you on every request)
PROXY_SERVER=
PROXY_USERNAME=
PROXY_PASSWORD=
# set if you'd like to block media requests to save proxy bandwidth
BLOCK_MEDIA=

# Set this to the URL of your webhook when using the self-hosted version of FireCrawl
SELF_HOSTED_WEBHOOK_URL=

# Resend API Key for transactional emails
RESEND_API_KEY=

# LOGGING_LEVEL determines the verbosity of logs that the system will output.
# Available levels are:
# NONE - No logs will be output.
# ERROR - For logging error messages that indicate a failure in a specific operation.
# WARN - For logging potentially harmful situations that are not necessarily errors.
# INFO - For logging informational messages that highlight the progress of the application.
# DEBUG - For logging detailed information on the flow through the system, primarily used for debugging.
# TRACE - For logging more detailed information than the DEBUG level.
# Set LOGGING_LEVEL to one of the above options to control logging output.
LOGGING_LEVEL=INFO

```

### [‚Äã](https://docs.firecrawl.dev/contributing/guide\#installing-dependencies)  Installing dependencies

First, install the dependencies using pnpm.

Copy

```bash
# cd apps/api # to make sure you're in the right folder
pnpm install # make sure you have pnpm version 9+!

```

### [‚Äã](https://docs.firecrawl.dev/contributing/guide\#running-the-project)  Running the project

You‚Äôre going to need to open 3 terminals for running the services. Here is [a video guide accurate as of Oct 2024](https://youtu.be/LHqg5QNI4UY) (optional: 4 terminals for running the services and testing).

### [‚Äã](https://docs.firecrawl.dev/contributing/guide\#terminal-1-setting-up-redis)  Terminal 1 - setting up redis

Run the command anywhere within your project

Copy

```bash
redis-server

```

### [‚Äã](https://docs.firecrawl.dev/contributing/guide\#terminal-2-setting-up-workers)  Terminal 2 - setting up workers

Now, navigate to the apps/api/ directory and run:

Copy

```bash
pnpm run workers
# if you are going to use the [llm-extract feature](https://github.com/mendableai/firecrawl/pull/586/), you should also export OPENAI_API_KEY=sk-______

```

This will start the workers who are responsible for processing crawl jobs.

### [‚Äã](https://docs.firecrawl.dev/contributing/guide\#terminal-3-setting-up-the-main-server)  Terminal 3 - setting up the main server

To do this, navigate to the apps/api/ directory. If you haven‚Äôt installed pnpm already, you can do so here: [https://pnpm.io/installation](https://pnpm.io/installation)

Next, run your server with:

Copy

```bash
pnpm run start

```

### [‚Äã](https://docs.firecrawl.dev/contributing/guide\#optional-terminal-4-sending-our-first-request)  _(Optional)_ Terminal 4 - sending our first request

Alright, now let‚Äôs send our first request.

Copy

```curl
curl -X GET http://localhost:3002/test

```

This should return the response Hello, world!

If you‚Äôd like to test the crawl endpoint, you can run this

Copy

```curl
curl -X POST http://localhost:3002/v0/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'

```

## [‚Äã](https://docs.firecrawl.dev/contributing/guide\#tests%3A)  Tests:

The best way to do this is run the test with `npm run test:local-no-auth` if you‚Äôd like to run the tests without authentication.

If you‚Äôd like to run the tests with authentication, run `npm run test:prod`

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/contributing/guide.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/contributing/guide)

[Open Source vs Cloud](https://docs.firecrawl.dev/contributing/open-source-or-cloud) [Self-hosting](https://docs.firecrawl.dev/contributing/self-host)

On this page

- [Running the project locally](https://docs.firecrawl.dev/contributing/guide#running-the-project-locally)
- [Installing dependencies](https://docs.firecrawl.dev/contributing/guide#installing-dependencies)
- [Running the project](https://docs.firecrawl.dev/contributing/guide#running-the-project)
- [Terminal 1 - setting up redis](https://docs.firecrawl.dev/contributing/guide#terminal-1-setting-up-redis)
- [Terminal 2 - setting up workers](https://docs.firecrawl.dev/contributing/guide#terminal-2-setting-up-workers)
- [Terminal 3 - setting up the main server](https://docs.firecrawl.dev/contributing/guide#terminal-3-setting-up-the-main-server)
- [(Optional) Terminal 4 - sending our first request](https://docs.firecrawl.dev/contributing/guide#optional-terminal-4-sending-our-first-request)
- [Tests:](https://docs.firecrawl.dev/contributing/guide#tests%3A)

## Dify Integration Guide
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

Dify

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/integrations/dify\#sync-data-from-websites-for-dify-workflows)  Sync Data from Websites for Dify workflows

Firecrawl can be used inside of [Dify the LLM workflow builder](https://cloud.dify.ai/). This page introduces how to scrape data from a web page, parse it into Markdown, and import it into the Dify knowledge base using their Firecrawl integration.

### [‚Äã](https://docs.firecrawl.dev/integrations/dify\#configuring-firecrawl)  Configuring Firecrawl

First, you need to configure Firecrawl credentials in the Data Source section of the Settings page.

![Configure Firecrawl key](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_config.avif)

Log in to your Firecrawl account and get your API Key, and then enter and save it in Dify.

![Save Firecrawl key](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_savekey.png)

### [‚Äã](https://docs.firecrawl.dev/integrations/dify\#scrape-target-webpage)  Scrape target webpage

Now comes the fun part, scraping and crawling. On the knowledge base creation page, select Sync from website and enter the URL to be scraped.

![Scraping setup](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_webscrape.webp)

The configuration options include: Whether to crawl sub-pages, Page crawling limit, Page scraping max depth, Excluded paths, Include only paths, and Content extraction scope. After completing the configuration, click Run to preview the parsed pages.

![Set Firecrawl configuration](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_fcoptions.webp)

### [‚Äã](https://docs.firecrawl.dev/integrations/dify\#review-import-results)  Review import results

After importing the parsed text from the webpage, it is stored in the knowledge base documents. View the import results and click Add URL to continue importing new web pages.

![See results of the Firecrawl scrape](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_dify_results.webp)

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/dify.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/dify)

[CrewAI](https://docs.firecrawl.dev/integrations/crewai) [Flowise](https://docs.firecrawl.dev/integrations/flowise)

On this page

- [Sync Data from Websites for Dify workflows](https://docs.firecrawl.dev/integrations/dify#sync-data-from-websites-for-dify-workflows)
- [Configuring Firecrawl](https://docs.firecrawl.dev/integrations/dify#configuring-firecrawl)
- [Scrape target webpage](https://docs.firecrawl.dev/integrations/dify#scrape-target-webpage)
- [Review import results](https://docs.firecrawl.dev/integrations/dify#review-import-results)

![Save Firecrawl key](https://docs.firecrawl.dev/integrations/dify)

![Configure Firecrawl key](https://docs.firecrawl.dev/integrations/dify)

## Search and Scrape
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Features

Search

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/features/search\#searching-the-web-and-scraping-the-results-with-firecrawl)  Searching the web and scraping the results with Firecrawl

Firecrawl integrates its SERP (Search Engine Results Page) API with its robust scraping infrastructure to provide a seamless search and scrape functionality through a single endpoint. Here‚Äôs why:

1. **Unified Search Query:**
Users submit a search query via the SERP endpoint.

2. **Automated Result Scraping:**
Firecrawl automatically processes the search results and utilizes its scraping capabilities to extract data from each result page.

3. **Data Delivery:**
The scraped data from all result pages is compiled and delivered in a clean markdown - ready to use.


This integration allows users to efficiently perform web searches and obtain comprehensive, scraped data from multiple sources with minimal effort.

For more details, refer to the [Search Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/search).

## [‚Äã](https://docs.firecrawl.dev/features/search\#search-any-query)  Search any query

### [‚Äã](https://docs.firecrawl.dev/features/search\#%2Fsearch-endpoint)  /search endpoint

Used to search the web, get the most relevant results, scrape each page and return the markdown.

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/search \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "query": "firecrawl",
      "pageOptions": {
        "fetchPageContent": true // false for a fast serp api
      }
    }'

```

Copy

```json
{
  "success": true,
  "data": [\
    {\
      "url": "https://docs.firecrawl.dev",\
      "markdown": "# Markdown Content",\
      "provider": "web-scraper",\
      "metadata": {\
        "title": "Firecrawl | Scrape the web reliably for your LLMs",\
        "description": "AI for CX and Sales",\
        "language": null,\
        "sourceURL": "https://docs.firecrawl.dev/"\
      }\
    }\
  ]
}

```

### [‚Äã](https://docs.firecrawl.dev/features/search\#with-python-sdk)  With Python SDK

#### [‚Äã](https://docs.firecrawl.dev/features/search\#installing-python-sdk)  Installing Python SDK

Copy

```bash
pip install firecrawl-py

```

#### [‚Äã](https://docs.firecrawl.dev/features/search\#search-a-query)  Search a query

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

result = app.search(query="What is firecrawl?")

```

The response will be similar to the one shown in the curl command above.

### [‚Äã](https://docs.firecrawl.dev/features/search\#with-javascript-sdk)  With JavaScript SDK

#### [‚Äã](https://docs.firecrawl.dev/features/search\#installing-javascript-sdk)  Installing JavaScript SDK

Copy

```bash
npm install @mendable/firecrawl-js

```

#### [‚Äã](https://docs.firecrawl.dev/features/search\#search-a-query-2)  Search a query

Copy

```javascript
import FirecrawlApp from '@mendable/firecrawl-js';

// Initialize the FirecrawlApp with your API key
const app = new FirecrawlApp({ apiKey: 'YOUR_API_KEY' });

// Perform a search
const result = await app.search('What is firecrawl?');

```

The response will be similar to the one shown in the curl command above.

### [‚Äã](https://docs.firecrawl.dev/features/search\#with-go-sdk)  With Go SDK

#### [‚Äã](https://docs.firecrawl.dev/features/search\#installing-go-sdk)  Installing Go SDK

Copy

```bash
go get github.com/mendableai/firecrawl-go

```

#### [‚Äã](https://docs.firecrawl.dev/features/search\#search-a-query-3)  Search a query

Copy

```go
import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
      log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  query := "What is firecrawl?"
  searchResult, err := app.Search(query)
  if err != nil {
    log.Fatalf("Failed to search: %v", err)
  }
  fmt.Println(searchResult)
}

```

### [‚Äã](https://docs.firecrawl.dev/features/search\#with-rust-sdk)  With Rust SDK

#### [‚Äã](https://docs.firecrawl.dev/features/search\#installing-rust-sdk)  Installing Rust SDK

Add the following to your `Cargo.toml`:

Copy

```toml
[dependencies]
firecrawl = "^0.1"
tokio = { version = "^1", features = ["full"] }
serde = { version = "^1.0", features = ["derive"] }
serde_json = "^1.0"
uuid = { version = "^1.10", features = ["v4"] }

[build-dependencies]
tokio = { version = "1", features = ["full"] }

```

#### [‚Äã](https://docs.firecrawl.dev/features/search\#search-a-query-4)  Search a query

Copy

```rust
async fn main() {
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  let query = "What is firecrawl?";
  let search_result = app.search(query).await;

  match search_result {
    Ok(data) => println!("Search Result: {}", data),
    Err(e) => eprintln!("Failed to search: {}", e),
  }
}

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/search.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/search)

[LLM Extract](https://docs.firecrawl.dev/v0/features/extract) [Langchain](https://docs.firecrawl.dev/integrations/langchain)

On this page

- [Searching the web and scraping the results with Firecrawl](https://docs.firecrawl.dev/features/search#searching-the-web-and-scraping-the-results-with-firecrawl)
- [Search any query](https://docs.firecrawl.dev/features/search#search-any-query)
- [/search endpoint](https://docs.firecrawl.dev/features/search#%2Fsearch-endpoint)
- [With Python SDK](https://docs.firecrawl.dev/features/search#with-python-sdk)
- [Installing Python SDK](https://docs.firecrawl.dev/features/search#installing-python-sdk)
- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query)
- [With JavaScript SDK](https://docs.firecrawl.dev/features/search#with-javascript-sdk)
- [Installing JavaScript SDK](https://docs.firecrawl.dev/features/search#installing-javascript-sdk)
- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query-2)
- [With Go SDK](https://docs.firecrawl.dev/features/search#with-go-sdk)
- [Installing Go SDK](https://docs.firecrawl.dev/features/search#installing-go-sdk)
- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query-3)
- [With Rust SDK](https://docs.firecrawl.dev/features/search#with-rust-sdk)
- [Installing Rust SDK](https://docs.firecrawl.dev/features/search#installing-rust-sdk)
- [Search a query](https://docs.firecrawl.dev/features/search#search-a-query-4)

## Firecrawl Python SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Python

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/sdks/python\#installation)  Installation

To install the Firecrawl Python SDK, you can use pip:

Python

Copy

```bash
pip install firecrawl-py

```

## [‚Äã](https://docs.firecrawl.dev/sdks/python\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.

Here‚Äôs an example of how to use the SDK:

Python

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_status = app.scrape_url(
  'https://firecrawl.dev',
  params={'formats': ['markdown', 'html']}
)
print(scrape_status)

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  }
)
print(crawl_status)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#scraping-a-url)  Scraping a URL

To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Python

Copy

```python
# Scrape a website:
scrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})
print(scrape_result)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#crawling-a-website)  Crawling a Website

To crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Python

Copy

```python
crawl_status = app.crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#asynchronous-crawling)  Asynchronous Crawling

To crawl a website asynchronously, use the `crawl_url_async` method. It returns the crawl `ID` which you can use to check the status of the crawl job. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Python

Copy

```python
crawl_status = app.async_crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  }
)
print(crawl_status)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.

Python

Copy

```python
crawl_status = app.check_crawl_status("<crawl_id>")
print(crawl_status)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#cancelling-a-crawl)  Cancelling a Crawl

To cancel an asynchronous crawl job, use the `cancel_crawl` method. It takes the job ID of the asynchronous crawl as a parameter and returns the cancellation status.

Python

Copy

```python
cancel_crawl = app.cancel_crawl(id)
print(cancel_crawl)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#map-a-website)  Map a Website

Use `map_url` to generate a list of URLs from a website. The `params` argument let you customize the mapping process, including options to exclude subdomains or to utilize the sitemap.

Python

Copy

```python
# Map a website:
map_result = app.map_url('https://firecrawl.dev')
print(map_result)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/python\#crawling-a-website-with-websockets)  Crawling a Website with WebSockets

To crawl a website with WebSockets, use the `crawl_url_and_watch` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Python

Copy

```python
# inside an async function...
nest_asyncio.apply()

# Define event handlers
def on_document(detail):
    print("DOC", detail)

def on_error(detail):
    print("ERR", detail['error'])

def on_done(detail):
    print("DONE", detail['status'])

    # Function to start the crawl and watch process
async def start_crawl_and_watch():
    # Initiate the crawl job and get the watcher
    watcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })

    # Add event listeners
    watcher.add_event_listener("document", on_document)
    watcher.add_event_listener("error", on_error)
    watcher.add_event_listener("done", on_done)

    # Start the watcher
    await watcher.connect()

# Run the event loop
await start_crawl_and_watch()

```

## [‚Äã](https://docs.firecrawl.dev/sdks/python\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/python.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/python)

[Overview](https://docs.firecrawl.dev/sdks/overview) [Node](https://docs.firecrawl.dev/sdks/node)

On this page

- [Installation](https://docs.firecrawl.dev/sdks/python#installation)
- [Usage](https://docs.firecrawl.dev/sdks/python#usage)
- [Scraping a URL](https://docs.firecrawl.dev/sdks/python#scraping-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/sdks/python#crawling-a-website)
- [Asynchronous Crawling](https://docs.firecrawl.dev/sdks/python#asynchronous-crawling)
- [Checking Crawl Status](https://docs.firecrawl.dev/sdks/python#checking-crawl-status)
- [Cancelling a Crawl](https://docs.firecrawl.dev/sdks/python#cancelling-a-crawl)
- [Map a Website](https://docs.firecrawl.dev/sdks/python#map-a-website)
- [Crawling a Website with WebSockets](https://docs.firecrawl.dev/sdks/python#crawling-a-website-with-websockets)
- [Error Handling](https://docs.firecrawl.dev/sdks/python#error-handling)

## Firecrawl Launch Week
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Launch Week II (New)

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-7-faster-markdown-parsing)  Day 7 - Faster Markdown Parsing

We‚Äôve rebuilt our Markdown parser from the ground up with a focus on speed and performance. This enhancement ensures that your web scraping tasks are more efficient and deliver higher-quality results.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#what%E2%80%99s-new%3F)  What‚Äôs New?

- **Speed Improvements**: Experience parsing speeds up to 4 times faster than before, allowing for quicker data processing and reduced waiting times.
- **Enhanced Reliability**: Our new parser handles a wider range of HTML content more gracefully, reducing errors and improving consistency.
- **Cleaner Markdown Output**: Get cleaner and more readable Markdown, making your data easier to work with and integrate into your workflows.

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-6-mobile-scraping-%2B-mobile-screenshots)  Day 6 - Mobile Scraping (+ Mobile Screenshots)

Firecrawl now introduces **mobile device emulation** for both scraping and screenshots, empowering you to interact with sites as if from a mobile device. This feature is essential for testing mobile-specific content, understanding responsive design, and gaining insights from mobile-specific elements.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#why-mobile-scraping%3F)  Why Mobile Scraping?

Mobile-first experiences are increasingly common, and this feature enables you to:

- Take high-fidelity mobile screenshots for a more accurate representation of how a site appears on mobile.
- Test and verify mobile layouts and UI elements, ensuring the accuracy of your scraping results for responsive websites.
- Scrape mobile-only content, gaining access to information or layouts that vary from desktop versions.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#usage)  Usage

To activate mobile scraping, simply add `"mobile": true` in your request, which will enable Firecrawl‚Äôs mobile emulation mode.

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_result = app.scrape_url('google.com',
    params={
        'formats': ['markdown', 'html'],
        'mobile': true
    }
)
print(scrape_result)

```

For further details, including additional configuration options, visit the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-5-actions-2-new-actions)  Day 5 - Actions (2 new actions)

Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.

We‚Äôre excited to introduce two powerful new actions:

1. **Scrape**: Capture the current page content at any point during your interaction sequence, returning both URL and HTML.
2. **Wait for Selector**: Wait for a specific element to appear on the page before proceeding, ensuring more reliable automation.

Copy

```json
actions = [\
    {"type": "scrape"},\
    {"type": "wait", "selector": "#my-element"},\
]

```

Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, scrape the current page content, and take a screenshot.

For more precise control, you can now use `{type: "wait", selector: "#my-element"}` to wait for a specific element to appear on the page.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#example)  Example

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_result = app.scrape_url('firecrawl.dev',
    params={
        'formats': ['markdown', 'html'],
        'actions': [\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "click", "selector": "textarea[title=\"Search\"]"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "write", "text": "firecrawl"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "press", "key": "ENTER"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "click", "selector": "h3"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "scrape"},\
            {"type": "screenshot"}\
        ]
    }
)
print(scrape_result)

```

### [‚Äã](https://docs.firecrawl.dev/launch-week\#output)  Output

JSON

Copy

```json
{
  "success": true,
  "data": {
    "markdown": "Our first Launch Week is over! [See the recap üöÄ](blog/firecrawl-launch-week-1-recap)...",
    "actions": {
      "screenshots": [\
        "https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png"\
      ],
      "scrapes": [\
        {\
          "url": "https://www.firecrawl.dev/",\
          "html": "<html><body><h1>Firecrawl</h1></body></html>"\
        }\
      ]
    },
    "metadata": {
      "title": "Home - Firecrawl",
      "description": "Firecrawl crawls and converts any website into clean markdown.",
      "language": "en",
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",
      "robots": "follow, index",
      "ogTitle": "Firecrawl",
      "ogDescription": "Turn any website into LLM-ready data.",
      "ogUrl": "https://www.firecrawl.dev/",
      "ogImage": "https://www.firecrawl.dev/og.png?123",
      "ogLocaleAlternate": [],
      "ogSiteName": "Firecrawl",
      "sourceURL": "http://google.com",
      "statusCode": 200
    }
  }
}

```

For more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-4-advanced-iframe-scraping)  Day 4 - Advanced iframe scraping

We‚Äôre excited to announce comprehensive iframe scraping support in Firecrawl. Our scraper can now seamlessly handle nested iframes, dynamically loaded content, and cross-origin frames - solving one of web scraping‚Äôs most challenging technical hurdles.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#technical-innovation)  Technical Innovation

Firecrawl now implements:

- Recursive iframe traversal and content extraction
- Cross-origin iframe handling with proper security context management
- Smart automatic wait for iframe content to load
- Support for dynamically injected iframes
- Proper handling of sandboxed iframes

### [‚Äã](https://docs.firecrawl.dev/launch-week\#why-it-matters)  Why it matters

Many modern websites use iframes for:

- Embedded content and widgets
- Payment forms and secure inputs
- Third-party integrations
- Advertisement frames
- Social media embeds

Previously, these elements were often black boxes in scraping results. Now, you get complete access to iframe content just like any other part of the page.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#usage-2)  Usage

No additional configuration needed! The iframe scraping happens automatically when you use any of our scraping or crawling endpoints. Whether you‚Äôre using `/scrape` for single pages or `/crawl` for entire websites, iframe content will be seamlessly integrated into your results.

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-3-credit-packs)  Day 3 - Credit Packs

Credit Packs allow you to you can easily top up your plan if your running low.
Additionally, we now offer Auto Recharge, which automatically recharges your account when you‚Äôre approaching your limit.
To enable visit the pricing page at [https://www.firecrawl.dev/pricing](https://www.firecrawl.dev/pricing)

### [‚Äã](https://docs.firecrawl.dev/launch-week\#credit-packs)  Credit Packs

Flexible monthly credit boosts for your projects.

- **$9/mo for 1000 credits**
- Add to any existing plan
- Choose the amount you need

### [‚Äã](https://docs.firecrawl.dev/launch-week\#auto-recharge-credits)  Auto Recharge Credits

Automatically top up your account when credits run low.

- **$11 per 1000 credits**
- Enable auto recharge with any subscription plan

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-2-geolocation)  Day 2 - Geolocation

Introducing location and language settings for scraping requests. Specify country and preferred languages to get relevant content based on your target location and language preferences.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#how-it-works)  How it works

When you specify the location settings, Firecrawl will use an appropriate proxy if available and emulate the corresponding language and timezone settings. By default, the location is set to ‚ÄòUS‚Äô if not specified.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#usage-3)  Usage

To use the location and language settings, include the `location` object in your request body with the following properties:

- `country`: ISO 3166-1 alpha-2 country code (e.g., ‚ÄòUS‚Äô, ‚ÄòAU‚Äô, ‚ÄòDE‚Äô, ‚ÄòJP‚Äô). Defaults to ‚ÄòUS‚Äô.
- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_result = app.scrape_url('airbnb.com',
    params={
        'formats': ['markdown', 'html'],
        'location': {
            'country': 'BR',
            'languages': ['pt-BR']
        }
    }
)
print(scrape_result)

```

## [‚Äã](https://docs.firecrawl.dev/launch-week\#day-1-batch-scrape)  Day 1 - Batch Scrape

You can now scrape multiple URLs at the same time with our new batch endpoint. Ideal for when you don‚Äôt need the scraping results immediately.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#how-it-works-2)  How it works

It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.

The sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.

### [‚Äã](https://docs.firecrawl.dev/launch-week\#usage-4)  Usage

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape multiple websites:
batch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})
print(batch_scrape_result)

# Or, you can use the asynchronous method:
batch_scrape_job = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})
print(batch_scrape_job)

# (async) You can then use the job ID to check the status of the batch scrape:
batch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])
print(batch_scrape_status)

```

### [‚Äã](https://docs.firecrawl.dev/launch-week\#response)  Response

If you‚Äôre using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.

#### [‚Äã](https://docs.firecrawl.dev/launch-week\#synchronous)  Synchronous

Completed

Copy

```json
{
  "status": "completed",
  "total": 36,
  "completed": 36,
  "creditsUsed": 36,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "next": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26",
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
#### [‚Äã](https://docs.firecrawl.dev/launch-week\#asynchronous)  Asynchronous\
\
You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\
\
Copy\
\
```json\
{\
  "success": true,\
  "id": "123-456-789",\
  "url": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789"\
}\
\
```\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/launch-week.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/launch-week)\
\
[Quickstart](https://docs.firecrawl.dev/introduction) [Welcome to V1](https://docs.firecrawl.dev/v1-welcome)\
\
On this page\
\
- [Day 7 - Faster Markdown Parsing](https://docs.firecrawl.dev/launch-week#day-7-faster-markdown-parsing)\
- [What‚Äôs New?](https://docs.firecrawl.dev/launch-week#what%E2%80%99s-new%3F)\
- [Day 6 - Mobile Scraping (+ Mobile Screenshots)](https://docs.firecrawl.dev/launch-week#day-6-mobile-scraping-%2B-mobile-screenshots)\
- [Why Mobile Scraping?](https://docs.firecrawl.dev/launch-week#why-mobile-scraping%3F)\
- [Usage](https://docs.firecrawl.dev/launch-week#usage)\
- [Day 5 - Actions (2 new actions)](https://docs.firecrawl.dev/launch-week#day-5-actions-2-new-actions)\
- [Example](https://docs.firecrawl.dev/launch-week#example)\
- [Output](https://docs.firecrawl.dev/launch-week#output)\
- [Day 4 - Advanced iframe scraping](https://docs.firecrawl.dev/launch-week#day-4-advanced-iframe-scraping)\
- [Technical Innovation](https://docs.firecrawl.dev/launch-week#technical-innovation)\
- [Why it matters](https://docs.firecrawl.dev/launch-week#why-it-matters)\
- [Usage](https://docs.firecrawl.dev/launch-week#usage-2)\
- [Day 3 - Credit Packs](https://docs.firecrawl.dev/launch-week#day-3-credit-packs)\
- [Credit Packs](https://docs.firecrawl.dev/launch-week#credit-packs)\
- [Auto Recharge Credits](https://docs.firecrawl.dev/launch-week#auto-recharge-credits)\
- [Day 2 - Geolocation](https://docs.firecrawl.dev/launch-week#day-2-geolocation)\
- [How it works](https://docs.firecrawl.dev/launch-week#how-it-works)\
- [Usage](https://docs.firecrawl.dev/launch-week#usage-3)\
- [Day 1 - Batch Scrape](https://docs.firecrawl.dev/launch-week#day-1-batch-scrape)\
- [How it works](https://docs.firecrawl.dev/launch-week#how-it-works-2)\
- [Usage](https://docs.firecrawl.dev/launch-week#usage-4)\
- [Response](https://docs.firecrawl.dev/launch-week#response)\
- [Synchronous](https://docs.firecrawl.dev/launch-week#synchronous)\
- [Asynchronous](https://docs.firecrawl.dev/launch-week#asynchronous)

## Firecrawl Langflow Integration
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

Langflow

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/integrations/langflow\#sync-web-data-in-langflow-workflows)  Sync web data in Langflow workflows

Firecrawl can be used inside of [Langflow, the AI workflow builder](https://www.langflow.org/). This page introduces how to configure and use a Firecrawl block inside of Langflow.

![Firecrawl Langflow block](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_langflow_block.png)

### [‚Äã](https://docs.firecrawl.dev/integrations/langflow\#scraping-with-firecrawl-blocks)  Scraping with Firecrawl blocks

1. Log in to your Firecrawl account and get your API Key, and then enter it on the block or pass it in from another part of the workflow.
2. (Optional) Connect Text Splitter.
3. Select the scrape mode to pick up a single page.
4. Input target URL to be scraped or pass it in from another part of the workflow.
5. Set up any Page Options and Extractor Options depending on what website and data you are trying to get. You can also pass these in from another part of the workflow.
6. Use the data in your workflows.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/langflow.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/langflow)

[Flowise](https://docs.firecrawl.dev/integrations/flowise) [Camel AI](https://docs.firecrawl.dev/integrations/camelai)

On this page

- [Sync web data in Langflow workflows](https://docs.firecrawl.dev/integrations/langflow#sync-web-data-in-langflow-workflows)
- [Scraping with Firecrawl blocks](https://docs.firecrawl.dev/integrations/langflow#scraping-with-firecrawl-blocks)

![Firecrawl Langflow block](https://docs.firecrawl.dev/integrations/langflow)

## Firecrawl API Service
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Quickstart

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

![Hero Light](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/hero.png)

## [‚Äã](https://docs.firecrawl.dev/v0/introduction\#welcome-to-firecrawl)  Welcome to Firecrawl

[Firecrawl](https://firecrawl.dev/?ref=github) is an API service that takes a URL, crawls it, and converts it into clean markdown. We crawl all accessible subpages and give you clean markdown for each. No sitemap required.

## [‚Äã](https://docs.firecrawl.dev/v0/introduction\#how-to-use-it%3F)  How to use it?

We provide an easy to use API with our hosted version. You can find the playground and documentation [here](https://firecrawl.dev/playground). You can also self host the backend if you‚Äôd like.

Check out the following resources to get started:

- [x] **API**: [Documentation](https://docs.firecrawl.dev/api-reference/introduction)
- [x] **SDKs**: [Python](https://docs.firecrawl.dev/sdks/python), [Node](https://docs.firecrawl.dev/sdks/node), [Go](https://docs.firecrawl.dev/sdks/go), [Rust](https://docs.firecrawl.dev/sdks/rust)
- [x] **LLM Frameworks**: [Langchain (python)](https://python.langchain.com/docs/integrations/document_loaders/firecrawl/), [Langchain (js)](https://js.langchain.com/docs/integrations/document_loaders/web_loaders/firecrawl), [Llama Index](https://docs.llamaindex.ai/en/latest/examples/data_connectors/WebPageDemo/#using-firecrawl-reader), [Crew.ai](https://docs.crewai.com/), [Composio](https://composio.dev/tools/firecrawl/all), [PraisonAI](https://docs.praison.ai/firecrawl/), [Superinterface](https://superinterface.ai/docs/assistants/functions/firecrawl), [Vectorize](https://docs.vectorize.io/integrations/source-connectors/firecrawl)
- [x] **Low-code Frameworks**: [Dify](https://dify.ai/blog/dify-ai-blog-integrated-with-firecrawl), [Langflow](https://docs.langflow.org/), [Flowise AI](https://docs.flowiseai.com/integrations/langchain/document-loaders/firecrawl), [Cargo](https://docs.getcargo.io/integration/firecrawl), [Pipedream](https://pipedream.com/apps/firecrawl/)
- [x] **Others**: [Zapier](https://zapier.com/apps/firecrawl/integrations), [Pabbly Connect](https://www.pabbly.com/connect/integrations/firecrawl/)
- [ ]  Want an SDK or Integration? Let us know by opening an issue.

**Self-host:** To self-host refer to guide [here](https://docs.firecrawl.dev/contributing/self-host).

### [‚Äã](https://docs.firecrawl.dev/v0/introduction\#api-key)  API Key

To use the API, you need to sign up on [Firecrawl](https://firecrawl.dev/) and get an API key.

## [‚Äã](https://docs.firecrawl.dev/v0/introduction\#crawling)  Crawling

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

### [‚Äã](https://docs.firecrawl.dev/v0/introduction\#installation)  Installation

Python

JavaScript

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/v0/introduction\#usage)  Usage

Python

JavaScript

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

crawl_result = app.crawl_url('docs.firecrawl.dev', {'crawlerOptions': {'excludes': ['blog/*']}})

# Get the markdown
for result in crawl_result:
    print(result['markdown'])

```

If you are not using the sdk or prefer to use webhook or a different polling method, you can set the `wait_until_done` to `false`.
This will return a jobId.

For cURL, /crawl will always return a jobId where you can use to check the status of the crawl.

Copy

```json
{ "jobId": "1234-5678-9101" }

```

### [‚Äã](https://docs.firecrawl.dev/v0/introduction\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Python

JavaScript

Go

Rust

cURL

Copy

```python
status = app.check_crawl_status(job_id)

```

#### [‚Äã](https://docs.firecrawl.dev/v0/introduction\#response)  Response

Copy

```json
{
  "status": "completed",
  "current": 22,
  "total": 22,
  "data": [\
    {\
      "content": "Raw Content ",\
      "markdown": "# Markdown Content",\
      "provider": "web-scraper",\
      "metadata": {\
        "title": "Firecrawl | Scrape the web reliably for your LLMs",\
        "description": "AI for CX and Sales",\
        "language": null,\
        "sourceURL": "https://docs.firecrawl.dev/"\
      }\
    }\
  ]
}

```

## [‚Äã](https://docs.firecrawl.dev/v0/introduction\#scraping)  Scraping

To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Python

JavaScript

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")

```

### [‚Äã](https://docs.firecrawl.dev/v0/introduction\#response-2)  Response

Copy

```json
{
  "success": true,
  "data": {
    "markdown": "<string>",
    "content": "<string>",
    "html": "<string>",
    "rawHtml": "<string>",
    "metadata": {
      "title": "<string>",
      "description": "<string>",
      "language": "<string>",
      "sourceURL": "<string>",
      "<any other metadata> ": "<string>",
      "pageStatusCode": 123,
      "pageError": "<string>"
    },
    "llm_extraction": {},
    "warning": "<string>"
  }
}

```

## [‚Äã](https://docs.firecrawl.dev/v0/introduction\#extraction)  Extraction

With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:

Python

JavaScript

Go

Rust

cURL

Copy

```python
class ArticleSchema(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")

data = app.scrape_url('https://news.ycombinator.com', {
'extractorOptions': {
'extractionSchema': TopArticlesSchema.model_json_schema(),
'mode': 'llm-extraction'
},
'pageOptions':{
'onlyMainContent': True
}
})
print(data["llm_extraction"])

```

## [‚Äã](https://docs.firecrawl.dev/v0/introduction\#contributing)  Contributing

We love contributions! Please read our [contributing guide](https://github.com/mendableai/firecrawl/blob/main/CONTRIBUTING.md) before submitting a pull request.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/introduction)

[Advanced Scraping Guide](https://docs.firecrawl.dev/v0/advanced-scraping-guide)

On this page

- [Welcome to Firecrawl](https://docs.firecrawl.dev/v0/introduction#welcome-to-firecrawl)
- [How to use it?](https://docs.firecrawl.dev/v0/introduction#how-to-use-it%3F)
- [API Key](https://docs.firecrawl.dev/v0/introduction#api-key)
- [Crawling](https://docs.firecrawl.dev/v0/introduction#crawling)
- [Installation](https://docs.firecrawl.dev/v0/introduction#installation)
- [Usage](https://docs.firecrawl.dev/v0/introduction#usage)
- [Check Crawl Job](https://docs.firecrawl.dev/v0/introduction#check-crawl-job)
- [Response](https://docs.firecrawl.dev/v0/introduction#response)
- [Scraping](https://docs.firecrawl.dev/v0/introduction#scraping)
- [Response](https://docs.firecrawl.dev/v0/introduction#response-2)
- [Extraction](https://docs.firecrawl.dev/v0/introduction#extraction)
- [Contributing](https://docs.firecrawl.dev/v0/introduction#contributing)

![Hero Light](https://docs.firecrawl.dev/v0/introduction)

## Firecrawl Rust SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Rust

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/sdks/rust\#installation)  Installation

To install the Firecrawl Rust SDK, add the following to your `Cargo.toml`:

Rust

Copy

```yaml
# Add this to your Cargo.toml
[dependencies]
firecrawl = "^1.0"
tokio = { version = "^1", features = ["full"] }

```

## [‚Äã](https://docs.firecrawl.dev/sdks/rust\#usage)  Usage

First, you need to obtain an API key from [firecrawl.dev](https://firecrawl.dev/). Then, you need to initialize the `FirecrawlApp`. From there, you can access functions like `FirecrawlApp::scrape_url`, which let you use our API.

Here‚Äôs an example of how to use the SDK in Rust:

Rust

Copy

```rust
use firecrawl::{crawl::{CrawlOptions, CrawlScrapeOptions, CrawlScrapeFormats}, FirecrawlApp, scrape::{ScrapeOptions, ScrapeFormats}};

#[tokio::main]
async fn main() {
    // Initialize the FirecrawlApp with the API key
    let app = FirecrawlApp::new("fc-YOUR_API_KEY").expect("Failed to initialize FirecrawlApp");

    // Scrape a URL
    let options = ScrapeOptions {
        formats vec! [ ScrapeFormats::Markdown, ScrapeFormats::HTML ].into(),
        ..Default::default()
    };

    let scrape_result = app.scrape_url("https://firecrawl.dev", options).await;

    match scrape_result {
        Ok(data) => println!("Scrape Result:\n{}", data.markdown.unwrap()),
        Err(e) => eprintln!("Map failed: {}", e),
    }

    // Crawl a website
    let crawl_options = CrawlOptions {
        scrape_options: CrawlScrapeOptions {
            formats: vec![ CrawlScrapeFormats::Markdown, CrawlScrapeFormats::HTML ].into(),
            ..Default::default()
        }.into(),
        limit: 100.into(),
        ..Default::default()
    };

    let crawl_result = app
        .crawl_url("https://mendable.ai", crawl_options)
        .await;

    match crawl_result {
        Ok(data) => println!("Crawl Result (used {} credits):\n{:#?}", data.credits_used, data.data),
        Err(e) => eprintln!("Crawl failed: {}", e),
    }
}

```

### [‚Äã](https://docs.firecrawl.dev/sdks/rust\#scraping-a-url)  Scraping a URL

To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a `Document`.

Rust

Copy

```rust
let options = ScrapeOptions {
    formats vec! [ ScrapeFormats::Markdown, ScrapeFormats::HTML ].into(),
    ..Default::default()
};

let scrape_result = app.scrape_url("https://firecrawl.dev", options).await;

match scrape_result {
    Ok(data) => println!("Scrape Result:\n{}", data.markdown.unwrap()),
    Err(e) => eprintln!("Map failed: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/sdks/rust\#scraping-with-extract)  Scraping with Extract

With Extract, you can easily extract structured data from any URL. You need to specify your schema in the JSON Schema format, using the `serde_json::json!` macro.

Rust

Copy

```rust
let json_schema = json!({
    "type": "object",
    "properties": {
        "top": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "title": {"type": "string"},
                    "points": {"type": "number"},
                    "by": {"type": "string"},
                    "commentsURL": {"type": "string"}
                },
                "required": ["title", "points", "by", "commentsURL"]
            },
            "minItems": 5,
            "maxItems": 5,
            "description": "Top 5 stories on Hacker News"
        }
    },
    "required": ["top"]
});

let llm_extraction_options = ScrapeOptions {
    formats: vec![ ScrapeFormats::Extract ].into(),
    extract: ExtractOptions {
        schema: json_schema.into(),
        ..Default::default()
    }.into(),
    ..Default::default()
};

let llm_extraction_result = app
    .scrape_url("https://news.ycombinator.com", llm_extraction_options)
    .await;

match llm_extraction_result {
    Ok(data) => println!("LLM Extraction Result:\n{:#?}", data.extract.unwrap()),
    Err(e) => eprintln!("LLM Extraction failed: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/sdks/rust\#crawling-a-website)  Crawling a Website

To crawl a website, use the `crawl_url` method. This will wait for the crawl to complete, which may take a long time based on your starting URL and your options.

Rust

Copy

```rust
let crawl_options = CrawlOptions {
    scrape_options: CrawlScrapeOptions {
        formats: vec![ CrawlScrapeFormats::Markdown, CrawlScrapeFormats::HTML ].into(),
        ..Default::default()
    }.into(),
    limit: 100.into(),
    ..Default::default()
};

let crawl_result = app
    .crawl_url("https://mendable.ai", crawl_options)
    .await;

match crawl_result {
    Ok(data) => println!("Crawl Result (used {} credits):\n{:#?}", data.credits_used, data.data),
    Err(e) => eprintln!("Crawl failed: {}", e),
}

```

#### [‚Äã](https://docs.firecrawl.dev/sdks/rust\#crawling-asynchronously)  Crawling asynchronously

To crawl without waiting for the result, use the `crawl_url_async` method. It takes the same parameters, but it returns a `CrawlAsyncRespone` struct, containing the crawl‚Äôs ID. You can use that ID with the `check_crawl_status` method to check the status at any time. Do note that completed crawls are deleted after 24 hours.

Rust

Copy

```rust
let crawl_id = app.crawl_url_async("https://mendable.ai", None).await?.id;

// ... later ...

let status = app.check_crawl_status(crawl_id).await?;

if status.status == CrawlStatusTypes::Completed {
    println!("Crawl is done: {:#?}", status.data);
} else {
    // ... wait some more ...
}

```

### [‚Äã](https://docs.firecrawl.dev/sdks/rust\#map-a-url)  Map a URL

Map all associated links from a starting URL.

Rust

Copy

```rust
let map_result = app.map_url("https://firecrawl.dev", None).await;

match map_result {
    Ok(data) => println!("Mapped URLs: {:#?}", data),
    Err(e) => eprintln!("Map failed: {}", e),
}

```

## [‚Äã](https://docs.firecrawl.dev/sdks/rust\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and by our dependencies, and combines them into the `FirecrawlError` enum, implementing `Error`, `Debug` and `Display`. All of our methods return a `Result<T, FirecrawlError>`.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/rust.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/rust)

[Go](https://docs.firecrawl.dev/sdks/go)

On this page

- [Installation](https://docs.firecrawl.dev/sdks/rust#installation)
- [Usage](https://docs.firecrawl.dev/sdks/rust#usage)
- [Scraping a URL](https://docs.firecrawl.dev/sdks/rust#scraping-a-url)
- [Scraping with Extract](https://docs.firecrawl.dev/sdks/rust#scraping-with-extract)
- [Crawling a Website](https://docs.firecrawl.dev/sdks/rust#crawling-a-website)
- [Crawling asynchronously](https://docs.firecrawl.dev/sdks/rust#crawling-asynchronously)
- [Map a URL](https://docs.firecrawl.dev/sdks/rust#map-a-url)
- [Error Handling](https://docs.firecrawl.dev/sdks/rust#error-handling)

## Website Crawling Features
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Features

Crawl

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

Firecrawl thoroughly crawls websites, ensuring comprehensive data extraction while bypassing any web blocker mechanisms. Here‚Äôs how it works:

1. **URL Analysis:**
Begins with a specified URL, identifying links by looking at the sitemap and then crawling the website. If no sitemap is found, it will crawl the website following the links.

2. **Recursive Traversal:**
Recursively follows each link to uncover all subpages.

3. **Content Scraping:**
Gathers content from every visited page while handling any complexities like JavaScript rendering or rate limits.

4. **Result Compilation:**
Converts collected data into clean markdown or structured output, perfect for LLM processing or any other task.


This method guarantees an exhaustive crawl and data collection from any starting URL.

## [‚Äã](https://docs.firecrawl.dev/features/crawl\#crawling)  Crawling

### [‚Äã](https://docs.firecrawl.dev/features/crawl\#%2Fcrawl-endpoint)  /crawl endpoint

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

By default - Crawl will ignore sublinks of a page if they aren‚Äôt children of the url you provide. So, the website.com/other-parent/blog-1 wouldn‚Äôt be returned if you crawled website.com/blogs/. If you want website.com/other-parent/blog-1, use the `allowBackwardLinks` parameter

### [‚Äã](https://docs.firecrawl.dev/features/crawl\#installation)  Installation

Python

Node

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/features/crawl\#usage)  Usage

Python

Node

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Crawl a website:
crawl_status = app.crawl_url(
  'https://firecrawl.dev',
  params={
    'limit': 100,
    'scrapeOptions': {'formats': ['markdown', 'html']}
  },
  poll_interval=30
)
print(crawl_status)

```

### [‚Äã](https://docs.firecrawl.dev/features/crawl\#response)  Response

If you‚Äôre using cURL or `async crawl` functions on SDKs, this will return an `ID` where you can use to check the status of the crawl.

Copy

```json
{
  "success": true,
  "id": "123-456-789",
  "url": "https://api.firecrawl.dev/v1/crawl/123-456-789"
}

```

### [‚Äã](https://docs.firecrawl.dev/features/crawl\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

This endpoint only works for crawls that are in progress or crawls that have completed recently.

Python

Node

Go

Rust

cURL

Copy

```python
crawl_status = app.check_crawl_status("<crawl_id>")
print(crawl_status)

```

#### [‚Äã](https://docs.firecrawl.dev/features/crawl\#response-handling)  Response Handling

The response varies based on the crawl‚Äôs status.

For not completed or large responses exceeding 10MB, a `next` URL parameter is provided. You must request this URL to retrieve the next 10MB of data. If the `next` parameter is absent, it indicates the end of the crawl data.

The skip parameter sets the maximum number of results returned for each chunk of results returned.

The skip and next parameter are only relavent when hitting the api directly. If you‚Äôre using the SDK, we handle this for you and will return all the results at once.

Scraping

Completed

Copy

```json
{
  "status": "scraping",
  "total": 36,
  "completed": 10,
  "creditsUsed": 10,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "next": "https://api.firecrawl.dev/v1/crawl/123-456-789?skip=10",
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/features/crawl\#crawl-websocket)  Crawl WebSocket\
\
Firecrawl‚Äôs WebSocket-based method, `Crawl URL and Watch`, enables real-time data extraction and monitoring. Start a crawl with a URL and customize it with options like page limits, allowed domains, and output formats, ideal for immediate data processing needs.\
\
Python\
\
Node\
\
Copy\
\
```python\
# inside an async function...\
nest_asyncio.apply()\
\
# Define event handlers\
def on_document(detail):\
    print("DOC", detail)\
\
def on_error(detail):\
    print("ERR", detail['error'])\
\
def on_done(detail):\
    print("DONE", detail['status'])\
\
    # Function to start the crawl and watch process\
async def start_crawl_and_watch():\
    # Initiate the crawl job and get the watcher\
    watcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })\
\
    # Add event listeners\
    watcher.add_event_listener("document", on_document)\
    watcher.add_event_listener("error", on_error)\
    watcher.add_event_listener("done", on_done)\
\
    # Start the watcher\
    await watcher.connect()\
\
# Run the event loop\
await start_crawl_and_watch()\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/features/crawl\#crawl-webhook)  Crawl Webhook\
\
You can now pass a `webhook` parameter to the `/crawl` endpoint. This will send a POST request to the URL you specify when the crawl is started, updated and completed.\
\
The webhook will now trigger for every page crawled and not just the whole result at the end.\
\
cURL\
\
Copy\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/crawl \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev",\
      "limit": 100,\
      "webhook": "https://example.com/webhook"\
    }'\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/features/crawl\#webhook-events)  Webhook Events\
\
There are now 4 types of events:\
\
- `crawl.started` \- Triggered when the crawl is started.\
- `crawl.page` \- Triggered for every page crawled.\
- `crawl.completed` \- Triggered when the crawl is completed to let you know it‚Äôs done (Beta)\*\*\
- `crawl.failed` \- Triggered when the crawl fails.\
\
### [‚Äã](https://docs.firecrawl.dev/features/crawl\#webhook-response)  Webhook Response\
\
- `success` \- If the webhook was successful in crawling the page correctly.\
- `type` \- The type of event that occurred.\
- `id` \- The ID of the crawl.\
- `data` \- The data that was scraped (Array). This will only be non empty on `crawl.page` and will contain 1 item if the page was scraped successfully. The response is the same as the `/scrape` endpoint.\
- `error` \- If the webhook failed, this will contain the error message.\
\
\*\*Beta consideration\
\
- There is a very tiny chance that the `crawl.completed` event may be triggered while the final `crawl.page` events are still being processed. We‚Äôre working on a fix for this.\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/crawl.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/crawl)\
\
[LLM Extract](https://docs.firecrawl.dev/features/llm-extract) [Map](https://docs.firecrawl.dev/features/map)\
\
On this page\
\
- [Crawling](https://docs.firecrawl.dev/features/crawl#crawling)\
- [/crawl endpoint](https://docs.firecrawl.dev/features/crawl#%2Fcrawl-endpoint)\
- [Installation](https://docs.firecrawl.dev/features/crawl#installation)\
- [Usage](https://docs.firecrawl.dev/features/crawl#usage)\
- [Response](https://docs.firecrawl.dev/features/crawl#response)\
- [Check Crawl Job](https://docs.firecrawl.dev/features/crawl#check-crawl-job)\
- [Response Handling](https://docs.firecrawl.dev/features/crawl#response-handling)\
- [Crawl WebSocket](https://docs.firecrawl.dev/features/crawl#crawl-websocket)\
- [Crawl Webhook](https://docs.firecrawl.dev/features/crawl#crawl-webhook)\
- [Webhook Events](https://docs.firecrawl.dev/features/crawl#webhook-events)\
- [Webhook Response](https://docs.firecrawl.dev/features/crawl#webhook-response)

## Firecrawl Node SDK Guide
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Node

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/sdks/node\#installation)  Installation

To install the Firecrawl Node SDK, you can use npm:

Node

```bash
npm install @mendable/firecrawl-js

```

## [‚Äã](https://docs.firecrawl.dev/sdks/node\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.

Here‚Äôs an example of how to use the SDK with error handling:

Node

```js
import FirecrawlApp, { CrawlParams, CrawlStatusResponse } from '@mendable/firecrawl-js';

const app = new FirecrawlApp({apiKey: "fc-YOUR_API_KEY"});

// Scrape a website
const scrapeResponse = await app.scrapeUrl('https://firecrawl.dev', {
  formats: ['markdown', 'html'],
});

if (!scrapeResponse.success) {
  throw new Error(`Failed to scrape: ${scrapeResponse.error}`)
}

console.log(scrapeResponse)

// Crawl a website
const crawlResponse = await app.crawlUrl('https://firecrawl.dev', {
  limit: 100,
  scrapeOptions: {
    formats: ['markdown', 'html'],
  }
});

if (!crawlResponse.success) {
  throw new Error(`Failed to crawl: ${crawlResponse.error}`)
}

console.log(crawlResponse)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#scraping-a-url)  Scraping a URL

To scrape a single URL with error handling, use the `scrapeUrl` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Node

```js
// Scrape a website:
const scrapeResult = await app.scrapeUrl('firecrawl.dev', { formats: ['markdown', 'html'] });

if (!scrapeResult.success) {
  throw new Error(`Failed to scrape: ${scrapeResult.error}`)
}

console.log(scrapeResult)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#crawling-a-website)  Crawling a Website

To crawl a website with error handling, use the `crawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Node

```js
const crawlResponse = await app.crawlUrl('https://firecrawl.dev', {
  limit: 100,
  scrapeOptions: {
    formats: ['markdown', 'html'],
  }
})

if (!crawlResponse.success) {
  throw new Error(`Failed to crawl: ${crawlResponse.error}`)
}

console.log(crawlResponse)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#asynchronous-crawling)  Asynchronous Crawling

To crawl a website asynchronously, use the `crawlUrlAsync` method. It returns the crawl `ID` which you can use to check the status of the crawl job. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Node

```js
const crawlResponse = await app.asyncCrawlUrl('https://firecrawl.dev', {
  limit: 100,
  scrapeOptions: {
    formats: ['markdown', 'html'],
  }
});

if (!crawlResponse.success) {
  throw new Error(`Failed to crawl: ${crawlResponse.error}`)
}

console.log(crawlResponse)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job with error handling, use the `checkCrawlStatus` method. It takes the `ID` as a parameter and returns the current status of the crawl job.

Node

```js
const crawlResponse = await app.checkCrawlStatus("<crawl_id>");

if (!crawlResponse.success) {
  throw new Error(`Failed to check crawl status: ${crawlResponse.error}`)
}

console.log(crawlResponse)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#cancelling-a-crawl)  Cancelling a Crawl

To cancel an asynchronous crawl job, use the `cancelCrawl` method. It takes the job ID of the asynchronous crawl as a parameter and returns the cancellation status.

Node

```js
const cancelCrawl = await app.cancelCrawl(id);
console.log(cancelCrawl)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#mapping-a-website)  Mapping a Website

To map a website with error handling, use the `mapUrl` method. It takes the starting URL as a parameter and returns the mapped data as a dictionary.

Node

```js
const mapResult = await app.mapUrl('https://firecrawl.dev');

if (!mapResult.success) {
  throw new Error(`Failed to map: ${mapResult.error}`)
}

console.log(mapResult)

```

### [‚Äã](https://docs.firecrawl.dev/sdks/node\#crawling-a-website-with-websockets)  Crawling a Website with WebSockets

To crawl a website with WebSockets, use the `crawlUrlAndWatch` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Node

```js
const watch = await app.crawlUrlAndWatch('mendable.ai', { excludePaths: ['blog/*'], limit: 5});

watch.addEventListener("document", doc => {
  console.log("DOC", doc.detail);
});

watch.addEventListener("error", err => {
  console.error("ERR", err.detail.error);
});

watch.addEventListener("done", state => {
  console.log("DONE", state.detail.status);
});

```

## [‚Äã](https://docs.firecrawl.dev/sdks/node\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message. The examples above demonstrate how to handle these errors using `try/catch` blocks.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/node.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/node)

[Python](https://docs.firecrawl.dev/sdks/python) [Go](https://docs.firecrawl.dev/sdks/go)

On this page

- [Installation](https://docs.firecrawl.dev/sdks/node#installation)
- [Usage](https://docs.firecrawl.dev/sdks/node#usage)
- [Scraping a URL](https://docs.firecrawl.dev/sdks/node#scraping-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/sdks/node#crawling-a-website)
- [Asynchronous Crawling](https://docs.firecrawl.dev/sdks/node#asynchronous-crawling)
- [Checking Crawl Status](https://docs.firecrawl.dev/sdks/node#checking-crawl-status)
- [Cancelling a Crawl](https://docs.firecrawl.dev/sdks/node#cancelling-a-crawl)
- [Mapping a Website](https://docs.firecrawl.dev/sdks/node#mapping-a-website)
- [Crawling a Website with WebSockets](https://docs.firecrawl.dev/sdks/node#crawling-a-website-with-websockets)
- [Error Handling](https://docs.firecrawl.dev/sdks/node#error-handling)

## Website Mapping Tool
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Features

Map

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/features/map\#introducing-%2Fmap)  Introducing /map

The easiest way to go from a single url to a map of the entire website. This is extremely useful for:

- When you need to prompt the end-user to choose which links to scrape
- Need to quickly know the links on a website
- Need to scrape pages of a website that are related to a specific topic (use the `search` parameter)
- Only need to scrape specific pages of a website

## [‚Äã](https://docs.firecrawl.dev/features/map\#alpha-considerations)  Alpha Considerations

This endpoint prioritizes speed, so it may not capture all website links. We are working on improvements. Feedback and suggestions are very welcome.

## [‚Äã](https://docs.firecrawl.dev/features/map\#mapping)  Mapping

### [‚Äã](https://docs.firecrawl.dev/features/map\#%2Fmap-endpoint)  /map endpoint

Used to map a URL and get urls of the website. This returns most links present on the website.

### [‚Äã](https://docs.firecrawl.dev/features/map\#installation)  Installation

Python

Node

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/features/map\#usage)  Usage

Python

Node

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Map a website:
map_result = app.map_url('https://firecrawl.dev')
print(map_result)

```

### [‚Äã](https://docs.firecrawl.dev/features/map\#response)  Response

SDKs will return the data object directly. cURL will return the payload exactly as shown below.

Copy

```json
{
  "status": "success",
  "links": [\
    "https://firecrawl.dev",\
    "https://www.firecrawl.dev/pricing",\
    "https://www.firecrawl.dev/blog",\
    "https://www.firecrawl.dev/playground",\
    "https://www.firecrawl.dev/smart-crawl",\
    ...\
  ]
}

```

#### [‚Äã](https://docs.firecrawl.dev/features/map\#map-with-search)  Map with search

Map with `search` param allows you to search for specific urls inside a website.

cURL

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://firecrawl.dev",
      "search": "docs"
    }'

```

Response will be an ordered list from the most relevant to the least relevant.

Copy

```json
{
  "status": "success",
  "links": [\
    "https://docs.firecrawl.dev",\
    "https://docs.firecrawl.dev/sdks/python",\
    "https://docs.firecrawl.dev/learn/rag-llama3",\
  ]
}

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/map.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/map)

[Crawl](https://docs.firecrawl.dev/features/crawl) [Extract (New)](https://docs.firecrawl.dev/features/extract)

On this page

- [Introducing /map](https://docs.firecrawl.dev/features/map#introducing-%2Fmap)
- [Alpha Considerations](https://docs.firecrawl.dev/features/map#alpha-considerations)
- [Mapping](https://docs.firecrawl.dev/features/map#mapping)
- [/map endpoint](https://docs.firecrawl.dev/features/map#%2Fmap-endpoint)
- [Installation](https://docs.firecrawl.dev/features/map#installation)
- [Usage](https://docs.firecrawl.dev/features/map#usage)
- [Response](https://docs.firecrawl.dev/features/map#response)
- [Map with search](https://docs.firecrawl.dev/features/map#map-with-search)

## Firecrawl Flowise Integration
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

Flowise

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/integrations/flowise\#sync-web-data-in-flowise-workflows)  Sync web data in Flowise workflows

Firecrawl can be used inside of [Flowise the Chatflow builder](https://flowiseai.com/). This page introduces how to configure and use a Firecrawl block inside of Flowise.

![Firecrawl Flowise block](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/fc_flowise_block.png)

### [‚Äã](https://docs.firecrawl.dev/integrations/flowise\#crawling-with-firecrawl-blocks)  Crawling with Firecrawl blocks

1. Log in to your Firecrawl account and get your API Key, and then enter it on the block.
2. (Optional) Connect Text Splitter.
3. Select the crawl mode to pick up a crawl pages below the target url.
4. Input target URL to be crawled.
5. Use the resulting documents in your workflows.

### [‚Äã](https://docs.firecrawl.dev/integrations/flowise\#scraping-with-firecrawl-blocks)  Scraping with Firecrawl blocks

1. Log in to your Firecrawl account and get your API Key, and then enter it on the block.
2. (Optional) Connect Text Splitter.
3. Select the scrape mode to pick up a single page.
4. Input target URL to be scraped.
5. Use the resulting documents in your workflows.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/flowise.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/flowise)

[Dify](https://docs.firecrawl.dev/integrations/dify) [Langflow](https://docs.firecrawl.dev/integrations/langflow)

On this page

- [Sync web data in Flowise workflows](https://docs.firecrawl.dev/integrations/flowise#sync-web-data-in-flowise-workflows)
- [Crawling with Firecrawl blocks](https://docs.firecrawl.dev/integrations/flowise#crawling-with-firecrawl-blocks)
- [Scraping with Firecrawl blocks](https://docs.firecrawl.dev/integrations/flowise#scraping-with-firecrawl-blocks)

![Firecrawl Flowise block](https://docs.firecrawl.dev/integrations/flowise)

## Web Scraping Features
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Scrape

Scrape

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

Firecrawl converts web pages into markdown, ideal for LLM applications.

- It manages complexities: proxies, caching, rate limits, js-blocked content
- Handles dynamic content: dynamic websites, js-rendered sites, PDFs, images
- Outputs clean markdown, structured data, screenshots or html.

For details, see the [Scrape Endpoint API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

## [‚Äã](https://docs.firecrawl.dev/features/scrape\#scraping-a-url-with-firecrawl)  Scraping a URL with Firecrawl

### [‚Äã](https://docs.firecrawl.dev/features/scrape\#%2Fscrape-endpoint)  /scrape endpoint

Used to scrape a URL and get its content.

### [‚Äã](https://docs.firecrawl.dev/features/scrape\#installation)  Installation

Python

Node

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/features/scrape\#usage)  Usage

Python

Node

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})
print(scrape_result)

```

For more details about the parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

### [‚Äã](https://docs.firecrawl.dev/features/scrape\#response)  Response

SDKs will return the data object directly. cURL will return the payload exactly as shown below.

Copy

```json
{
  "success": true,
  "data" : {
    "markdown": "Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...",\
    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "https://firecrawl.dev",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/features/scrape\#scrape-formats)  Scrape Formats\
\
You can now choose what formats you want your output in. You can specify multiple output formats. Supported formats are:\
\
- Markdown (markdown)\
- HTML (html)\
- Raw HTML (rawHtml) (with no modifications)\
- Screenshot (screenshot or screenshot@fullPage)\
- Links (links)\
- Extract (extract) - structured output\
\
Output keys will match the format you choose.\
\
## [‚Äã](https://docs.firecrawl.dev/features/scrape\#extract-structured-data)  Extract structured data\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#%2Fscrape-with-extract-endpoint)  /scrape (with extract) endpoint\
\
Used to extract structured data from scraped pages.\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
from pydantic import BaseModel, Field\
\
# Initialize the FirecrawlApp with your API key\
app = FirecrawlApp(api_key='your_api_key')\
\
class ExtractSchema(BaseModel):\
    company_mission: str\
    supports_sso: bool\
    is_open_source: bool\
    is_in_yc: bool\
\
data = app.scrape_url('https://docs.firecrawl.dev/', {\
    'formats': ['json'],\
    'jsonOptions': {\
        'schema': ExtractSchema.model_json_schema(),\
    }\
})\
print(data["json"])\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
        "supports_sso": true,\
        "is_open_source": false,\
        "is_in_yc": true\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#extracting-without-schema-new)  Extracting without schema (New)\
\
You can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\
\
cURL\
\
Copy\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/scrape \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev/",\
      "formats": ["json"],\
      "jsonOptions": {\
        "prompt": "Extract the company mission from the page."\
      }\
    }'\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#extract-object)  Extract object\
\
The `extract` object accepts the following parameters:\
\
- `schema`: The schema to use for the extraction.\
- `systemPrompt`: The system prompt to use for the extraction.\
- `prompt`: The prompt to use for the extraction without a schema.\
\
## [‚Äã](https://docs.firecrawl.dev/features/scrape\#interacting-with-the-page-with-actions)  Interacting with the page with Actions\
\
Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.\
\
Here is an example of how to use actions to navigate to google.com, search for Firecrawl, click on the first result, and take a screenshot.\
\
It is important to almost always use the `wait` action before/after executing other actions to give enough time for the page to load.\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#example)  Example\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('firecrawl.dev',\
    params={\
        'formats': ['markdown', 'html'],\
        'actions': [\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "click", "selector": "textarea[title=\"Search\"]"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "write", "text": "firecrawl"},\
            {"type": "wait", "milliseconds": 2000},\
            {"type": "press", "key": "ENTER"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "click", "selector": "h3"},\
            {"type": "wait", "milliseconds": 3000},\
            {"type": "scrape"},\
            {"type": "screenshot"}\
        ]\
    }\
)\
print(scrape_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#output)  Output\
\
JSON\
\
Copy\
\
```json\
{\
  "success": true,\
  "data": {\
    "markdown": "Our first Launch Week is over! [See the recap üöÄ](blog/firecrawl-launch-week-1-recap)...",\
    "actions": {\
      "screenshots": [\
        "https://alttmdsdujxrfnakrkyi.supabase.co/storage/v1/object/public/media/screenshot-75ef2d87-31e0-4349-a478-fb432a29e241.png"\
      ],\
      "scrapes": [\
        {\
          "url": "https://www.firecrawl.dev/",\
          "html": "<html><body><h1>Firecrawl</h1></body></html>"\
        }\
      ]\
    },\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "http://google.com",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
For more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).\
\
## [‚Äã](https://docs.firecrawl.dev/features/scrape\#location-and-language)  Location and Language\
\
Specify country and preferred languages to get relevant content based on your target location and language preferences.\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#how-it-works)  How it works\
\
When you specify the location settings, Firecrawl will use an appropriate proxy if available and emulate the corresponding language and timezone settings. By default, the location is set to ‚ÄòUS‚Äô if not specified.\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#usage-2)  Usage\
\
To use the location and language settings, include the `location` object in your request body with the following properties:\
\
- `country`: ISO 3166-1 alpha-2 country code (e.g., ‚ÄòUS‚Äô, ‚ÄòAU‚Äô, ‚ÄòDE‚Äô, ‚ÄòJP‚Äô). Defaults to ‚ÄòUS‚Äô.\
- `languages`: An array of preferred languages and locales for the request in order of priority. Defaults to the language of the specified location.\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape a website:\
scrape_result = app.scrape_url('airbnb.com',\
    params={\
        'formats': ['markdown', 'html'],\
        'location': {\
            'country': 'BR',\
            'languages': ['pt-BR']\
        }\
    }\
)\
print(scrape_result)\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/features/scrape\#batch-scraping-multiple-urls)  Batch scraping multiple URLs\
\
You can now batch scrape multiple URLs at the same time. It takes the starting URLs and optional parameters as arguments. The params argument allows you to specify additional options for the batch scrape job, such as the output formats.\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#how-it-works-2)  How it works\
\
It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.\
\
The sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#usage-3)  Usage\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape multiple websites:\
batch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\
print(batch_scrape_result)\
\
# Or, you can use the asynchronous method:\
batch_scrape_job = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})\
print(batch_scrape_job)\
\
# (async) You can then use the job ID to check the status of the batch scrape:\
batch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])\
print(batch_scrape_status)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/features/scrape\#response-2)  Response\
\
If you‚Äôre using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.\
\
#### [‚Äã](https://docs.firecrawl.dev/features/scrape\#synchronous)  Synchronous\
\
Completed\
\
Copy\
\
```json\
{\
  "status": "completed",\
  "total": 36,\
  "completed": 36,\
  "creditsUsed": 36,\
  "expiresAt": "2024-00-00T00:00:00.000Z",\
  "next": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26",\
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
#### [‚Äã](https://docs.firecrawl.dev/features/scrape\#asynchronous)  Asynchronous\
\
You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\
\
Copy\
\
```json\
{\
  "success": true,\
  "id": "123-456-789",\
  "url": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789"\
}\
\
```\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/scrape)\
\
[Advanced Scraping Guide](https://docs.firecrawl.dev/advanced-scraping-guide) [Batch Scrape](https://docs.firecrawl.dev/features/batch-scrape)\
\
On this page\
\
- [Scraping a URL with Firecrawl](https://docs.firecrawl.dev/features/scrape#scraping-a-url-with-firecrawl)\
- [/scrape endpoint](https://docs.firecrawl.dev/features/scrape#%2Fscrape-endpoint)\
- [Installation](https://docs.firecrawl.dev/features/scrape#installation)\
- [Usage](https://docs.firecrawl.dev/features/scrape#usage)\
- [Response](https://docs.firecrawl.dev/features/scrape#response)\
- [Scrape Formats](https://docs.firecrawl.dev/features/scrape#scrape-formats)\
- [Extract structured data](https://docs.firecrawl.dev/features/scrape#extract-structured-data)\
- [/scrape (with extract) endpoint](https://docs.firecrawl.dev/features/scrape#%2Fscrape-with-extract-endpoint)\
- [Extracting without schema (New)](https://docs.firecrawl.dev/features/scrape#extracting-without-schema-new)\
- [Extract object](https://docs.firecrawl.dev/features/scrape#extract-object)\
- [Interacting with the page with Actions](https://docs.firecrawl.dev/features/scrape#interacting-with-the-page-with-actions)\
- [Example](https://docs.firecrawl.dev/features/scrape#example)\
- [Output](https://docs.firecrawl.dev/features/scrape#output)\
- [Location and Language](https://docs.firecrawl.dev/features/scrape#location-and-language)\
- [How it works](https://docs.firecrawl.dev/features/scrape#how-it-works)\
- [Usage](https://docs.firecrawl.dev/features/scrape#usage-2)\
- [Batch scraping multiple URLs](https://docs.firecrawl.dev/features/scrape#batch-scraping-multiple-urls)\
- [How it works](https://docs.firecrawl.dev/features/scrape#how-it-works-2)\
- [Usage](https://docs.firecrawl.dev/features/scrape#usage-3)\
- [Response](https://docs.firecrawl.dev/features/scrape#response-2)\
- [Synchronous](https://docs.firecrawl.dev/features/scrape#synchronous)\
- [Asynchronous](https://docs.firecrawl.dev/features/scrape#asynchronous)

## Firecrawl V1 API
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Welcome to V1

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

Firecrawl V1 is here! With that we introduce a more reliable and developer friendly API.

Here is what‚Äôs new:

- Output Formats for `/scrape`. Choose what formats you want your output in.
- New [`/map` endpoint](https://docs.firecrawl.dev/features/map) for getting most of the URLs of a webpage.
- Developer friendly API for `/crawl/{id}` status.
- 2x Rate Limits for all plans.
- [Go SDK](https://docs.firecrawl.dev/sdks/go) and [Rust SDK](https://docs.firecrawl.dev/sdks/rust)
- Teams support
- API Key Management in the dashboard.
- `onlyMainContent` is now default to `true`.
- `/crawl` webhooks and websocket support.

## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#scrape-formats)  Scrape Formats

You can now choose what formats you want your output in. You can specify multiple output formats. Supported formats are:

- Markdown (markdown)
- HTML (html)
- Raw HTML (rawHtml) (with no modifications)
- Screenshot (screenshot or screenshot@fullPage)
- Links (links)
- Extract (extract) - structured output

Output keys will match the format you choose.

Python

Node

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape a website:
scrape_result = app.scrape_url('firecrawl.dev', params={'formats': ['markdown', 'html']})
print(scrape_result)

```

### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#response)  Response

SDKs will return the data object directly. cURL will return the payload exactly as shown below.

Copy

```json
{
  "success": true,
  "data" : {
    "markdown": "Launch Week I is here! [See our Day 2 Release üöÄ](https://www.firecrawl.dev/blog/launch-week-i-day-2-doubled-rate-limits)[üí• Get 2 months free...",\
    "html": "<!DOCTYPE html><html lang=\"en\" class=\"light\" style=\"color-scheme: light;\"><body class=\"__variable_36bd41 __variable_d7dc5d font-inter ...",\
    "metadata": {\
      "title": "Home - Firecrawl",\
      "description": "Firecrawl crawls and converts any website into clean markdown.",\
      "language": "en",\
      "keywords": "Firecrawl,Markdown,Data,Mendable,Langchain",\
      "robots": "follow, index",\
      "ogTitle": "Firecrawl",\
      "ogDescription": "Turn any website into LLM-ready data.",\
      "ogUrl": "https://www.firecrawl.dev/",\
      "ogImage": "https://www.firecrawl.dev/og.png?123",\
      "ogLocaleAlternate": [],\
      "ogSiteName": "Firecrawl",\
      "sourceURL": "https://firecrawl.dev",\
      "statusCode": 200\
    }\
  }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#introducing-%2Fmap-alpha)  Introducing /map (Alpha)\
\
The easiest way to go from a single url to a map of the entire website.\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#usage)  Usage\
\
Python\
\
Node\
\
Go\
\
Rust\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Map a website:\
map_result = app.map_url('https://firecrawl.dev')\
print(map_result)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#response-2)  Response\
\
SDKs will return the data object directly. cURL will return the payload exactly as shown below.\
\
Copy\
\
```json\
{\
  "status": "success",\
  "links": [\
    "https://firecrawl.dev",\
    "https://www.firecrawl.dev/pricing",\
    "https://www.firecrawl.dev/blog",\
    "https://www.firecrawl.dev/playground",\
    "https://www.firecrawl.dev/smart-crawl",\
    ...\
  ]\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#websockets)  WebSockets\
\
To crawl a website with WebSockets, use the `Crawl URL and Watch` method.\
\
Python\
\
Node\
\
Copy\
\
```python\
# inside an async function...\
nest_asyncio.apply()\
\
# Define event handlers\
def on_document(detail):\
    print("DOC", detail)\
\
def on_error(detail):\
    print("ERR", detail['error'])\
\
def on_done(detail):\
    print("DONE", detail['status'])\
\
    # Function to start the crawl and watch process\
async def start_crawl_and_watch():\
    # Initiate the crawl job and get the watcher\
    watcher = app.crawl_url_and_watch('firecrawl.dev', { 'excludePaths': ['blog/*'], 'limit': 5 })\
\
    # Add event listeners\
    watcher.add_event_listener("document", on_document)\
    watcher.add_event_listener("error", on_error)\
    watcher.add_event_listener("done", on_done)\
\
    # Start the watcher\
    await watcher.connect()\
\
# Run the event loop\
await start_crawl_and_watch()\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#extract-format)  Extract format\
\
LLM extraction is now available in v1 under the `extract` format. To extract structured from a page, you can pass a schema to the endpoint or just provide a prompt.\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
from pydantic import BaseModel, Field\
\
# Initialize the FirecrawlApp with your API key\
app = FirecrawlApp(api_key='your_api_key')\
\
class ExtractSchema(BaseModel):\
    company_mission: str\
    supports_sso: bool\
    is_open_source: bool\
    is_in_yc: bool\
\
data = app.scrape_url('https://docs.firecrawl.dev/', {\
    'formats': ['json'],\
    'jsonOptions': {\
        'schema': ExtractSchema.model_json_schema(),\
    }\
})\
print(data["json"])\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
        "supports_sso": true,\
        "is_open_source": false,\
        "is_in_yc": true\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#extracting-without-schema-new)  Extracting without schema (New)\
\
You can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.\
\
cURL\
\
Copy\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/scrape \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev/",\
      "formats": ["json"],\
      "jsonOptions": {\
        "prompt": "Extract the company mission from the page."\
      }\
    }'\
\
```\
\
Output:\
\
JSON\
\
Copy\
\
```json\
{\
    "success": true,\
    "data": {\
      "json": {\
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",\
      },\
      "metadata": {\
        "title": "Mendable",\
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "robots": "follow, index",\
        "ogTitle": "Mendable",\
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",\
        "ogUrl": "https://docs.firecrawl.dev/",\
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",\
        "ogLocaleAlternate": [],\
        "ogSiteName": "Mendable",\
        "sourceURL": "https://docs.firecrawl.dev/"\
      },\
    }\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#new-crawl-webhook)  New Crawl Webhook\
\
You can now pass a `webhook` parameter to the `/crawl` endpoint. This will send a POST request to the URL you specify when the crawl is started, updated and completed.\
\
The webhook will now trigger for every page crawled and not just the whole result at the end.\
\
cURL\
\
Copy\
\
```bash\
curl -X POST https://api.firecrawl.dev/v1/crawl \\
    -H 'Content-Type: application/json' \\
    -H 'Authorization: Bearer YOUR_API_KEY' \\
    -d '{\
      "url": "https://docs.firecrawl.dev",\
      "limit": 100,\
      "webhook": "https://example.com/webhook"\
    }'\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#webhook-events)  Webhook Events\
\
There are now 4 types of events:\
\
- `crawl.started` \- Triggered when the crawl is started.\
- `crawl.page` \- Triggered for every page crawled.\
- `crawl.completed` \- Triggered when the crawl is completed to let you know it‚Äôs done.\
- `crawl.failed` \- Triggered when the crawl fails.\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#webhook-response)  Webhook Response\
\
- `success` \- If the webhook was successful in crawling the page correctly.\
- `type` \- The type of event that occurred.\
- `id` \- The ID of the crawl.\
- `data` \- The data that was scraped (Array). This will only be non empty on `crawl.page` and will contain 1 item if the page was scraped successfully. The response is the same as the `/scrape` endpoint.\
- `error` \- If the webhook failed, this will contain the error message.\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#migrating-from-v0)  Migrating from V0\
\
> ‚ö†Ô∏è **Deprecation Notice**: V0 endpoints will be deprecated on April 1st, 2025. Please migrate to V1 endpoints before then to ensure uninterrupted service.\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#%2Fscrape-endpoint)  /scrape endpoint\
\
The updated `/scrape` endpoint has been redesigned for enhanced reliability and ease of use. The structure of the new `/scrape` request body is as follows:\
\
Copy\
\
```json\
{\
  "url": "<string>",\
  "formats": ["markdown", "html", "rawHtml", "links", "screenshot", "json"],\
  "includeTags": ["<string>"],\
  "excludeTags": ["<string>"],\
  "headers": { "<key>": "<value>" },\
  "waitFor": 123,\
  "timeout": 123\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#formats)  Formats\
\
You can now choose what formats you want your output in. You can specify multiple output formats. Supported formats are:\
\
- Markdown (markdown)\
- HTML (html)\
- Raw HTML (rawHtml) (with no modifications)\
- Screenshot (screenshot or screenshot@fullPage)\
- Links (links)\
- JSON (json)\
\
By default, the output will be include only the markdown format.\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#details-on-the-new-request-body)  Details on the new request body\
\
The table below outlines the changes to the request body parameters for the `/scrape` endpoint in V1.\
\
| Parameter | Change | Description |\
| --- | --- | --- |\
| `onlyIncludeTags` | Moved and Renamed | Moved to root level. And renamed to `includeTags`. |\
| `removeTags` | Moved and Renamed | Moved to root level. And renamed to `excludeTags`. |\
| `onlyMainContent` | Moved | Moved to root level. `true` by default. |\
| `waitFor` | Moved | Moved to root level. |\
| `headers` | Moved | Moved to root level. |\
| `parsePDF` | Moved | Moved to root level. |\
| `extractorOptions` | No Change |  |\
| `timeout` | No Change |  |\
| `pageOptions` | Removed | No need for `pageOptions` parameter. The scrape options were moved to root level. |\
| `replaceAllPathsWithAbsolutePaths` | Removed | `replaceAllPathsWithAbsolutePaths` is not needed anymore. Every path is now default to absolute path. |\
| `includeHtml` | Removed | add `"html"` to `formats` instead. |\
| `includeRawHtml` | Removed | add `"rawHtml"` to `formats` instead. |\
| `screenshot` | Removed | add `"screenshot"` to `formats` instead. |\
| `fullPageScreenshot` | Removed | add `"screenshot@fullPage"` to `formats` instead. |\
| `extractorOptions` | Removed | Use `"extract"` format instead with `extract` object. |\
\
The new `extract` format is described in the [llm-extract](https://docs.firecrawl.dev/features/extract) section.\
\
## [‚Äã](https://docs.firecrawl.dev/v1-welcome\#%2Fcrawl-endpoint)  /crawl endpoint\
\
We‚Äôve also updated the `/crawl` endpoint on `v1`. Check out the improved body request below:\
\
Copy\
\
```json\
{\
  "url": "<string>",\
  "excludePaths": ["<string>"],\
  "includePaths": ["<string>"],\
  "maxDepth": 2,\
  "ignoreSitemap": true,\
  "limit": 10,\
  "allowBackwardLinks": true,\
  "allowExternalLinks": true,\
  "scrapeOptions": {\
    // same options as in /scrape\
    "formats": ["markdown", "html", "rawHtml", "screenshot", "links"],\
    "headers": { "<key>": "<value>" },\
    "includeTags": ["<string>"],\
    "excludeTags": ["<string>"],\
    "onlyMainContent": true,\
    "waitFor": 123\
  }\
}\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/v1-welcome\#details-on-the-new-request-body-2)  Details on the new request body\
\
The table below outlines the changes to the request body parameters for the `/crawl` endpoint in V1.\
\
| Parameter | Change | Description |\
| --- | --- | --- |\
| `pageOptions` | Renamed | Renamed to `scrapeOptions`. |\
| `includes` | Moved and Renamed | Moved to root level. Renamed to `includePaths`. |\
| `excludes` | Moved and Renamed | Moved to root level. Renamed to `excludePaths`. |\
| `allowBackwardCrawling` | Moved and Renamed | Moved to root level. Renamed to `allowBackwardLinks`. |\
| `allowExternalLinks` | Moved | Moved to root level. |\
| `maxDepth` | Moved | Moved to root level. |\
| `ignoreSitemap` | Moved | Moved to root level. |\
| `limit` | Moved | Moved to root level. |\
| `crawlerOptions` | Removed | No need for `crawlerOptions` parameter. The crawl options were moved to root level. |\
| `timeout` | Removed | Use `timeout` in `scrapeOptions` instead. |\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v1-welcome.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v1-welcome)\
\
[Launch Week II (New)](https://docs.firecrawl.dev/launch-week) [Rate Limits](https://docs.firecrawl.dev/rate-limits)\
\
On this page\
\
- [Scrape Formats](https://docs.firecrawl.dev/v1-welcome#scrape-formats)\
- [Response](https://docs.firecrawl.dev/v1-welcome#response)\
- [Introducing /map (Alpha)](https://docs.firecrawl.dev/v1-welcome#introducing-%2Fmap-alpha)\
- [Usage](https://docs.firecrawl.dev/v1-welcome#usage)\
- [Response](https://docs.firecrawl.dev/v1-welcome#response-2)\
- [WebSockets](https://docs.firecrawl.dev/v1-welcome#websockets)\
- [Extract format](https://docs.firecrawl.dev/v1-welcome#extract-format)\
- [Extracting without schema (New)](https://docs.firecrawl.dev/v1-welcome#extracting-without-schema-new)\
- [New Crawl Webhook](https://docs.firecrawl.dev/v1-welcome#new-crawl-webhook)\
- [Webhook Events](https://docs.firecrawl.dev/v1-welcome#webhook-events)\
- [Webhook Response](https://docs.firecrawl.dev/v1-welcome#webhook-response)\
- [Migrating from V0](https://docs.firecrawl.dev/v1-welcome#migrating-from-v0)\
- [/scrape endpoint](https://docs.firecrawl.dev/v1-welcome#%2Fscrape-endpoint)\
- [Formats](https://docs.firecrawl.dev/v1-welcome#formats)\
- [Details on the new request body](https://docs.firecrawl.dev/v1-welcome#details-on-the-new-request-body)\
- [/crawl endpoint](https://docs.firecrawl.dev/v1-welcome#%2Fcrawl-endpoint)\
- [Details on the new request body](https://docs.firecrawl.dev/v1-welcome#details-on-the-new-request-body-2)

## Camel AI Integration
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

Camel AI

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/integrations/camelai\#installation)  Installation

Copy

```bash
pip install camel-ai

```

## [‚Äã](https://docs.firecrawl.dev/integrations/camelai\#usage)  Usage

With Camel AI and Firecrawl you can quickly build multi-agent systems than use data from the web.

### [‚Äã](https://docs.firecrawl.dev/integrations/camelai\#using-firecrawl-to-gather-an-entire-website)  Using Firecrawl to Gather an Entire Website

Copy

```python
mock_app = MockFirecrawlApp.return_value
firecrawl = Firecrawl(
    api_key='FC_API_KEY', api_url='https://api.test.com'
)
url = 'https://example.com'
response = [{'markdown': 'Markdown content'}]
mock_app.crawl_url.return_value = respons
result = firecrawl.markdown_crawl(url)

```

### [‚Äã](https://docs.firecrawl.dev/integrations/camelai\#using-firecrawl-to-gather-a-single-page)  Using Firecrawl to Gather a Single Page

Copy

```python
mock_app = MockFirecrawlApp.return_value
firecrawl = Firecrawl(
    api_key='test_api_key', api_url='https://api.test.com'
)
url = 'https://example.com'
response = 'Scraped content'
mock_app.scrape_url.return_value = response

result = firecrawl.scrape(url)

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/camelai.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/camelai)

[Langflow](https://docs.firecrawl.dev/integrations/langflow) [SourceSync.ai](https://docs.firecrawl.dev/integrations/sourcesyncai)

On this page

- [Installation](https://docs.firecrawl.dev/integrations/camelai#installation)
- [Usage](https://docs.firecrawl.dev/integrations/camelai#usage)
- [Using Firecrawl to Gather an Entire Website](https://docs.firecrawl.dev/integrations/camelai#using-firecrawl-to-gather-an-entire-website)
- [Using Firecrawl to Gather a Single Page](https://docs.firecrawl.dev/integrations/camelai#using-firecrawl-to-gather-a-single-page)

## Firecrawl LlamaIndex Integration
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Integrations

Llamaindex

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

> Note: this integration is still using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction). You can install the 0.0.20 version for the Python SDK or the 0.0.36 for the Node SDK.

## [‚Äã](https://docs.firecrawl.dev/integrations/llamaindex\#installation)  Installation

Copy

```bash
pip install firecrawl-py==0.0.20 llama_index llama-index llama-index-readers-web

```

## [‚Äã](https://docs.firecrawl.dev/integrations/llamaindex\#usage)  Usage

### [‚Äã](https://docs.firecrawl.dev/integrations/llamaindex\#using-firecrawl-to-gather-an-entire-website)  Using FireCrawl to Gather an Entire Website

Copy

```python
from llama_index.readers.web import FireCrawlWebReader
from llama_index.core import SummaryIndex
import os

# Initialize FireCrawlWebReader to crawl a website
firecrawl_reader = FireCrawlWebReader(
    api_key="<your_api_key>",  # Replace with your actual API key from https://www.firecrawl.dev/
    mode="scrape",  # Choose between "crawl" and "scrape" for single page scraping
    params={"additional": "parameters"}  # Optional additional parameters
)

# Set the environment variable for the virtual key
os.environ["OPENAI_API_KEY"] = "<OPENAI_API_KEY>"

# Load documents from a single page URL
documents = firecrawl_reader.load_data(url="http://paulgraham.com/")
index = SummaryIndex.from_documents(documents)

# Set Logging to DEBUG for more detailed outputs
query_engine = index.as_query_engine()
response = query_engine.query("What did the author do growing up?")
display(Markdown(f"<b>{response}</b>"))

```

### [‚Äã](https://docs.firecrawl.dev/integrations/llamaindex\#using-firecrawl-to-gather-a-single-page)  Using FireCrawl to Gather a Single Page

Copy

```python
from llama_index.readers.web import FireCrawlWebReader

# Initialize the FireCrawlWebReader with your API key and desired mode
firecrawl_reader = FireCrawlWebReader(
    api_key="<your_api_key>",  # Replace with your actual API key from https://www.firecrawl.dev/
    mode="scrape",  # Choose between "crawl" and "scrape"
    params={"additional": "parameters"}  # Optional additional parameters
)

# Load documents from a specified URL
documents = firecrawl_reader.load_data(url="http://paulgraham.com/worked.html")
index = SummaryIndex.from_documents(documents)

# Set Logging to DEBUG for more detailed outputs
query_engine = index.as_query_engine()
response = query_engine.query("What did the author do growing up?")
display(Markdown(f"<b>{response}</b>"))

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/integrations/llamaindex.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/integrations/llamaindex)

[Langchain](https://docs.firecrawl.dev/integrations/langchain) [CrewAI](https://docs.firecrawl.dev/integrations/crewai)

On this page

- [Installation](https://docs.firecrawl.dev/integrations/llamaindex#installation)
- [Usage](https://docs.firecrawl.dev/integrations/llamaindex#usage)
- [Using FireCrawl to Gather an Entire Website](https://docs.firecrawl.dev/integrations/llamaindex#using-firecrawl-to-gather-an-entire-website)
- [Using FireCrawl to Gather a Single Page](https://docs.firecrawl.dev/integrations/llamaindex#using-firecrawl-to-gather-a-single-page)

## Firecrawl Rate Limits
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Rate Limits

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/rate-limits\#standard-plans)  Standard Plans

| Plan | /scrape (requests/min) | /map (requests/min) | /crawl (requests/min) | /search (requests/min) |
| --- | --- | --- | --- | --- |
| Free | 10 | 10 | 1 | 5 |
| Hobby | 20 | 20 | 3 | 10 |
| Standard | 100 | 100 | 10 | 50 |
| Growth | 1000 | 1000 | 50 | 500 |

|  | /crawl/status (requests/min) |
| --- | --- |
| Default | 150 |

These rate limits are enforced to ensure fair usage and availability of the API for all users. If you require higher limits, please contact us at [hello@firecrawl.com](mailto:hello@firecrawl.com) to discuss custom plans.

### [‚Äã](https://docs.firecrawl.dev/rate-limits\#batch-endpoints)  Batch Endpoints

Batch endpoints follow the /crawl rate limit.

## [‚Äã](https://docs.firecrawl.dev/rate-limits\#extract)  Extract

| Plan | /extract (requests/min) |
| --- | --- |
| Free | 10 |
| Hobby | 20 |
| Standard | 100 |
| Growth | 1000 |
| Enterprise | Custom |

|  | /extract/status (requests/min) |
| --- | --- |
| Free | 500 |

## [‚Äã](https://docs.firecrawl.dev/rate-limits\#legacy-plans)  Legacy Plans

| Plan | /scrape (requests/min) | /crawl (concurrent req) | /search (requests/min) |
| --- | --- | --- | --- |
| Starter | 20 | 3 | 20 |
| Standard Legacy | 40 | 40 | 40 |
| Scaled Legacy | 50 | 20 | 50 |

If you require higher limits, please contact us at [hello@firecrawl.com](mailto:hello@firecrawl.com) to discuss custom plans.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/rate-limits.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/rate-limits)

[Welcome to V1](https://docs.firecrawl.dev/v1-welcome) [Integrations](https://docs.firecrawl.dev/integrations)

On this page

- [Standard Plans](https://docs.firecrawl.dev/rate-limits#standard-plans)
- [Batch Endpoints](https://docs.firecrawl.dev/rate-limits#batch-endpoints)
- [Extract](https://docs.firecrawl.dev/rate-limits#extract)
- [Legacy Plans](https://docs.firecrawl.dev/rate-limits#legacy-plans)

## Firecrawl SDK Overview
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Overall

Overview

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/sdks/overview\#official-sdks)  Official SDKs

[**Python SDK** \\
\\
Explore the Python SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/python) [**Node SDK** \\
\\
Explore the Node SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/node)

## [‚Äã](https://docs.firecrawl.dev/sdks/overview\#community-sdks)  Community SDKs

[**Go SDK** \\
\\
Explore the Go SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/go) [**Rust SDK** \\
\\
Explore the Rust SDK for Firecrawl.](https://docs.firecrawl.dev/sdks/rust)

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/sdks/overview.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/sdks/overview)

[Python](https://docs.firecrawl.dev/sdks/python)

On this page

- [Official SDKs](https://docs.firecrawl.dev/sdks/overview#official-sdks)
- [Community SDKs](https://docs.firecrawl.dev/sdks/overview#community-sdks)

## LLM Data Extraction
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Scrape

LLM Extract

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/features/llm-extract\#scrape-and-extract-structured-data-with-firecrawl)  Scrape and extract structured data with Firecrawl

Firecrawl leverages Large Language Models (LLMs) to efficiently extract structured data from web pages. Here‚Äôs how:

1. **Schema Definition:**
Define the URL to scrape and the desired data schema using JSON Schema (following OpenAI tool schema). This schema specifies the data structure you expect to extract from the page.

2. **Scrape Endpoint:**
Pass the URL and the schema to the scrape endpoint. Documentation for this endpoint can be found here:
[Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

3. **Structured Data Retrieval:**
Receive the scraped data in the structured format defined by your schema. You can then use this data as needed in your application or for further processing.


This method streamlines data extraction, reducing manual handling and enhancing efficiency.

## [‚Äã](https://docs.firecrawl.dev/features/llm-extract\#extract-structured-data)  Extract structured data

### [‚Äã](https://docs.firecrawl.dev/features/llm-extract\#%2Fscrape-with-extract-endpoint)  /scrape (with extract) endpoint

Used to extract structured data from scraped pages.

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp
from pydantic import BaseModel, Field

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key='your_api_key')

class ExtractSchema(BaseModel):
    company_mission: str
    supports_sso: bool
    is_open_source: bool
    is_in_yc: bool

data = app.scrape_url('https://docs.firecrawl.dev/', {
    'formats': ['json'],
    'jsonOptions': {
        'schema': ExtractSchema.model_json_schema(),
    }
})
print(data["json"])

```

Output:

JSON

Copy

```json
{
    "success": true,
    "data": {
      "json": {
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",
        "supports_sso": true,
        "is_open_source": false,
        "is_in_yc": true
      },
      "metadata": {
        "title": "Mendable",
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
        "robots": "follow, index",
        "ogTitle": "Mendable",
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
        "ogUrl": "https://docs.firecrawl.dev/",
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
        "ogLocaleAlternate": [],
        "ogSiteName": "Mendable",
        "sourceURL": "https://docs.firecrawl.dev/"
      },
    }
}

```

### [‚Äã](https://docs.firecrawl.dev/features/llm-extract\#extracting-without-schema-new)  Extracting without schema (New)

You can now extract without a schema by just passing a `prompt` to the endpoint. The llm chooses the structure of the data.

cURL

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "formats": ["json"],
      "jsonOptions": {
        "prompt": "Extract the company mission from the page."
      }
    }'

```

Output:

JSON

Copy

```json
{
    "success": true,
    "data": {
      "json": {
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",
      },
      "metadata": {
        "title": "Mendable",
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
        "robots": "follow, index",
        "ogTitle": "Mendable",
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
        "ogUrl": "https://docs.firecrawl.dev/",
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
        "ogLocaleAlternate": [],
        "ogSiteName": "Mendable",
        "sourceURL": "https://docs.firecrawl.dev/"
      },
    }
}

```

### [‚Äã](https://docs.firecrawl.dev/features/llm-extract\#extract-object)  Extract object

The `extract` object accepts the following parameters:

- `schema`: The schema to use for the extraction.
- `systemPrompt`: The system prompt to use for the extraction.
- `prompt`: The prompt to use for the extraction without a schema.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/llm-extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/llm-extract)

[Batch Scrape](https://docs.firecrawl.dev/features/batch-scrape) [Crawl](https://docs.firecrawl.dev/features/crawl)

On this page

- [Scrape and extract structured data with Firecrawl](https://docs.firecrawl.dev/features/llm-extract#scrape-and-extract-structured-data-with-firecrawl)
- [Extract structured data](https://docs.firecrawl.dev/features/llm-extract#extract-structured-data)
- [/scrape (with extract) endpoint](https://docs.firecrawl.dev/features/llm-extract#%2Fscrape-with-extract-endpoint)
- [Extracting without schema (New)](https://docs.firecrawl.dev/features/llm-extract#extracting-without-schema-new)
- [Extract object](https://docs.firecrawl.dev/features/llm-extract#extract-object)

## Firecrawl Rust SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Rust

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/rust).

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#installation)  Installation

To install the Firecrawl Rust SDK, add the following to your `Cargo.toml`:

Copy

```toml
[dependencies]
firecrawl = "^0.1"
tokio = { version = "^1", features = ["full"] }
serde = { version = "^1.0", features = ["derive"] }
serde_json = "^1.0"
uuid = { version = "^1.10", features = ["v4"] }

[build-dependencies]
tokio = { version = "1", features = ["full"] }

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` struct.

Here‚Äôs an example of how to use the SDK in Rust:

Copy

```rust
use firecrawl::FirecrawlApp;

#[tokio::main]
async fn main() {
  let api_key = "YOUR_API_KEY";
  let api_url = "https://api.firecrawl.dev";
  let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

  // Scrape a single URL
  let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;
  match scrape_result {
    Ok(data) => println!("Scraped Data: {}", data),
    Err(e) => eprintln!("Error occurred while scraping: {}", e),
  }
  // Crawl a website
  let crawl_params = json!({
    "pageOptions": {
      "onlyMainContent": true
    }
  });

  let crawl_result = app.crawl_url("https://docs.firecrawl.dev", Some(crawl_params)).await;

  match crawl_result {
    Ok(data) => println!("Crawl Result: {}", data),
    Err(e) => eprintln!("Error occurred while crawling: {}", e),
  }
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#scraping-a-url)  Scraping a URL

To scrape a single URL with error handling, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a `serde_json::Value`.

Copy

```rust
let scrape_result = app.scrape_url("https://docs.firecrawl.dev", None).await;

match scrape_result {
  Ok(data) => println!("Scraped Data: {}", data),
  Err(e) => eprintln!("Failed to scrape URL: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#crawling-a-website)  Crawling a Website

To crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Copy

```rust
let crawl_params = json!({
  "crawlerOptions": {
    "excludes": ["blog/"],
    "includes": [], // leave empty for all pages
    "limit": 1000
  },
  "pageOptions": {
    "onlyMainContent": true
  }
});
let crawl_result = app.crawl_url("https://docs.firecrawl.dev", Some(crawl_params)).await;

match crawl_result {
  Ok(data) => println!("Crawl Result: {}", data),
  Err(e) => eprintln!("Failed to crawl URL: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.

Copy

```rust
let job_id = "your_job_id_here";
let status = app.check_crawl_status(job_id).await;

match status {
  Ok(data) => println!("Crawl Status: {}", data),
  Err(e) => eprintln!("Failed to check crawl status: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#canceling-a-crawl-job)  Canceling a Crawl Job

To cancel a crawl job, use the `cancel_crawl_job` method. It takes the job ID as a parameter and returns the cancellation status of the crawl job.

Copy

```rust
let job_id = "your_job_id_here";
let canceled = app.cancel_crawl_job(job_id).await;

match canceled {
  Ok(status) => println!("Cancellation Status: {}", status),
  Err(e) => eprintln!("Failed to cancel crawl job: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#extracting-structured-data-from-a-url)  Extracting structured data from a URL

With LLM extraction, you can easily extract structured data from any URL. Here is how you to use it:

Copy

```rust
let json_schema = json!({
  "type": "object",
  "properties": {
    "top": {
      "type": "array",
      "items": {
        "type": "object",
        "properties": {
        "title": {"type": "string"},
        "points": {"type": "number"},
        "by": {"type": "string"},
        "commentsURL": {"type": "string"}
      },
      "required": ["title", "points", "by", "commentsURL"]
      },
      "minItems": 5,
      "maxItems": 5,
      "description": "Top 5 stories on Hacker News"
    }
  },
  "required": ["top"]
});

let llm_extraction_params = json!({
  "extractorOptions": {
    "extractionSchema": json_schema
  }
});

let scrape_result = app.scrape_url("https://news.ycombinator.com", Some(llm_extraction_params)).await;

match scrape_result {
  Ok(data) => println!("LLM Extraction Result: {}", data),
  Err(e) => eprintln!("Failed to perform LLM extraction: {}", e),
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#search-for-a-query)  Search for a query

To search the web, get the most relevant results, scrape each page and return the markdown, use the `search` method. The method takes the query as a parameter and returns the search results.

Copy

```rust
let query = "What is firecrawl?";
let search_result = app.search(query).await;

match search_result {
  Ok(data) => println!("Search Result: {}", data),
  Err(e) => eprintln!("Failed to search: {}", e),
}

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/rust\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/rust.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/rust)

[Go](https://docs.firecrawl.dev/v0/sdks/go)

On this page

- [Installation](https://docs.firecrawl.dev/v0/sdks/rust#installation)
- [Usage](https://docs.firecrawl.dev/v0/sdks/rust#usage)
- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/rust#scraping-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/rust#crawling-a-website)
- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/rust#checking-crawl-status)
- [Canceling a Crawl Job](https://docs.firecrawl.dev/v0/sdks/rust#canceling-a-crawl-job)
- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/rust#extracting-structured-data-from-a-url)
- [Search for a query](https://docs.firecrawl.dev/v0/sdks/rust#search-for-a-query)
- [Error Handling](https://docs.firecrawl.dev/v0/sdks/rust#error-handling)

## Self-Hosting Firecrawl
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Contributing

Self-hosting

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

#### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#contributor%3F)  Contributor?

Welcome to [Firecrawl](https://firecrawl.dev/) üî•! Here are some instructions on how to get the project locally so you can run it on your own and contribute.

If you‚Äôre contributing, note that the process is similar to other open-source repos, i.e., fork Firecrawl, make changes, run tests, PR.

If you have any questions or would like help getting on board, join our Discord community [here](https://discord.gg/gSmWdAkdwd) for more information or submit an issue on Github [here](https://github.com/mendableai/firecrawl/issues/new/choose)!

## [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#self-hosting-firecrawl)  Self-hosting Firecrawl

Refer to [SELF\_HOST.md](https://github.com/mendableai/firecrawl/blob/main/SELF_HOST.md) for instructions on how to run it locally.

## [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#why%3F)  Why?

Self-hosting Firecrawl is particularly beneficial for organizations with stringent security policies that require data to remain within controlled environments. Here are some key reasons to consider self-hosting:

- **Enhanced Security and Compliance:** By self-hosting, you ensure that all data handling and processing complies with internal and external regulations, keeping sensitive information within your secure infrastructure. Note that Firecrawl is a Mendable product and relies on SOC2 Type2 certification, which means that the platform adheres to high industry standards for managing data security.
- **Customizable Services:** Self-hosting allows you to tailor the services, such as the Playwright service, to meet specific needs or handle particular use cases that may not be supported by the standard cloud offering.
- **Learning and Community Contribution:** By setting up and maintaining your own instance, you gain a deeper understanding of how Firecrawl works, which can also lead to more meaningful contributions to the project.

### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#considerations)  Considerations

However, there are some limitations and additional responsibilities to be aware of:

1. **Limited Access to Fire-engine:** Currently, self-hosted instances of Firecrawl do not have access to Fire-engine, which includes advanced features for handling IP blocks, robot detection mechanisms, and more. This means that while you can manage basic scraping tasks, more complex scenarios might require additional configuration or might not be supported.
2. **Manual Configuration Required:** If you need to use scraping methods beyond the basic fetch and Playwright options, you will need to manually configure these in the `.env` file. This requires a deeper understanding of the technologies and might involve more setup time.

Self-hosting Firecrawl is ideal for those who need full control over their scraping and data processing environments but comes with the trade-off of additional maintenance and configuration efforts.

## [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#steps)  Steps

1. First, start by installing the dependencies

- Docker [instructions](https://docs.docker.com/get-docker/)

2. Set environment variables

Create an `.env` in the root directory you can copy over the template in `apps/api/.env.example`

To start, we wont set up authentication, or any optional sub services (pdf parsing, JS blocking support, AI features)

Copy

```
# .env

# ===== Required ENVS ======
NUM_WORKERS_PER_QUEUE=8
PORT=3002
HOST=0.0.0.0

#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379
REDIS_URL=redis://redis:6379

#for self-hosting using docker, use redis://redis:6379. For running locally, use redis://localhost:6379
REDIS_RATE_LIMIT_URL=redis://redis:6379
PLAYWRIGHT_MICROSERVICE_URL=http://playwright-service:3000/html

## To turn on DB authentication, you need to set up supabase.
USE_DB_AUTHENTICATION=false

# ===== Optional ENVS ======

# Supabase Setup (used to support DB authentication, advanced logging, etc.)
SUPABASE_ANON_TOKEN=
SUPABASE_URL=
SUPABASE_SERVICE_TOKEN=

# Other Optionals
# use if you've set up authentication and want to test with a real API key
TEST_API_KEY=
# set if you'd like to test the scraping rate limit
RATE_LIMIT_TEST_API_KEY_SCRAPE=
# set if you'd like to test the crawling rate limit
RATE_LIMIT_TEST_API_KEY_CRAWL=
# set if you'd like to use scraping Be to handle JS blocking
SCRAPING_BEE_API_KEY=
# add for LLM dependednt features (image alt generation, etc.)
OPENAI_API_KEY=
BULL_AUTH_KEY=@
# use if you're configuring basic logging with logtail
LOGTAIL_KEY=
# set if you have a llamaparse key you'd like to use to parse pdfs
LLAMAPARSE_API_KEY=
# set if you'd like to send slack server health status messages
SLACK_WEBHOOK_URL=
# set if you'd like to send posthog events like job logs
POSTHOG_API_KEY=
# set if you'd like to send posthog events like job logs
POSTHOG_HOST=

# set if you'd like to use the fire engine closed beta
FIRE_ENGINE_BETA_URL=

# Proxy Settings for Playwright (Alternative you can can use a proxy service like oxylabs, which rotates IPs for you on every request)
PROXY_SERVER=
PROXY_USERNAME=
PROXY_PASSWORD=
# set if you'd like to block media requests to save proxy bandwidth
BLOCK_MEDIA=

# Set this to the URL of your webhook when using the self-hosted version of FireCrawl
SELF_HOSTED_WEBHOOK_URL=

# Resend API Key for transactional emails
RESEND_API_KEY=

# LOGGING_LEVEL determines the verbosity of logs that the system will output.
# Available levels are:
# NONE - No logs will be output.
# ERROR - For logging error messages that indicate a failure in a specific operation.
# WARN - For logging potentially harmful situations that are not necessarily errors.
# INFO - For logging informational messages that highlight the progress of the application.
# DEBUG - For logging detailed information on the flow through the system, primarily used for debugging.
# TRACE - For logging more detailed information than the DEBUG level.
# Set LOGGING_LEVEL to one of the above options to control logging output.
LOGGING_LEVEL=INFO

```

3. _(Optional) Running with TypeScript Playwright Service_
   - Update the `docker-compose.yml` file to change the Playwright service:





     Copy









     ```plaintext
         build: apps/playwright-service

     ```







     TO





     Copy









     ```plaintext
         build: apps/playwright-service-ts

     ```

   - Set the `PLAYWRIGHT_MICROSERVICE_URL` in your `.env` file:





     Copy









     ```plaintext
     PLAYWRIGHT_MICROSERVICE_URL=http://localhost:3000/scrape

     ```

   - Don‚Äôt forget to set the proxy server in your `.env` file as needed.
4. Build and run the Docker containers:





Copy









```bash
docker compose build
docker compose up

```


This will run a local instance of Firecrawl which can be accessed at `http://localhost:3002`.

You should be able to see the Bull Queue Manager UI on `http://localhost:3002/admin/@/queues`.

5. _(Optional)_ Test the API

If you‚Äôd like to test the crawl endpoint, you can run this:

Copy

```bash
curl -X POST http://localhost:3002/v0/crawl \
    -H 'Content-Type: application/json' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'

```

## [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#troubleshooting)  Troubleshooting

This section provides solutions to common issues you might encounter while setting up or running your self-hosted instance of Firecrawl.

### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#supabase-client-is-not-configured)  Supabase client is not configured

**Symptom:**

Copy

```bash
[YYYY-MM-DDTHH:MM:SS.SSSz]ERROR - Attempted to access Supabase client when it's not configured.
[YYYY-MM-DDTHH:MM:SS.SSSz]ERROR - Error inserting scrape event: Error: Supabase client is not configured.

```

**Explanation:**
This error occurs because the Supabase client setup is not completed. You should be able to scrape and crawl with no problems. Right now it‚Äôs not possible to configure Supabase in self-hosted instances.

### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#you%E2%80%99re-bypassing-authentication)  You‚Äôre bypassing authentication

**Symptom:**

Copy

```bash
[YYYY-MM-DDTHH:MM:SS.SSSz]WARN - You're bypassing authentication

```

**Explanation:**
This error occurs because the Supabase client setup is not completed. You should be able to scrape and crawl with no problems. Right now it‚Äôs not possible to configure Supabase in self-hosted instances.

### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#docker-containers-fail-to-start)  Docker containers fail to start

**Symptom:**
Docker containers exit unexpectedly or fail to start.

**Solution:**
Check the Docker logs for any error messages using the command:

Copy

```bash
docker logs [container_name]

```

- Ensure all required environment variables are set correctly in the .env file.
- Verify that all Docker services defined in docker-compose.yml are correctly configured and the necessary images are available.

### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#connection-issues-with-redis)  Connection issues with Redis

**Symptom:**
Errors related to connecting to Redis, such as timeouts or ‚ÄúConnection refused‚Äù.

**Solution:**

- Ensure that the Redis service is up and running in your Docker environment.
- Verify that the REDIS\_URL and REDIS\_RATE\_LIMIT\_URL in your .env file point to the correct Redis instance.
- Check network settings and firewall rules that may block the connection to the Redis port.

### [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#api-endpoint-does-not-respond)  API endpoint does not respond

**Symptom:**
API requests to the Firecrawl instance timeout or return no response.

**Solution:**

- Ensure that the Firecrawl service is running by checking the Docker container status.
- Verify that the PORT and HOST settings in your .env file are correct and that no other service is using the same port.
- Check the network configuration to ensure that the host is accessible from the client making the API request.

By addressing these common issues, you can ensure a smoother setup and operation of your self-hosted Firecrawl instance.

## [‚Äã](https://docs.firecrawl.dev/contributing/self-host\#install-firecrawl-on-a-kubernetes-cluster-simple-version)  Install Firecrawl on a Kubernetes Cluster (Simple Version)

Read the [examples/kubernetes-cluster-install/README.md](https://github.com/mendableai/firecrawl/blob/main/examples/kubernetes-cluster-install/README.md) for instructions on how to install Firecrawl on a Kubernetes Cluster.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/contributing/self-host.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/contributing/self-host)

[Running locally](https://docs.firecrawl.dev/contributing/guide)

On this page

- [Contributor?](https://docs.firecrawl.dev/contributing/self-host#contributor%3F)
- [Self-hosting Firecrawl](https://docs.firecrawl.dev/contributing/self-host#self-hosting-firecrawl)
- [Why?](https://docs.firecrawl.dev/contributing/self-host#why%3F)
- [Considerations](https://docs.firecrawl.dev/contributing/self-host#considerations)
- [Steps](https://docs.firecrawl.dev/contributing/self-host#steps)
- [Troubleshooting](https://docs.firecrawl.dev/contributing/self-host#troubleshooting)
- [Supabase client is not configured](https://docs.firecrawl.dev/contributing/self-host#supabase-client-is-not-configured)
- [You‚Äôre bypassing authentication](https://docs.firecrawl.dev/contributing/self-host#you%E2%80%99re-bypassing-authentication)
- [Docker containers fail to start](https://docs.firecrawl.dev/contributing/self-host#docker-containers-fail-to-start)
- [Connection issues with Redis](https://docs.firecrawl.dev/contributing/self-host#connection-issues-with-redis)
- [API endpoint does not respond](https://docs.firecrawl.dev/contributing/self-host#api-endpoint-does-not-respond)
- [Install Firecrawl on a Kubernetes Cluster (Simple Version)](https://docs.firecrawl.dev/contributing/self-host#install-firecrawl-on-a-kubernetes-cluster-simple-version)

## Website Crawling Features
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Features

Crawl

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

Firecrawl thoroughly crawls websites, ensuring comprehensive data extraction while bypassing any web blocker mechanisms. Here‚Äôs how it works:

1. **URL Analysis:**
Begins with a specified URL, identifying links by looking at the sitemap and then crawling the website. If no sitemap is found, it will crawl the website following the links.

2. **Recursive Traversal:**
Recursively follows each link to uncover all subpages.

3. **Content Scraping:**
Gathers content from every visited page while handling any complexities like JavaScript rendering or rate limits.

4. **Result Compilation:**
Converts collected data into clean markdown or structured output, perfect for LLM processing or any other task.


This method guarantees an exhaustive crawl and data collection from any starting URL.

## [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#crawling)  Crawling

### [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#%2Fcrawl-endpoint)  /crawl endpoint

Used to crawl a URL and all accessible subpages. This submits a crawl job and returns a job ID to check the status of the crawl.

### [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#installation)  Installation

Python

JavaScript

Go

Rust

Copy

```bash
pip install firecrawl-py

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#usage)  Usage

Python

JavaScript

Go

Rust

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

crawl_result = app.crawl_url('mendable.ai', {'crawlerOptions': {'excludes': ['blog/*']}})

# Get the markdown
for result in crawl_result:
    print(result['markdown'])

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#job-id-response)  Job ID Response

If you are not using the sdk or prefer to use webhook or a different polling method, you can set the `wait_until_done` to `false`.
This will return a jobId.

For cURL, /crawl will always return a jobId where you can use to check the status of the crawl.

Copy

```json
{ "jobId": "1234-5678-9101" }

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Python

JavaScript

Go

Rust

cURL

Copy

```python
status = app.check_crawl_status(job_id)

```

#### [‚Äã](https://docs.firecrawl.dev/v0/features/crawl\#response)  Response

Copy

```json
{
  "status": "completed",
  "current": 22,
  "total": 22,
  "data": [\
    {\
      "content": "Raw Content ",\
      "markdown": "# Markdown Content",\
      "provider": "web-scraper",\
      "metadata": {\
        "title": "Mendable | AI for CX and Sales",\
        "description": "AI for CX and Sales",\
        "language": null,\
        "sourceURL": "https://www.mendable.ai/"\
      }\
    }\
  ]
}

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/features/crawl.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/features/crawl)

[Scrape](https://docs.firecrawl.dev/v0/features/scrape) [LLM Extract](https://docs.firecrawl.dev/v0/features/extract)

On this page

- [Crawling](https://docs.firecrawl.dev/v0/features/crawl#crawling)
- [/crawl endpoint](https://docs.firecrawl.dev/v0/features/crawl#%2Fcrawl-endpoint)
- [Installation](https://docs.firecrawl.dev/v0/features/crawl#installation)
- [Usage](https://docs.firecrawl.dev/v0/features/crawl#usage)
- [Job ID Response](https://docs.firecrawl.dev/v0/features/crawl#job-id-response)
- [Check Crawl Job](https://docs.firecrawl.dev/v0/features/crawl#check-crawl-job)
- [Response](https://docs.firecrawl.dev/v0/features/crawl#response)

## Data Extraction Features
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Extract

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#introducing-%2Fextract-beta)  Introducing /extract (Beta)

[Please use the new /extract Open Beta docs by clicking here üî•](https://docs.firecrawl.dev/features/extract)

Extract structured data from a single, multiple URLs, or entire websites using Large Language Models (LLMs). Our new `/extract` endpoint allows you to:

- Extract structured data from full websites at once
- Connect or build data enrichment applications that need structured data from websites
- Develop AI applications that need clean data from multiple websites

## [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#considerations)  Considerations

The `/extract` endpoint provides flexible data extraction with customizable schemas. Results can be improved through prompt tuning. It is currently in beta and we welcome your feedback.

## [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#extracting-data)  Extracting Data

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#%2Fextract-endpoint)  /extract endpoint

Used to extract structured data from entire websites.

When specifying URLs, you can append `/*` to the URL to extract information from the entire website path rather than just a single page.

For example, `https://firecrawl.dev/*` will attempt to extract data from all pages on the firecrawl.dev domain. The `/*` is still in under testing so please let us know if you have any issues by emailing [help@firecrawl.dev](mailto:help@firecrawl.dev).

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#usage)  Usage

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp
from pydantic import BaseModel, Field

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key='your_api_key')

class ExtractSchema(BaseModel):
    company_mission: str
    supports_sso: bool
    is_open_source: bool
    is_in_yc: bool

data = app.extract([\
  'https://docs.firecrawl.dev/*',\
  'https://firecrawl.dev/',\
  'https://www.ycombinator.com/companies/'\
], {
    'prompt': 'Extract the company mission, whether it supports SSO, whether it is open source, and whether it is in Y Combinator from the page.',
    'schema': ExtractSchema.model_json_schema(),
})
print(data)

```

For more details about the parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/extract).

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#response-sdks)  Response (sdks)

JSON

Copy

```json
{
  "success": true,
  "data": {
    "company_mission": "Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.",
    "supports_sso": false,
    "is_open_source": true,
    "is_in_yc": true
  }
}

```

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#response-async-or-not-using-sdks)  Response (async or not using sdks)

JSON

Copy

```json
{
  "success": true,
  "id": "850eb555-db9c-42b9-9d96-bac1fca8bb23",
  "urlTrace": []
}

```

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#checking-extract-status)  Checking extract status

You can use the `/extract/ID` endpoint to check the status of an extract job.

This endpoint only works for extract jobs that are in progress or extract jobs that have completed recently (within the last 24 hours).

cURL

Copy

```bash
curl -X GET https://api.firecrawl.dev/v1/extract/<extract_id> \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY'

```

#### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#pending-response)  Pending Response

Extract jobs can have one of the following states:

- `completed`: The extract job has finished successfully.
- `pending`: The extract job is still in progress.
- `failed`: The extract job encountered an error and did not complete.
- `cancelled`: The extract job was cancelled by the user.

JSON

Copy

```json
{
  "success": true,
  "data": [],
  "status": "processing",
  "expiresAt": "2025-01-08T20:58:12.000Z"
}

```

#### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#completed-response)  Completed Response

JSON

Copy

```json
{
  "success": true,
  "data": {
      "company_mission": "Firecrawl is the easiest way to extract data from the web. Developers use us to reliably convert URLs into LLM-ready markdown or structured data with a single API call.",
      "supports_sso": false,
      "is_open_source": true,
      "is_in_yc": true
    },
  "status": "completed",
  "expiresAt": "2025-01-08T20:58:12.000Z"
}

```

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#extracting-without-schema)  Extracting without schema

You can now extract without a schema by just passing a `prompt` to the endpoint. The LLM chooses the structure of the data.

cURL

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/extract \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "urls": [\
        "https://docs.firecrawl.dev/",\
        "https://firecrawl.dev/"\
      ],
      "prompt": "Extract Firecrawl'\''s mission from the page."
    }'

```

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#improving-results-with-web-search-%26-external-links)  Improving Results with Web Search & External Links

If you want to improve the results of the extraction, you can pass an `enableWebSearch` parameter to the endpoint. This will allow it to attempt to find the data from external links - outside the scope of the provided URLs.

### [‚Äã](https://docs.firecrawl.dev/features/extract-beta\#billing)  Billing

While `/extract` is in beta, we are charging 5 credits per URL scraped used to form the final response. This is to prevent abuse. This will be changed in the future.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/extract-beta.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/extract-beta)

On this page

- [Introducing /extract (Beta)](https://docs.firecrawl.dev/features/extract-beta#introducing-%2Fextract-beta)
- [Considerations](https://docs.firecrawl.dev/features/extract-beta#considerations)
- [Extracting Data](https://docs.firecrawl.dev/features/extract-beta#extracting-data)
- [/extract endpoint](https://docs.firecrawl.dev/features/extract-beta#%2Fextract-endpoint)
- [Usage](https://docs.firecrawl.dev/features/extract-beta#usage)
- [Response (sdks)](https://docs.firecrawl.dev/features/extract-beta#response-sdks)
- [Response (async or not using sdks)](https://docs.firecrawl.dev/features/extract-beta#response-async-or-not-using-sdks)
- [Checking extract status](https://docs.firecrawl.dev/features/extract-beta#checking-extract-status)
- [Pending Response](https://docs.firecrawl.dev/features/extract-beta#pending-response)
- [Completed Response](https://docs.firecrawl.dev/features/extract-beta#completed-response)
- [Extracting without schema](https://docs.firecrawl.dev/features/extract-beta#extracting-without-schema)
- [Improving Results with Web Search & External Links](https://docs.firecrawl.dev/features/extract-beta#improving-results-with-web-search-%26-external-links)
- [Billing](https://docs.firecrawl.dev/features/extract-beta#billing)

## Firecrawl Data Extraction
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Features

LLM Extract

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#scrape-and-extract-structured-data-with-firecrawl)  Scrape and extract structured data with Firecrawl

Firecrawl leverages Large Language Models (LLMs) to efficiently extract structured data from web pages. Here‚Äôs how:

1. **Schema Definition:**
Define the URL to scrape and the desired data schema using JSON Schema (following OpenAI tool schema). This schema specifies the data structure you expect to extract from the page.

2. **Scrape Endpoint:**
Pass the URL and the schema to the scrape endpoint. Documentation for this endpoint can be found here:
[Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

3. **Structured Data Retrieval:**
Receive the scraped data in the structured format defined by your schema. You can then use this data as needed in your application or for further processing.


This method streamlines data extraction, reducing manual handling and enhancing efficiency.

## [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#extract-structured-data)  Extract structured data

### [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#%2Fscrape-with-extract-endpoint)  /scrape (with extract) endpoint

Used to extract structured data from scraped pages.

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "extractorOptions": {
        "mode": "llm-extraction",
        "extractionPrompt": "Based on the information on the page, extract the information from the schema. ",
        "extractionSchema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [\
            "company_mission",\
            "supports_sso",\
            "is_open_source",\
            "is_in_yc"\
          ]
        }
      }
    }'

```

Copy

```json
{
    "success": true,
    "data": {
      "content": "Raw Content",
      "metadata": {
        "title": "Mendable",
        "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
        "robots": "follow, index",
        "ogTitle": "Mendable",
        "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
        "ogUrl": "https://docs.firecrawl.dev/",
        "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
        "ogLocaleAlternate": [],
        "ogSiteName": "Mendable",
        "sourceURL": "https://docs.firecrawl.dev/"
      },
      "llm_extraction": {
        "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",
        "supports_sso": true,
        "is_open_source": false,
        "is_in_yc": true
      }
    }
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#using-python-sdk)  Using Python SDK

Copy

```python
from firecrawl import FirecrawlApp

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key='your_api_key', version='v0')

class ArticleSchema(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#with-javascript-sdk)  With JavaScript SDK

Copy

```js
import FirecrawlApp from "@mendable/firecrawl-js";
import { z } from "zod";

const app = new FirecrawlApp({
  apiKey: "fc-YOUR_API_KEY",
  version: "v0"
});

// Define schema to extract contents into
const schema = z.object({
  top: z
    .array(
      z.object({
        title: z.string(),
        points: z.number(),
        by: z.string(),
        commentsURL: z.string(),
      })
    )
    .length(5)
    .describe("Top 5 stories on Hacker News"),
});

const scrapeResult = await app.scrapeUrl("https://news.ycombinator.com", {
  extractorOptions: { extractionSchema: schema },
});

console.log(scrapeResult.data["llm_extraction"]);

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#with-go-sdk)  With Go SDK

Go

Copy

```go
import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
      log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  jsonSchema := map[string]any{
    "type": "object",
    "properties": map[string]any{
      "top": map[string]any{
        "type": "array",
        "items": map[string]any{
          "type": "object",
          "properties": map[string]any{
            "title":       map[string]string{"type": "string"},
            "points":      map[string]string{"type": "number"},
            "by":          map[string]string{"type": "string"},
            "commentsURL": map[string]string{"type": "string"},
          },
          "required": []string{"title", "points", "by", "commentsURL"},
        },
        "minItems":    5,
        "maxItems":    5,
        "description": "Top 5 stories on Hacker News",
      },
    },
    "required": []string{"top"},
  }

  llmExtractionParams := map[string]any{
    "extractorOptions": firecrawl.ExtractorOptions{
      ExtractionSchema: jsonSchema,
    },
  }

  scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
  if err != nil {
    log.Fatalf("Failed to perform LLM extraction: %v", err)
  }
  fmt.Println(scrapeResult)
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/features/extract\#with-rust-sdk)  With Rust SDK

Rust

Copy

```rust
use firecrawl::FirecrawlApp;

#[tokio::main]
async fn main() {
    // Initialize the FirecrawlApp with the API key
    let api_key = "YOUR_API_KEY";
    let api_url = "https://api.firecrawl.dev";
    let app = FirecrawlApp::new(api_key, api_url).expect("Failed to initialize FirecrawlApp");

    // Define schema to extract contents into
    let json_schema = json!({
        "type": "object",
        "properties": {
            "top": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "points": {"type": "number"},
                        "by": {"type": "string"},
                        "commentsURL": {"type": "string"}
                    },
                    "required": ["title", "points", "by", "commentsURL"]
                },
                "minItems": 5,
                "maxItems": 5,
                "description": "Top 5 stories on Hacker News"
            }
        },
        "required": ["top"]
    });

    let llm_extraction_params = json!({
        "extractorOptions": {
            "extractionSchema": json_schema,
            "mode": "llm-extraction"
        },
        "pageOptions": {
            "onlyMainContent": true
        }
    });

    let llm_extraction_result = app
        .scrape_url("https://news.ycombinator.com", Some(llm_extraction_params))
        .await;
    match llm_extraction_result {
        Ok(data) => println!("LLM Extraction Result:\n{}", data["llm_extraction"]),
        Err(e) => eprintln!("LLM Extraction failed: {}", e),
    }
}

```

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/features/extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/features/extract)

[Crawl](https://docs.firecrawl.dev/v0/features/crawl) [Search](https://docs.firecrawl.dev/features/search)

On this page

- [Scrape and extract structured data with Firecrawl](https://docs.firecrawl.dev/v0/features/extract#scrape-and-extract-structured-data-with-firecrawl)
- [Extract structured data](https://docs.firecrawl.dev/v0/features/extract#extract-structured-data)
- [/scrape (with extract) endpoint](https://docs.firecrawl.dev/v0/features/extract#%2Fscrape-with-extract-endpoint)
- [Using Python SDK](https://docs.firecrawl.dev/v0/features/extract#using-python-sdk)
- [With JavaScript SDK](https://docs.firecrawl.dev/v0/features/extract#with-javascript-sdk)
- [With Go SDK](https://docs.firecrawl.dev/v0/features/extract#with-go-sdk)
- [With Rust SDK](https://docs.firecrawl.dev/v0/features/extract#with-rust-sdk)

## Firecrawl Go SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Go

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/go).

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#installation)  Installation

To install the Firecrawl Go SDK, you can use go get:

Copy

```bash
go get github.com/mendableai/firecrawl-go

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` struct.

Here‚Äôs an example of how to use the SDK with error handling:

Copy

```go
import (
  "fmt"
  "log"

  "github.com/mendableai/firecrawl-go"
)

func main() {
  // Initialize the FirecrawlApp with your API key
  app, err := firecrawl.NewFirecrawlApp("YOUR_API_KEY")
  if err != nil {
    log.Fatalf("Failed to initialize FirecrawlApp: %v", err)
  }

  // Scrape a single URL
  scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
  if err != nil {
    log.Fatalf("Error occurred while scraping: %v", err)
  }
  fmt.Println(scrapedData)

  // Crawl a website
  params := map[string]any{
    "pageOptions": map[string]any{
      "onlyMainContent": true,
    },
  }

  crawlResult, err := app.CrawlURL("docs.firecrawl.dev", params)
  if err != nil {
    log.Fatalf("Error occurred while crawling: %v", err)
  }
  fmt.Println(crawlResult)
}

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#scraping-a-url)  Scraping a URL

To scrape a single URL with error handling, use the `ScrapeURL` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Copy

```go
scrapedData, err := app.ScrapeURL("docs.firecrawl.dev", nil)
if err != nil {
  log.Fatalf("Failed to scrape URL: %v", err)
}
fmt.Println(scrapedData)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#crawling-a-website)  Crawling a Website

To crawl a website, use the `CrawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Copy

```go
crawlParams := map[string]any{
  "crawlerOptions": map[string]any{
    "excludes": []string{"blog/*"},
    "includes": []string{}, // leave empty for all pages
    "limit": 1000,
  },
  "pageOptions": map[string]any{
    "onlyMainContent": true,
  },
}
crawlResult, err := app.CrawlURL("docs.firecrawl.dev", crawlParams, true, 2, idempotencyKey)
if err != nil {
  log.Fatalf("Failed to crawl URL: %v", err)
}
fmt.Println(crawlResult)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job, use the `CheckCrawlStatus` method. It takes the job ID as a parameter and returns the current status of the crawl job.

Copy

```go
status, err := app.CheckCrawlStatus(jobId)
if err != nil {
  log.Fatalf("Failed to check crawl status: %v", err)
}
fmt.Println(status)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#canceling-a-crawl-job)  Canceling a Crawl Job

To cancel a crawl job, use the `CancelCrawlJob` method. It takes the job ID as a parameter and returns the cancellation status of the crawl job.

Copy

```go
canceled, err := app.CancelCrawlJob(jobId)
if err != nil {
  log.Fatalf("Failed to cancel crawl job: %v", err)
}
fmt.Println(canceled)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#extracting-structured-data-from-a-url)  Extracting structured data from a URL

With LLM extraction, you can easily extract structured data from any URL. Here is how you to use it:

Copy

```go
jsonSchema := map[string]any{
  "type": "object",
  "properties": map[string]any{
    "top": map[string]any{
      "type": "array",
      "items": map[string]any{
        "type": "object",
        "properties": map[string]any{
          "title":       map[string]string{"type": "string"},
          "points":      map[string]string{"type": "number"},
          "by":          map[string]string{"type": "string"},
          "commentsURL": map[string]string{"type": "string"},
        },
        "required": []string{"title", "points", "by", "commentsURL"},
      },
      "minItems":    5,
      "maxItems":    5,
      "description": "Top 5 stories on Hacker News",
    },
  },
  "required": []string{"top"},
}

llmExtractionParams := map[string]any{
  "extractorOptions": firecrawl.ExtractorOptions{
    ExtractionSchema: jsonSchema,
  },
}

scrapeResult, err := app.ScrapeURL("https://news.ycombinator.com", llmExtractionParams)
if err != nil {
  log.Fatalf("Failed to perform LLM extraction: %v", err)
}
fmt.Println(scrapeResult)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#search-for-a-query)  Search for a query

To search the web, get the most relevant results, scrap each page and return the markdown, use the `Search` method. The method takes the query as a parameter and returns the search results.

Copy

```go
query := "What is firecrawl?"
searchResult, err := app.Search(query)
if err != nil {
  log.Fatalf("Failed to search: %v", err)
}
fmt.Println(searchResult)

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/go\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/go.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/go)

[Node](https://docs.firecrawl.dev/v0/sdks/node) [Rust](https://docs.firecrawl.dev/v0/sdks/rust)

On this page

- [Installation](https://docs.firecrawl.dev/v0/sdks/go#installation)
- [Usage](https://docs.firecrawl.dev/v0/sdks/go#usage)
- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/go#scraping-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/go#crawling-a-website)
- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/go#checking-crawl-status)
- [Canceling a Crawl Job](https://docs.firecrawl.dev/v0/sdks/go#canceling-a-crawl-job)
- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/go#extracting-structured-data-from-a-url)
- [Search for a query](https://docs.firecrawl.dev/v0/sdks/go#search-for-a-query)
- [Error Handling](https://docs.firecrawl.dev/v0/sdks/go#error-handling)

## Advanced Scraping Guide
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Advanced Scraping Guide

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

This guide will walk you through the different endpoints of Firecrawl and how to use them fully with all its parameters.

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#basic-scraping-with-firecrawl-%2Fscrape)  Basic scraping with Firecrawl (/scrape)

To scrape a single page and get clean markdown content, you can use the `/scrape` endpoint.

Python

JavaScript

Go

Rust

cURL

Copy

```python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")

```

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#scraping-pdfs)  Scraping PDFs

**Firecrawl supports scraping PDFs by default.** You can use the `/scrape` endpoint to scrape a PDF link and get the text content of the PDF. You can disable this by setting `parsePDF` to `false`.

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#scrape-options)  Scrape Options

When using the `/scrape` endpoint, you can customize the scraping behavior with many parameters. Here are the available options:

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#setting-the-content-formats-on-response-with-formats)  Setting the content formats on response with `formats`

- **Type**: `array`
- **Enum**: `["markdown", "links", "html", "rawHtml", "screenshot", "json"]`
- **Description**: Specify the formats to include in the response. Options include:

  - `markdown`: Returns the scraped content in Markdown format.
  - `links`: Includes all hyperlinks found on the page.
  - `html`: Provides the content in HTML format.
  - `rawHtml`: Delivers the raw HTML content, without any processing.
  - `screenshot`: Includes a screenshot of the page as it appears in the browser.
  - `json`: Extracts structured information from the page using the LLM.
- **Default**: `["markdown"]`

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#getting-the-full-page-content-as-markdown-with-onlymaincontent)  Getting the full page content as markdown with `onlyMainContent`

- **Type**: `boolean`
- **Description**: By default, the scraper will only return the main content of the page, excluding headers, navigation bars, footers, etc. Set this to `false` to return the full page content.
- **Default**: `true`

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#setting-the-tags-to-include-with-includetags)  Setting the tags to include with `includeTags`

- **Type**: `array`
- **Description**: Specify the HTML tags, classes and ids to include in the response.
- **Default**: undefined

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#setting-the-tags-to-exclude-with-excludetags)  Setting the tags to exclude with `excludeTags`

- **Type**: `array`
- **Description**: Specify the HTML tags, classes and ids to exclude from the response.
- **Default**: undefined

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#waiting-for-the-page-to-load-with-waitfor)  Waiting for the page to load with `waitFor`

- **Type**: `integer`
- **Description**: To be used only as a last resort. Wait for a specified amount of milliseconds for the page to load before fetching content.
- **Default**: `0`

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#setting-the-maximum-timeout)  Setting the maximum `timeout`

- **Type**: `integer`
- **Description**: Set the maximum duration in milliseconds that the scraper will wait for the page to respond before aborting the operation.
- **Default**: `30000` (30 seconds)

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#example-usage)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization : Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "formats": ["markdown", "links", "html", "rawHtml", "screenshot"],
      "includeTags": ["h1", "p", "a", ".main-content"],
      "excludeTags": ["#ad", "#footer"],
      "onlyMainContent": false,
      "waitFor": 1000,
      "timeout": 15000
    }'

```

In this example, the scraper will:

- Return the full page content as markdown.
- Include the markdown, raw HTML, HTML, links and screenshot in the response.
- The response will include only the HTML tags `<h1>`, `<p>`, `<a>`, and elements with the class `.main-content`, while excluding any elements with the IDs `#ad` and `#footer`.
- Wait for 1000 milliseconds (1 second) for the page to load before fetching the content.
- Set the maximum duration of the scrape request to 15000 milliseconds (15 seconds).

Here is the API Reference for it: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#extractor-options)  Extractor Options

When using the `/scrape` endpoint, you can specify options for **extracting structured information** from the page content using the `extract` parameter. Here are the available options:

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#using-the-llm-extraction)  Using the LLM Extraction

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#schema)  schema

- **Type**: `object`
- **Required**: False if prompt is provided
- **Description**: The schema for the data to be extracted. This defines the structure of the extracted data.

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#system-prompt)  system prompt

- **Type**: `string`
- **Required**: False
- **Description**: System prompt for the LLM.

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#prompt)  prompt

- **Type**: `string`
- **Required**: False if schema is provided
- **Description**: A prompt for the LLM to extract the data in the correct structure.
- **Example**: `"Extract the features of the product"`

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#example-usage-2)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://firecrawl.dev",
      "formats": ["markdown", "json"],
      "json": {
        "prompt": "Extract the features of the product"
      }
    }'

```

Copy

```json
{
  "success": true,
  "data": {
    "content": "Raw Content",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/",
      "statusCode": 200
    },
    "extract": {
      "product": "Firecrawl",
      "features": {
        "general": {
          "description": "Turn websites into LLM-ready data.",
          "openSource": true,
          "freeCredits": 500,
          "useCases": [\
            "AI applications",\
            "Data science",\
            "Market research",\
            "Content aggregation"\
          ]
        },
        "crawlingAndScraping": {
          "crawlAllAccessiblePages": true,
          "noSitemapRequired": true,
          "dynamicContentHandling": true,
          "dataCleanliness": {
            "process": "Advanced algorithms",
            "outputFormat": "Markdown"
          }
        },
        ...
      }
    }
  }
}

```

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#actions)  Actions

When using the `/scrape` endpoint, Firecrawl allows you to perform various actions on a web page before scraping its content. This is particularly useful for interacting with dynamic content, navigating through pages, or accessing content that requires user interaction.

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#available-actions)  Available Actions

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#wait)  wait

- **Type**: `object`
- **Description**: Wait for a specified amount of milliseconds.
- **Properties**:

  - `type`: `"wait"`
  - `milliseconds`: Number of milliseconds to wait.
- **Example**:





Copy









```json
{
    "type": "wait",
    "milliseconds": 2000
}

```


#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#screenshot)  screenshot

- **Type**: `object`
- **Description**: Take a screenshot.
- **Properties**:

  - `type`: `"screenshot"`
  - `fullPage`: Should the screenshot be full-page or viewport sized? (default: `false`)
- **Example**:





Copy









```json
{
    "type": "screenshot",
    "fullPage": true
}

```


#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#click)  click

- **Type**: `object`
- **Description**: Click on an element.
- **Properties**:

  - `type`: `"click"`
  - `selector`: Query selector to find the element by.
- **Example**:





Copy









```json
{
    "type": "click",
    "selector": "#load-more-button"
}

```


#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#write)  write

- **Type**: `object`
- **Description**: Write text into an input field.
- **Properties**:

  - `type`: `"write"`
  - `text`: Text to type.
  - `selector`: Query selector for the input field.
- **Example**:





Copy









```json
{
    "type": "write",
    "text": "Hello, world!",
    "selector": "#search-input"
}

```


#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#press)  press

- **Type**: `object`
- **Description**: Press a key on the page.
- **Properties**:

  - `type`: `"press"`
  - `key`: Key to press.
- **Example**:





Copy









```json
{
    "type": "press",
    "key": "Enter"
}

```


#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#scroll)  scroll

- **Type**: `object`
- **Description**: Scroll the page.
- **Properties**:

  - `type`: `"scroll"`
  - `direction`: Direction to scroll ( `"up"` or `"down"`).
  - `amount`: Amount to scroll in pixels.
- **Example**:





Copy









```json
{
    "type": "scroll",
    "direction": "down",
    "amount": 500
}

```


For more details about the actions parameters, refer to the [API Reference](https://docs.firecrawl.dev/api-reference/endpoint/scrape).

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#crawling-multiple-pages)  Crawling Multiple Pages

To crawl multiple pages, you can use the `/crawl` endpoint. This endpoint allows you to specify a base URL you want to crawl and all accessible subpages will be crawled.

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'

```

Returns a id

Copy

```json
{ "id": "1234-5678-9101" }

```

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Copy

```bash
curl -X GET https://api.firecrawl.dev/v1/crawl/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY'

```

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#pagination%2Fnext-url)  Pagination/Next URL

If the content is larger than 10MB or if the crawl job is still running, the response will include a `next` parameter. This parameter is a URL to the next page of results. You can use this parameter to get the next page of results.

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#crawler-options)  Crawler Options

When using the `/crawl` endpoint, you can customize the crawling behavior with request body parameters. Here are the available options:

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#includepaths)  `includePaths`

- **Type**: `array`
- **Description**: URL patterns to include in the crawl. Only URLs matching these patterns will be crawled.
- **Example**: `["/blog/*", "/products/*"]`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#excludepaths)  `excludePaths`

- **Type**: `array`
- **Description**: URL patterns to exclude from the crawl. URLs matching these patterns will be skipped.
- **Example**: `["/admin/*", "/login/*"]`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#maxdepth)  `maxDepth`

- **Type**: `integer`
- **Description**: Maximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.
- **Example**: `2`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#limit)  `limit`

- **Type**: `integer`
- **Description**: Maximum number of pages to crawl.
- **Default**: `10000`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#allowbackwardlinks)  `allowBackwardLinks`

- **Type**: `boolean`
- **Description**: This option permits the crawler to navigate to URLs that are higher in the directory structure than the base URL. For instance, if the base URL is `example.com/blog/topic`, enabling this option allows crawling to pages like `example.com/blog` or `example.com`, which are backward in the path hierarchy relative to the base URL.
- **Default**: `false`

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#allowexternallinks)  `allowExternalLinks`

- **Type**: `boolean`
- **Description**: This option allows the crawler to follow links that point to external domains. Be careful with this option, as it can cause the crawl to stop only based only on the `limit` and `maxDepth` values.
- **Default**: `false`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#scrapeoptions)  scrapeOptions

As part of the crawler options, you can also specify the `scrapeOptions` parameter. This parameter allows you to customize the scraping behavior for each page.

- **Type**: `object`
- **Description**: Options for the scraper.
- **Example**: `{"formats": ["markdown", "links", "html", "rawHtml", "screenshot"], "includeTags": ["h1", "p", "a", ".main-content"], "excludeTags": ["#ad", "#footer"], "onlyMainContent": false, "waitFor": 1000, "timeout": 15000}`
- **Default**: `{ "formats": ["markdown"] }`
- **See**: [Scrape Options](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-content-formats-on-response-with-formats)

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#example-usage-3)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization : Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "includePaths": ["/blog/*", "/products/*"],
      "excludePaths": ["/admin/*", "/login/*"],
      "maxDepth": 2,
      "limit": 1000
    }'

```

In this example, the crawler will:

- Only crawl URLs that match the patterns `/blog/*` and `/products/*`.
- Skip URLs that match the patterns `/admin/*` and `/login/*`.
- Return the full document data for each page.
- Crawl up to a maximum depth of 2.
- Crawl a maximum of 1000 pages.

## [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#mapping-website-links-with-%2Fmap)  Mapping Website Links with `/map`

The `/map` endpoint is adept at identifying URLs that are contextually related to a given website. This feature is crucial for understanding a site‚Äôs contextual link environment, which can greatly aid in strategic site analysis and navigation planning.

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#usage)  Usage

To use the `/map` endpoint, you need to send a GET request with the URL of the page you want to map. Here is an example using `curl`:

Copy

```bash
curl -X POST https://api.firecrawl.dev/v1/map \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'

```

This will return a JSON object containing links contextually related to the url.

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#example-response)  Example Response

Copy

```json
  {
    "success":true,
    "links":[\
      "https://docs.firecrawl.dev",\
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete",\
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-get",\
      "https://docs.firecrawl.dev/api-reference/endpoint/crawl-post",\
      "https://docs.firecrawl.dev/api-reference/endpoint/map",\
      "https://docs.firecrawl.dev/api-reference/endpoint/scrape",\
      "https://docs.firecrawl.dev/api-reference/introduction",\
      "https://docs.firecrawl.dev/articles/search-announcement",\
      ...\
    ]
  }

```

### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#map-options)  Map Options

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#search)  `search`

- **Type**: `string`
- **Description**: Search for links containing specific text.
- **Example**: `"blog"`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#limit-2)  `limit`

- **Type**: `integer`
- **Description**: Maximum number of links to return.
- **Default**: `100`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#ignoresitemap)  `ignoreSitemap`

- **Type**: `boolean`
- **Description**: Ignore the website sitemap when crawling
- **Default**: `true`

#### [‚Äã](https://docs.firecrawl.dev/advanced-scraping-guide\#includesubdomains)  `includeSubdomains`

- **Type**: `boolean`
- **Description**: Include subdomains of the website
- **Default**: `false`

Here is the API Reference for it: [Map Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/map)

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/advanced-scraping-guide.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/advanced-scraping-guide)

[Integrations](https://docs.firecrawl.dev/integrations) [Scrape](https://docs.firecrawl.dev/features/scrape)

On this page

- [Basic scraping with Firecrawl (/scrape)](https://docs.firecrawl.dev/advanced-scraping-guide#basic-scraping-with-firecrawl-%2Fscrape)
- [Scraping PDFs](https://docs.firecrawl.dev/advanced-scraping-guide#scraping-pdfs)
- [Scrape Options](https://docs.firecrawl.dev/advanced-scraping-guide#scrape-options)
- [Setting the content formats on response with formats](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-content-formats-on-response-with-formats)
- [Getting the full page content as markdown with onlyMainContent](https://docs.firecrawl.dev/advanced-scraping-guide#getting-the-full-page-content-as-markdown-with-onlymaincontent)
- [Setting the tags to include with includeTags](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-tags-to-include-with-includetags)
- [Setting the tags to exclude with excludeTags](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-tags-to-exclude-with-excludetags)
- [Waiting for the page to load with waitFor](https://docs.firecrawl.dev/advanced-scraping-guide#waiting-for-the-page-to-load-with-waitfor)
- [Setting the maximum timeout](https://docs.firecrawl.dev/advanced-scraping-guide#setting-the-maximum-timeout)
- [Example Usage](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage)
- [Extractor Options](https://docs.firecrawl.dev/advanced-scraping-guide#extractor-options)
- [Using the LLM Extraction](https://docs.firecrawl.dev/advanced-scraping-guide#using-the-llm-extraction)
- [schema](https://docs.firecrawl.dev/advanced-scraping-guide#schema)
- [system prompt](https://docs.firecrawl.dev/advanced-scraping-guide#system-prompt)
- [prompt](https://docs.firecrawl.dev/advanced-scraping-guide#prompt)
- [Example Usage](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage-2)
- [Actions](https://docs.firecrawl.dev/advanced-scraping-guide#actions)
- [Available Actions](https://docs.firecrawl.dev/advanced-scraping-guide#available-actions)
- [wait](https://docs.firecrawl.dev/advanced-scraping-guide#wait)
- [screenshot](https://docs.firecrawl.dev/advanced-scraping-guide#screenshot)
- [click](https://docs.firecrawl.dev/advanced-scraping-guide#click)
- [write](https://docs.firecrawl.dev/advanced-scraping-guide#write)
- [press](https://docs.firecrawl.dev/advanced-scraping-guide#press)
- [scroll](https://docs.firecrawl.dev/advanced-scraping-guide#scroll)
- [Crawling Multiple Pages](https://docs.firecrawl.dev/advanced-scraping-guide#crawling-multiple-pages)
- [Check Crawl Job](https://docs.firecrawl.dev/advanced-scraping-guide#check-crawl-job)
- [Pagination/Next URL](https://docs.firecrawl.dev/advanced-scraping-guide#pagination%2Fnext-url)
- [Crawler Options](https://docs.firecrawl.dev/advanced-scraping-guide#crawler-options)
- [includePaths](https://docs.firecrawl.dev/advanced-scraping-guide#includepaths)
- [excludePaths](https://docs.firecrawl.dev/advanced-scraping-guide#excludepaths)
- [maxDepth](https://docs.firecrawl.dev/advanced-scraping-guide#maxdepth)
- [limit](https://docs.firecrawl.dev/advanced-scraping-guide#limit)
- [allowBackwardLinks](https://docs.firecrawl.dev/advanced-scraping-guide#allowbackwardlinks)
- [allowExternalLinks](https://docs.firecrawl.dev/advanced-scraping-guide#allowexternallinks)
- [scrapeOptions](https://docs.firecrawl.dev/advanced-scraping-guide#scrapeoptions)
- [Example Usage](https://docs.firecrawl.dev/advanced-scraping-guide#example-usage-3)
- [Mapping Website Links with /map](https://docs.firecrawl.dev/advanced-scraping-guide#mapping-website-links-with-%2Fmap)
- [Usage](https://docs.firecrawl.dev/advanced-scraping-guide#usage)
- [Example Response](https://docs.firecrawl.dev/advanced-scraping-guide#example-response)
- [Map Options](https://docs.firecrawl.dev/advanced-scraping-guide#map-options)
- [search](https://docs.firecrawl.dev/advanced-scraping-guide#search)
- [limit](https://docs.firecrawl.dev/advanced-scraping-guide#limit-2)
- [ignoreSitemap](https://docs.firecrawl.dev/advanced-scraping-guide#ignoresitemap)
- [includeSubdomains](https://docs.firecrawl.dev/advanced-scraping-guide#includesubdomains)

## Firecrawl Python SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Python

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/python).

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#installation)  Installation

To install the Firecrawl Python SDK, you can use pip:

Copy

```bash
pip install firecrawl-py==0.0.16

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.

Here‚Äôs an example of how to use the SDK:

Copy

```python
from firecrawl import FirecrawlApp

# Initialize the FirecrawlApp with your API key
app = FirecrawlApp(api_key='your_api_key')

# Scrape a single URL
url = 'https://docs.firecrawl.dev'
scraped_data = app.scrape_url(url)

# Crawl a website
crawl_url = 'https://docs.firecrawl.dev'
params = {
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#scraping-a-url)  Scraping a URL

To scrape a single URL, use the `scrape_url` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Copy

```python
url = 'https://example.com'
scraped_data = app.scrape_url(url)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#extracting-structured-data-from-a-url)  Extracting structured data from a URL

With LLM extraction, you can easily extract structured data from any URL. We support pydantic schemas to make it easier for you too. Here is how you to use it:

Copy

```python
class ArticleSchema(BaseModel):
    title: str
    points: int
    by: str
    commentsURL: str

class TopArticlesSchema(BaseModel):
    top: List[ArticleSchema] = Field(..., max_items=5, description="Top 5 stories")

data = app.scrape_url('https://news.ycombinator.com', {
    'extractorOptions': {
        'extractionSchema': TopArticlesSchema.model_json_schema(),
        'mode': 'llm-extraction'
    },
    'pageOptions':{
        'onlyMainContent': True
    }
})
print(data["llm_extraction"])

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#crawling-a-website)  Crawling a Website

To crawl a website, use the `crawl_url` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

The `wait_until_done` parameter determines whether the method should wait for the crawl job to complete before returning the result. If set to `True`, the method will periodically check the status of the crawl job until it is completed or the specified `timeout` (in seconds) is reached. If set to `False`, the method will return immediately with the job ID, and you can manually check the status of the crawl job using the `check_crawl_status` method.

Copy

```python
crawl_url = 'https://example.com'
params = {
    'crawlerOptions': {
        'excludes': ['blog/*'],
        'includes': [], # leave empty for all pages
        'limit': 1000,
    },
    'pageOptions': {
        'onlyMainContent': True
    }
}
crawl_result = app.crawl_url(crawl_url, params=params, wait_until_done=True, timeout=5)

```

If `wait_until_done` is set to `True`, the `crawl_url` method will return the crawl result once the job is completed. If the job fails or is stopped, an exception will be raised.

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job, use the `check_crawl_status` method. It takes the job ID as a parameter and returns the current status of the crawl job.

Copy

```python
job_id = crawl_result['jobId']
status = app.check_crawl_status(job_id)

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#search-for-a-query)  Search for a query

Used to search the web, get the most relevant results, scrap each page and return the markdown.

Copy

```python
query = 'what is mendable?'
search_result = app.search(query)

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/python\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/python.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/python)

[Node](https://docs.firecrawl.dev/v0/sdks/node)

On this page

- [Installation](https://docs.firecrawl.dev/v0/sdks/python#installation)
- [Usage](https://docs.firecrawl.dev/v0/sdks/python#usage)
- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/python#scraping-a-url)
- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/python#extracting-structured-data-from-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/python#crawling-a-website)
- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/python#checking-crawl-status)
- [Search for a query](https://docs.firecrawl.dev/v0/sdks/python#search-for-a-query)
- [Error Handling](https://docs.firecrawl.dev/v0/sdks/python#error-handling)

## Firecrawl Node SDK
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

SDKs

Node

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

> Note: this is using [v0 version of the Firecrawl API](https://docs.firecrawl.dev/v0/introduction) which is being deprecated. We recommend switching to [v1](https://docs.firecrawl.dev/sdks/node).

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#installation)  Installation

To install the Firecrawl Node SDK, you can use npm:

Copy

```bash
npm install @mendable/firecrawl-js@0.0.36

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#usage)  Usage

1. Get an API key from [firecrawl.dev](https://firecrawl.dev/)
2. Set the API key as an environment variable named `FIRECRAWL_API_KEY` or pass it as a parameter to the `FirecrawlApp` class.

Here‚Äôs an example of how to use the SDK with error handling:

Copy

```js
import FirecrawlApp from '@mendable/firecrawl-js';

// Initialize the FirecrawlApp with your API key
const app = new FirecrawlApp({ apiKey: "YOUR_API_KEY" });

// Scrape a single URL
const url = 'https://docs.firecrawl.dev';
const scrapedData = await app.scrapeUrl(url);

// Crawl a website
const crawlUrl = 'https://docs.firecrawl.dev';
const params = {
  crawlerOptions: {
    excludes: ['blog/'],
    includes: [], // leave empty for all pages
    limit: 1000,
  },
  pageOptions: {
      onlyMainContent: true
  }
};

const crawlResult = await app.crawlUrl(crawlUrl, params);

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#scraping-a-url)  Scraping a URL

To scrape a single URL with error handling, use the `scrapeUrl` method. It takes the URL as a parameter and returns the scraped data as a dictionary.

Copy

```js
const url = 'https://example.com';
const scrapedData = await app.scrapeUrl(url);

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#crawling-a-website)  Crawling a Website

To crawl a website with error handling, use the `crawlUrl` method. It takes the starting URL and optional parameters as arguments. The `params` argument allows you to specify additional options for the crawl job, such as the maximum number of pages to crawl, allowed domains, and the output format.

Copy

```js
const crawlUrl = 'https://example.com';

const params = {
  crawlerOptions: {
    excludes: ['blog/'],
    includes: [], // leave empty for all pages
    limit: 1000,
  },
  pageOptions: {
    onlyMainContent: true
  }
};

const waitUntilDone = true;
const pollInterval = 5;

const crawlResult = await app.crawlUrl(
  crawlUrl,
  params,
  waitUntilDone,
  pollInterval
);

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#checking-crawl-status)  Checking Crawl Status

To check the status of a crawl job with error handling, use the `checkCrawlStatus` method. It takes the job ID as a parameter and returns the current status of the crawl job.

Copy

```js
const status = await app.checkCrawlStatus(jobId);

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#extracting-structured-data-from-a-url)  Extracting structured data from a URL

With LLM extraction, you can easily extract structured data from any URL. We support zod schema to make it easier for you too. Here is how you to use it:

Copy

```js
import FirecrawlApp from "@mendable/firecrawl-js";
import { z } from "zod";

const app = new FirecrawlApp({
  apiKey: "fc-YOUR_API_KEY",
});

// Define schema to extract contents into
const schema = z.object({
  top: z
    .array(
      z.object({
        title: z.string(),
        points: z.number(),
        by: z.string(),
        commentsURL: z.string(),
      })
    )
    .length(5)
    .describe("Top 5 stories on Hacker News"),
});

const scrapeResult = await app.scrapeUrl("https://firecrawl.dev", {
  extractorOptions: { extractionSchema: schema },
});

console.log(scrapeResult.data["llm_extraction"]);

```

### [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#search-for-a-query)  Search for a query

With the `search` method, you can search for a query in a search engine and get the top results along with the page content for each result. The method takes the query as a parameter and returns the search results.

Copy

```js
const query = 'what is mendable?';
const searchResults = await app.search(query, {
  pageOptions: {
    fetchPageContent: true // Fetch the page content for each search result
  }
});

```

## [‚Äã](https://docs.firecrawl.dev/v0/sdks/node\#error-handling)  Error Handling

The SDK handles errors returned by the Firecrawl API and raises appropriate exceptions. If an error occurs during a request, an exception will be raised with a descriptive error message. The examples above demonstrate how to handle these errors using `try/catch` blocks.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/sdks/node.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/sdks/node)

[Python](https://docs.firecrawl.dev/v0/sdks/python) [Go](https://docs.firecrawl.dev/v0/sdks/go)

On this page

- [Installation](https://docs.firecrawl.dev/v0/sdks/node#installation)
- [Usage](https://docs.firecrawl.dev/v0/sdks/node#usage)
- [Scraping a URL](https://docs.firecrawl.dev/v0/sdks/node#scraping-a-url)
- [Crawling a Website](https://docs.firecrawl.dev/v0/sdks/node#crawling-a-website)
- [Checking Crawl Status](https://docs.firecrawl.dev/v0/sdks/node#checking-crawl-status)
- [Extracting structured data from a URL](https://docs.firecrawl.dev/v0/sdks/node#extracting-structured-data-from-a-url)
- [Search for a query](https://docs.firecrawl.dev/v0/sdks/node#search-for-a-query)
- [Error Handling](https://docs.firecrawl.dev/v0/sdks/node#error-handling)

## Advanced Scraping Guide
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Get Started

Advanced Scraping Guide

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

This guide will walk you through the different endpoints of Firecrawl and how to use them fully with all its parameters.

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#basic-scraping-with-firecrawl-%2Fscrape)  Basic scraping with Firecrawl (/scrape)

To scrape a single page and get clean markdown content, you can use the `/scrape` endpoint.

Python

JavaScript

Go

Rust

cURL

Copy

```python
# pip install firecrawl-py

from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")

content = app.scrape_url("https://docs.firecrawl.dev")

```

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#scraping-pdfs)  Scraping pdfs

**Firecrawl supports scraping pdfs by default.** You can use the `/scrape` endpoint to scrape a pdf link and get the text content of the pdf. You can disable this by setting `pageOptions.parsePDF` to `false`.

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#page-options)  Page Options

When using the `/scrape` endpoint, you can customize the scraping behavior with the `pageOptions` parameter. Here are the available options:

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#getting-cleaner-content-with-onlymaincontent)  Getting cleaner content with `onlyMainContent`

- **Type**: `boolean`
- **Description**: Only return the main content of the page, excluding headers, navigation bars, footers, etc.
- **Default**: `false`

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#getting-the-html-with-includehtml)  Getting the HTML with `includeHtml`

- **Type**: `boolean`
- **Description**: Include the HTML version content of the page. This will add an `html` key in the response.
- **Default**: `false`

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#getting-the-raw-html-with-includerawhtml)  Getting the raw HTML with `includeRawHtml`

- **Type**: `boolean`
- **Description**: Include the raw HTML content of the page. This will add an `rawHtml` key in the response.
- **Default**: `false`

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#getting-a-screenshot-of-the-page-with-screenshot)  Getting a screenshot of the page with `screenshot`

- **Type**: `boolean`
- **Decription**: Include a screenshot of the top of the page that you are scraping.
- **Default**: `false`

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#waiting-for-the-page-to-load-with-waitfor)  Waiting for the page to load with `waitFor`

- **Type**: `integer`
- **Description**: To be used only as a last resort. Wait for a specified amount of milliseconds for the page to load before fetching content.
- **Default**: `0`

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#example-usage)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization : Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml":true,
        "screenshot": true,
        "waitFor": 5000
      }
    }'

```

In this example, the scraper will:

- Return only the main content of the page.
- Include the raw HTML content in the response in the `html` key.
- Wait for 5000 milliseconds (5 seconds) for the page to load before fetching the content.

Here is the API Reference for it: [Scrape Endpoint Documentation](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#extractor-options)  Extractor Options

When using the `/scrape` endpoint, you can specify options for **extracting structured information** from the page content using the `extractorOptions` parameter. Here are the available options:

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#mode)  mode

- **Type**: `string`

- **Enum**: `["llm-extraction", "llm-extraction-from-raw-html"]`

- **Description**: The extraction mode to use.
  - `llm-extraction`: Extracts information from the cleaned and parsed content.
  - `llm-extraction-from-raw-html`: Extracts information directly from the raw HTML.
- **Type**: `string`

- **Description**: A prompt describing what information to extract from the page.


### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#extractionschema)  extractionSchema

- **Type**: `object`
- **Description**: The schema for the data to be extracted. This defines the structure of the extracted data.

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#example-usage-2)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev/",
      "extractorOptions": {
        "mode": "llm-extraction",
        "extractionPrompt": "Based on the information on the page, extract the information from the schema. ",
        "extractionSchema": {
          "type": "object",
          "properties": {
            "company_mission": {
                      "type": "string"
            },
            "supports_sso": {
                      "type": "boolean"
            },
            "is_open_source": {
                      "type": "boolean"
            },
            "is_in_yc": {
                      "type": "boolean"
            }
          },
          "required": [\
            "company_mission",\
            "supports_sso",\
            "is_open_source",\
            "is_in_yc"\
          ]
        }
      }
    }'

```

Copy

```json
{
  "success": true,
  "data": {
    "content": "Raw Content",
    "metadata": {
      "title": "Mendable",
      "description": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "robots": "follow, index",
      "ogTitle": "Mendable",
      "ogDescription": "Mendable allows you to easily build AI chat applications. Ingest, customize, then deploy with one line of code anywhere you want. Brought to you by SideGuide",
      "ogUrl": "https://docs.firecrawl.dev/",
      "ogImage": "https://docs.firecrawl.dev/mendable_new_og1.png",
      "ogLocaleAlternate": [],
      "ogSiteName": "Mendable",
      "sourceURL": "https://docs.firecrawl.dev/"
    },
    "llm_extraction": {
      "company_mission": "Train a secure AI on your technical resources that answers customer and employee questions so your team doesn't have to",
      "supports_sso": true,
      "is_open_source": false,
      "is_in_yc": true
    }
  }
}

```

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#adjusting-timeout)  Adjusting Timeout

You can adjust the timeout for the scraping process using the `timeout` parameter in milliseconds.

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#example-usage-3)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/scrape \
    -H '
    Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "timeout": 50000
    }'

```

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#crawling-multiple-pages)  Crawling Multiple Pages

To crawl multiple pages, you can use the `/crawl` endpoint. This endpoint allows you to specify a base URL you want to crawl and all accessible subpages will be crawled.

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev"
    }'

```

Returns a jobId

Copy

```json
{ "jobId": "1234-5678-9101" }

```

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#check-crawl-job)  Check Crawl Job

Used to check the status of a crawl job and get its result.

Copy

```bash
curl -X GET https://api.firecrawl.dev/v0/crawl/status/1234-5678-9101 \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer YOUR_API_KEY'

```

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#crawler-options)  Crawler Options

When using the `/crawl` endpoint, you can customize the crawling behavior with the `crawlerOptions` parameter. Here are the available options:

#### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#includes)  `includes`

- **Type**: `array`
- **Description**: URL patterns to include in the crawl. Only URLs matching these patterns will be crawled.
- **Example**: `["/blog/*", "/products/*"]`

#### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#excludes)  `excludes`

- **Type**: `array`
- **Description**: URL patterns to exclude from the crawl. URLs matching these patterns will be skipped.
- **Example**: `["/admin/*", "/login/*"]`

#### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#returnonlyurls)  `returnOnlyUrls`

- **Type**: `boolean`
- **Description**: If set to `true`, the response will only include a list of URLs instead of the full document data.
- **Default**: `false`

#### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#maxdepth)  `maxDepth`

- **Type**: `integer`
- **Description**: Maximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.
- **Example**: `2`

#### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#mode-2)  `mode`

- **Type**: `string`
- **Enum**: `["default", "fast"]`
- **Description**: The crawling mode to use. `fast` mode crawls websites without a sitemap 4x faster but may be less accurate and is not recommended for heavily JavaScript-rendered websites.
- **Default**: `default`

#### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#limit)  `limit`

- **Type**: `integer`
- **Description**: Maximum number of pages to crawl.
- **Default**: `10000`

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#example-usage-4)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization : Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "excludes": ["/admin/*", "/login/*"],
        "returnOnlyUrls": false,
        "maxDepth": 2,
        "mode": "fast",
        "limit": 1000
      }
    }'

```

In this example, the crawler will:

- Only crawl URLs that match the patterns `/blog/*` and `/products/*`.
- Skip URLs that match the patterns `/admin/*` and `/login/*`.
- Return the full document data for each page.
- Crawl up to a maximum depth of 2.
- Use the fast crawling mode.
- Crawl a maximum of 1000 pages.

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#page-options-%2B-crawler-options)  Page Options + Crawler Options

You can combine the `pageOptions` and `crawlerOptions` parameters to customize both the full crawling behavior.

### [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#example-usage-5)  Example Usage

Copy

```bash
curl -X POST https://api.firecrawl.dev/v0/crawl \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{
      "url": "https://docs.firecrawl.dev",
      "pageOptions": {
        "onlyMainContent": true,
        "includeHtml": true,
        "includeRawHtml": true,
        "screenshot": true,
        "waitFor": 5000
      },
      "crawlerOptions": {
        "includes": ["/blog/*", "/products/*"],
        "maxDepth": 2,
        "mode": "fast",
      }
    }'

```

In this example, the crawler will:

- Return only the main content for each page.
- Include the raw HTML content for each page.
- Wait for 5000 milliseconds for each page to load before fetching its content.
- Only crawl URLs that match the patterns `/blog/*` and `/products/*`.
- Crawl up to a maximum depth of 2.
- Use the fast crawling mode.

## [‚Äã](https://docs.firecrawl.dev/v0/advanced-scraping-guide\#extractor-options-%2B-crawler-options)  Extractor Options + Crawler Options

Coming soon‚Ä¶

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/advanced-scraping-guide.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/advanced-scraping-guide)

[Quickstart](https://docs.firecrawl.dev/v0/introduction) [Rate Limits](https://docs.firecrawl.dev/rate-limits)

On this page

- [Basic scraping with Firecrawl (/scrape)](https://docs.firecrawl.dev/v0/advanced-scraping-guide#basic-scraping-with-firecrawl-%2Fscrape)
- [Scraping pdfs](https://docs.firecrawl.dev/v0/advanced-scraping-guide#scraping-pdfs)
- [Page Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options)
- [Getting cleaner content with onlyMainContent](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-cleaner-content-with-onlymaincontent)
- [Getting the HTML with includeHtml](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-the-html-with-includehtml)
- [Getting the raw HTML with includeRawHtml](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-the-raw-html-with-includerawhtml)
- [Getting a screenshot of the page with screenshot](https://docs.firecrawl.dev/v0/advanced-scraping-guide#getting-a-screenshot-of-the-page-with-screenshot)
- [Waiting for the page to load with waitFor](https://docs.firecrawl.dev/v0/advanced-scraping-guide#waiting-for-the-page-to-load-with-waitfor)
- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage)
- [Extractor Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options)
- [mode](https://docs.firecrawl.dev/v0/advanced-scraping-guide#mode)
- [extractionSchema](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractionschema)
- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-2)
- [Adjusting Timeout](https://docs.firecrawl.dev/v0/advanced-scraping-guide#adjusting-timeout)
- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-3)
- [Crawling Multiple Pages](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawling-multiple-pages)
- [Check Crawl Job](https://docs.firecrawl.dev/v0/advanced-scraping-guide#check-crawl-job)
- [Crawler Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#crawler-options)
- [includes](https://docs.firecrawl.dev/v0/advanced-scraping-guide#includes)
- [excludes](https://docs.firecrawl.dev/v0/advanced-scraping-guide#excludes)
- [returnOnlyUrls](https://docs.firecrawl.dev/v0/advanced-scraping-guide#returnonlyurls)
- [maxDepth](https://docs.firecrawl.dev/v0/advanced-scraping-guide#maxdepth)
- [mode](https://docs.firecrawl.dev/v0/advanced-scraping-guide#mode-2)
- [limit](https://docs.firecrawl.dev/v0/advanced-scraping-guide#limit)
- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-4)
- [Page Options + Crawler Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#page-options-%2B-crawler-options)
- [Example Usage](https://docs.firecrawl.dev/v0/advanced-scraping-guide#example-usage-5)
- [Extractor Options + Crawler Options](https://docs.firecrawl.dev/v0/advanced-scraping-guide#extractor-options-%2B-crawler-options)

## Open Source vs Cloud
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Contributing

Open Source vs Cloud

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

Firecrawl is open source available under the [AGPL-3.0 license](https://github.com/mendableai/firecrawl/blob/main/LICENSE).

To deliver the best possible product, we offer a hosted version of Firecrawl alongside our open-source offering. The cloud solution allows us to continuously innovate and maintain a high-quality, sustainable service for all users.

Firecrawl Cloud is available at [firecrawl.dev](https://firecrawl.dev/) and offers a range of features that are not available in the open source version:

![Firecrawl Cloud vs Open Source](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/images/open-source-cloud.png)

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/contributing/open-source-or-cloud.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/contributing/open-source-or-cloud)

[SourceSync.ai](https://docs.firecrawl.dev/integrations/sourcesyncai) [Running locally](https://docs.firecrawl.dev/contributing/guide)

![Firecrawl Cloud vs Open Source](https://docs.firecrawl.dev/contributing/open-source-or-cloud)

## Batch Scraping
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Scrape

Batch Scrape

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#batch-scraping-multiple-urls)  Batch scraping multiple URLs

You can now batch scrape multiple URLs at the same time. It takes the starting URLs and optional parameters as arguments. The params argument allows you to specify additional options for the batch scrape job, such as the output formats.

### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#how-it-works)  How it works

It is very similar to how the `/crawl` endpoint works. It submits a batch scrape job and returns a job ID to check the status of the batch scrape.

The sdk provides 2 methods, synchronous and asynchronous. The synchronous method will return the results of the batch scrape job, while the asynchronous method will return a job ID that you can use to check the status of the batch scrape.

### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#usage)  Usage

Python

Node

cURL

Copy

```python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="fc-YOUR_API_KEY")

# Scrape multiple websites:
batch_scrape_result = app.batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})
print(batch_scrape_result)

# Or, you can use the asynchronous method:
batch_scrape_job = app.async_batch_scrape_urls(['firecrawl.dev', 'mendable.ai'], {'formats': ['markdown', 'html']})
print(batch_scrape_job)

# (async) You can then use the job ID to check the status of the batch scrape:
batch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])
print(batch_scrape_status)

```

### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#response)  Response

If you‚Äôre using the sync methods from the SDKs, it will return the results of the batch scrape job. Otherwise, it will return a job ID that you can use to check the status of the batch scrape.

#### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#synchronous)  Synchronous

Completed

Copy

```json
{
  "status": "completed",
  "total": 36,
  "completed": 36,
  "creditsUsed": 36,
  "expiresAt": "2024-00-00T00:00:00.000Z",
  "next": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26",
  "data": [\
    {\
      "markdown": "[Firecrawl Docs home page![light logo](https://mintlify.s3-us-west-1.amazonaws.com/firecrawl/logo/light.svg)!...",\
      "html": "<!DOCTYPE html><html lang=\"en\" class=\"js-focus-visible lg:[--scroll-mt:9.5rem]\" data-js-focus-visible=\"\">...",\
      "metadata": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "language": "en",\
        "sourceURL": "https://docs.firecrawl.dev/learn/rag-llama3",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot.",\
        "ogLocaleAlternate": [],\
        "statusCode": 200\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
#### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#asynchronous)  Asynchronous\
\
You can then use the job ID to check the status of the batch scrape by calling the `/batch/scrape/{id}` endpoint. This endpoint is meant to be used while the job is still running or right after it has completed **as batch scrape jobs expire after 24 hours**.\
\
Copy\
\
```json\
{\
  "success": true,\
  "id": "123-456-789",\
  "url": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789"\
}\
\
```\
\
## [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#batch-scrape-with-extraction)  Batch scrape with extraction\
\
You can also use the batch scrape endpoint to extract structured data from the pages. This is useful if you want to get the same structured data from a list of URLs.\
\
Python\
\
Node\
\
cURL\
\
Copy\
\
```python\
from firecrawl import FirecrawlApp\
\
app = FirecrawlApp(api_key="fc-YOUR_API_KEY")\
\
# Scrape multiple websites:\
batch_scrape_result = app.batch_scrape_urls(\
    ['https://docs.firecrawl.dev', 'https://docs.firecrawl.dev/sdks/overview'],\
    {\
        'formats': ['extract'],\
        'extract': {\
            'prompt': 'Extract the title and description from the page.',\
            'schema': {\
                'type': 'object',\
                'properties': {\
                    'title': {'type': 'string'},\
                    'description': {'type': 'string'}\
                },\
                'required': ['title', 'description']\
            }\
        }\
    }\
)\
print(batch_scrape_result)\
\
# Or, you can use the asynchronous method:\
batch_scrape_job = app.async_batch_scrape_urls(\
    ['https://docs.firecrawl.dev', 'https://docs.firecrawl.dev/sdks/overview'],\
    {\
        'formats': ['extract'],\
        'extract': {\
            'prompt': 'Extract the title and description from the page.',\
            'schema': {\
                'type': 'object',\
                'properties': {\
                    'title': {'type': 'string'},\
                    'description': {'type': 'string'}\
                },\
                'required': ['title', 'description']\
            }\
        }\
    }\
)\
print(batch_scrape_job)\
\
# (async) You can then use the job ID to check the status of the batch scrape:\
batch_scrape_status = app.check_batch_scrape_status(batch_scrape_job['id'])\
print(batch_scrape_status)\
\
```\
\
### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#response-2)  Response\
\
#### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#synchronous-2)  Synchronous\
\
Completed\
\
Copy\
\
```json\
{\
  "status": "completed",\
  "total": 36,\
  "completed": 36,\
  "creditsUsed": 36,\
  "expiresAt": "2024-00-00T00:00:00.000Z",\
  "next": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789?skip=26",\
  "data": [\
    {\
      "extract": {\
        "title": "Build a 'Chat with website' using Groq Llama 3 | Firecrawl",\
        "description": "Learn how to use Firecrawl, Groq Llama 3, and Langchain to build a 'Chat with your website' bot."\
      }\
    },\
    ...\
  ]\
}\
\
```\
\
#### [‚Äã](https://docs.firecrawl.dev/features/batch-scrape\#asynchronous-2)  Asynchronous\
\
Copy\
\
```json\
{\
  "success": true,\
  "id": "123-456-789",\
  "url": "https://api.firecrawl.dev/v1/batch/scrape/123-456-789"\
}\
\
```\
\
[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/features/batch-scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/features/batch-scrape)\
\
[Scrape](https://docs.firecrawl.dev/features/scrape) [LLM Extract](https://docs.firecrawl.dev/features/llm-extract)\
\
On this page\
\
- [Batch scraping multiple URLs](https://docs.firecrawl.dev/features/batch-scrape#batch-scraping-multiple-urls)\
- [How it works](https://docs.firecrawl.dev/features/batch-scrape#how-it-works)\
- [Usage](https://docs.firecrawl.dev/features/batch-scrape#usage)\
- [Response](https://docs.firecrawl.dev/features/batch-scrape#response)\
- [Synchronous](https://docs.firecrawl.dev/features/batch-scrape#synchronous)\
- [Asynchronous](https://docs.firecrawl.dev/features/batch-scrape#asynchronous)\
- [Batch scrape with extraction](https://docs.firecrawl.dev/features/batch-scrape#batch-scrape-with-extraction)\
- [Response](https://docs.firecrawl.dev/features/batch-scrape#response-2)\
- [Synchronous](https://docs.firecrawl.dev/features/batch-scrape#synchronous-2)\
- [Asynchronous](https://docs.firecrawl.dev/features/batch-scrape#asynchronous-2)

## Firecrawl API Overview
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Using the API

Introduction

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/api-reference/introduction\#features)  Features

[**Scrape** \\
\\
Extract content from any webpage in markdown or json format.](https://docs.firecrawl.dev/api-reference/endpoint/scrape) [**Crawl** \\
\\
Crawl entire websites, extract their content and metadata.](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post) [**Map** \\
\\
Get a complete list of URLs from any website quickly and reliably.](https://docs.firecrawl.dev/api-reference/endpoint/map) [**Extract** \\
\\
Extract structured data from entire webpages using natural language.](https://docs.firecrawl.dev/api-reference/endpoint/extract) [**Search** \\
\\
Search the web and get full page content in any format.](https://docs.firecrawl.dev/api-reference/endpoint/search)

## [‚Äã](https://docs.firecrawl.dev/api-reference/introduction\#base-url)  Base URL

All requests contain the following base URL:

Copy

```bash
https://api.firecrawl.dev

```

## [‚Äã](https://docs.firecrawl.dev/api-reference/introduction\#authentication)  Authentication

For authentication, it‚Äôs required to include an Authorization header. The header should contain `Bearer fc-123456789`, where `fc-123456789` represents your API Key.

Copy

```bash
Authorization: Bearer fc-123456789

```

‚Äã

## [‚Äã](https://docs.firecrawl.dev/api-reference/introduction\#response-codes)  Response codes

Firecrawl employs conventional HTTP status codes to signify the outcome of your requests.

Typically, 2xx HTTP status codes denote success, 4xx codes represent failures related to the user, and 5xx codes signal infrastructure problems.

| Status | Description |
| --- | --- |
| 200 | Request was successful. |
| 400 | Verify the correctness of the parameters. |
| 401 | The API key was not provided. |
| 402 | Payment required |
| 404 | The requested resource could not be located. |
| 429 | The rate limit has been surpassed. |
| 5xx | Signifies a server error with Firecrawl. |

Refer to the Error Codes section for a detailed explanation of all potential API errors.

‚Äã

## [‚Äã](https://docs.firecrawl.dev/api-reference/introduction\#rate-limit)  Rate limit

The Firecrawl API has a rate limit to ensure the stability and reliability of the service. The rate limit is applied to all endpoints and is based on the number of requests made within a specific time frame.

When you exceed the rate limit, you will receive a 429 response code.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/introduction)

[Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape)

On this page

- [Features](https://docs.firecrawl.dev/api-reference/introduction#features)
- [Base URL](https://docs.firecrawl.dev/api-reference/introduction#base-url)
- [Authentication](https://docs.firecrawl.dev/api-reference/introduction#authentication)
- [Response codes](https://docs.firecrawl.dev/api-reference/introduction#response-codes)
- [Rate limit](https://docs.firecrawl.dev/api-reference/introduction#rate-limit)

## Search Endpoint
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Search Endpoints

Search

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

POST

/

search

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/search \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "query": "<string>",
  "limit": 5,
  "tbs": "<string>",
  "lang": "en",
  "country": "us",
  "location": "<string>",
  "timeout": 60000,
  "scrapeOptions": {}
}'
```

200

408

500

Copy

```
{
  "success": true,
  "data": [\
    {\
      "title": "<string>",\
      "description": "<string>",\
      "url": "<string>",\
      "markdown": "<string>",\
      "html": "<string>",\
      "rawHtml": "<string>",\
      "links": [\
        "<string>"\
      ],\
      "screenshot": "<string>",\
      "metadata": {\
        "title": "<string>",\
        "description": "<string>",\
        "sourceURL": "<string>",\
        "statusCode": 123,\
        "error": "<string>"\
      }\
    }\
  ],
  "warning": "<string>"
}
```

The search endpoint combines web search (SERP) with Firecrawl‚Äôs scraping capabilities to return full page content for any query.

Include `scrapeOptions` with `formats: ["markdown"]` to get complete markdown content for each search result otherwise you will default to getting the SERP results (url, title, description).

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-query)

query

string

required

The search query

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-limit)

limit

integer

default:

5

Maximum number of results to return

Required range: `1 <= x <= 10`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-tbs)

tbs

string

Time-based search parameter

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-lang)

lang

string

default:

en

Language code for search results

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-country)

country

string

default:

us

Country code for search results

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-location)

location

string

Location parameter for search results

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-timeout)

timeout

integer

default:

60000

Timeout in milliseconds

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-scrape-options)

scrapeOptions

object

Options for scraping search results

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#body-scrape-options-formats)

scrapeOptions.formats

enum<string>\[\]

Formats to include in the output

Available options:

`markdown`,

`html`,

`rawHtml`,

`links`,

`screenshot`,

`screenshot@fullPage`,

`extract`

#### Response

200

200408500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data)

data

object\[\]

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-title)

data.title

string

Title from search result

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-description)

data.description

string

Description from search result

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-url)

data.url

string

URL of the search result

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-markdown)

data.markdown

string \| null

Markdown content if scraping was requested

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-html)

data.html

string \| null

HTML content if requested in formats

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-raw-html)

data.rawHtml

string \| null

Raw HTML content if requested in formats

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-links)

data.links

string\[\]

Links found if requested in formats

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-screenshot)

data.screenshot

string \| null

Screenshot URL if requested in formats

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata)

data.metadata

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-title)

data.metadata.title

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-description)

data.metadata.description

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-source-url)

data.metadata.sourceURL

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-status-code)

data.metadata.statusCode

integer

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-data-metadata-error)

data.metadata.error

string \| null

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/search#response-warning)

warning

string \| null

Warning message if any issues occurred

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/search.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/search)

[Get Extract Status](https://docs.firecrawl.dev/api-reference/endpoint/extract-get) [Credit Usage](https://docs.firecrawl.dev/api-reference/endpoint/credit-usage)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/search \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "query": "<string>",
  "limit": 5,
  "tbs": "<string>",
  "lang": "en",
  "country": "us",
  "location": "<string>",
  "timeout": 60000,
  "scrapeOptions": {}
}'
```

200

408

500

Copy

```
{
  "success": true,
  "data": [\
    {\
      "title": "<string>",\
      "description": "<string>",\
      "url": "<string>",\
      "markdown": "<string>",\
      "html": "<string>",\
      "rawHtml": "<string>",\
      "links": [\
        "<string>"\
      ],\
      "screenshot": "<string>",\
      "metadata": {\
        "title": "<string>",\
        "description": "<string>",\
        "sourceURL": "<string>",\
        "statusCode": 123,\
        "error": "<string>"\
      }\
    }\
  ],
  "warning": "<string>"
}
```

## Data Extraction API
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Extract Endpoints

Extract

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

POST

/

extract

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/extract \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "urls": [\
    "<string>"\
  ],
  "prompt": "<string>",
  "schema": {
    "property1": "<string>",
    "property2": 123
  },
  "enableWebSearch": false,
  "ignoreSitemap": false,
  "includeSubdomains": true,
  "showSources": false,
  "scrapeOptions": {
    "formats": [\
      "markdown"\
    ],
    "onlyMainContent": true,
    "includeTags": [\
      "<string>"\
    ],
    "excludeTags": [\
      "<string>"\
    ],
    "headers": {},
    "waitFor": 0,
    "mobile": false,
    "skipTlsVerification": false,
    "timeout": 30000,
    "jsonOptions": {
      "schema": {},
      "systemPrompt": "<string>",
      "prompt": "<string>"
    },
    "actions": [\
      {\
        "type": "wait",\
        "milliseconds": 2,\
        "selector": "#my-element"\
      }\
    ],
    "location": {
      "country": "US",
      "languages": [\
        "en-US"\
      ]
    },
    "removeBase64Images": true,
    "blockAds": true,
    "proxy": "basic"
  }
}'
```

200

400

500

Copy

```
{
  "success": true,
  "id": "<string>"
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-urls)

urls

string\[\]

required

The URLs to extract data from. URLs should be in glob format.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-prompt)

prompt

string

Prompt to guide the extraction process

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-schema)

schema

object

Schema to define the structure of the extracted data

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-schema-property1)

schema.property1

string

required

Description of property1

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-schema-property2)

schema.property2

integer

required

Description of property2

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-enable-web-search)

enableWebSearch

boolean

default:

false

When true, the extraction will use web search to find additional data

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-ignore-sitemap)

ignoreSitemap

boolean

default:

false

When true, sitemap.xml files will be ignored during website scanning

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-include-subdomains)

includeSubdomains

boolean

default:

true

When true, subdomains of the provided URLs will also be scanned

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-show-sources)

showSources

boolean

default:

false

When true, the sources used to extract the data will be included in the response as `sources` key

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options)

scrapeOptions

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-formats)

scrapeOptions.formats

enum<string>\[\]

Formats to include in the output.

Available options:

`markdown`,

`html`,

`rawHtml`,

`links`,

`screenshot`,

`screenshot@fullPage`,

`json`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-only-main-content)

scrapeOptions.onlyMainContent

boolean

default:

true

Only return the main content of the page excluding headers, navs, footers, etc.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-include-tags)

scrapeOptions.includeTags

string\[\]

Tags to include in the output.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-exclude-tags)

scrapeOptions.excludeTags

string\[\]

Tags to exclude from the output.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-headers)

scrapeOptions.headers

object

Headers to send with the request. Can be used to send cookies, user-agent, etc.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-wait-for)

scrapeOptions.waitFor

integer

default:

0

Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-mobile)

scrapeOptions.mobile

boolean

default:

false

Set to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-skip-tls-verification)

scrapeOptions.skipTlsVerification

boolean

default:

false

Skip TLS certificate verification when making requests

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-timeout)

scrapeOptions.timeout

integer

default:

30000

Timeout in milliseconds for the request

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-json-options)

scrapeOptions.jsonOptions

object

Extract object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-json-options-schema)

scrapeOptions.jsonOptions.schema

object

The schema to use for the extraction (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-json-options-system-prompt)

scrapeOptions.jsonOptions.systemPrompt

string

The system prompt to use for the extraction (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-json-options-prompt)

scrapeOptions.jsonOptions.prompt

string

The prompt to use for the extraction without a schema (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-actions)

scrapeOptions.actions

object\[\]

Actions to perform on the page before grabbing the content

- Wait
- Screenshot
- Click
- Write text
- Press a key
- Scroll
- Scrape
- Execute JavaScript

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-actions-type)

scrapeOptions.actions.type

enum<string>

required

Wait for a specified amount of milliseconds

Available options:

`wait`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-actions-milliseconds)

scrapeOptions.actions.milliseconds

integer

Number of milliseconds to wait

Required range: `x >= 1`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-actions-selector)

scrapeOptions.actions.selector

string

Query selector to find the element by

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-location)

scrapeOptions.location

object

Location settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-location-country)

scrapeOptions.location.country

string

default:

US

ISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-location-languages)

scrapeOptions.location.languages

string\[\]

Preferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See [https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-remove-base64-images)

scrapeOptions.removeBase64Images

boolean

Removes all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-block-ads)

scrapeOptions.blockAds

boolean

default:

true

Enables ad-blocking and cookie popup blocking.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#body-scrape-options-proxy)

scrapeOptions.proxy

enum<string>

Specifies the type of proxy to use.

- **basic**: Proxies for scraping sites with none to basic anti-bot solutions. Fast and usually works.
- **stealth**: Stealth proxies for scraping sites with advanced anti-bot solutions. Slower, but more reliable on certain sites.

If you do not specify a proxy, Firecrawl will automatically attempt to determine which one you need based on the target site.

Available options:

`basic`,

`stealth`

#### Response

200

200400500

application/json

Successful extraction

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/extract#response-id)

id

string

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/extract.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/extract)

[Map](https://docs.firecrawl.dev/api-reference/endpoint/map) [Get Extract Status](https://docs.firecrawl.dev/api-reference/endpoint/extract-get)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/extract \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "urls": [\
    "<string>"\
  ],
  "prompt": "<string>",
  "schema": {
    "property1": "<string>",
    "property2": 123
  },
  "enableWebSearch": false,
  "ignoreSitemap": false,
  "includeSubdomains": true,
  "showSources": false,
  "scrapeOptions": {
    "formats": [\
      "markdown"\
    ],
    "onlyMainContent": true,
    "includeTags": [\
      "<string>"\
    ],
    "excludeTags": [\
      "<string>"\
    ],
    "headers": {},
    "waitFor": 0,
    "mobile": false,
    "skipTlsVerification": false,
    "timeout": 30000,
    "jsonOptions": {
      "schema": {},
      "systemPrompt": "<string>",
      "prompt": "<string>"
    },
    "actions": [\
      {\
        "type": "wait",\
        "milliseconds": 2,\
        "selector": "#my-element"\
      }\
    ],
    "location": {
      "country": "US",
      "languages": [\
        "en-US"\
      ]
    },
    "removeBase64Images": true,
    "blockAds": true,
    "proxy": "basic"
  }
}'
```

200

400

500

Copy

```
{
  "success": true,
  "id": "<string>"
}
```

## Web Scraping API
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Scrape Endpoints

Scrape

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

POST

/

scrape

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/scrape \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "formats": [\
    "markdown"\
  ],
  "onlyMainContent": true,
  "includeTags": [\
    "<string>"\
  ],
  "excludeTags": [\
    "<string>"\
  ],
  "headers": {},
  "waitFor": 0,
  "mobile": false,
  "skipTlsVerification": false,
  "timeout": 30000,
  "jsonOptions": {
    "schema": {},
    "systemPrompt": "<string>",
    "prompt": "<string>"
  },
  "actions": [\
    {\
      "type": "wait",\
      "milliseconds": 2,\
      "selector": "#my-element"\
    }\
  ],
  "location": {
    "country": "US",
    "languages": [\
      "en-US"\
    ]
  },
  "removeBase64Images": true,
  "blockAds": true,
  "proxy": "basic"
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "data": {
    "markdown": "<string>",
    "html": "<string>",
    "rawHtml": "<string>",
    "screenshot": "<string>",
    "links": [\
      "<string>"\
    ],
    "actions": {
      "screenshots": [\
        "<string>"\
      ]
    },
    "metadata": {
      "title": "<string>",
      "description": "<string>",
      "language": "<string>",
      "sourceURL": "<string>",
      "<any other metadata> ": "<string>",
      "statusCode": 123,
      "error": "<string>"
    },
    "llm_extraction": {},
    "warning": "<string>"
  }
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-url)

url

string

required

The URL to scrape

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-formats)

formats

enum<string>\[\]

Formats to include in the output.

Available options:

`markdown`,

`html`,

`rawHtml`,

`links`,

`screenshot`,

`screenshot@fullPage`,

`json`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-only-main-content)

onlyMainContent

boolean

default:

true

Only return the main content of the page excluding headers, navs, footers, etc.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-include-tags)

includeTags

string\[\]

Tags to include in the output.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-exclude-tags)

excludeTags

string\[\]

Tags to exclude from the output.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-headers)

headers

object

Headers to send with the request. Can be used to send cookies, user-agent, etc.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-wait-for)

waitFor

integer

default:

0

Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-mobile)

mobile

boolean

default:

false

Set to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-skip-tls-verification)

skipTlsVerification

boolean

default:

false

Skip TLS certificate verification when making requests

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-timeout)

timeout

integer

default:

30000

Timeout in milliseconds for the request

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options)

jsonOptions

object

Extract object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options-schema)

jsonOptions.schema

object

The schema to use for the extraction (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options-system-prompt)

jsonOptions.systemPrompt

string

The system prompt to use for the extraction (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-json-options-prompt)

jsonOptions.prompt

string

The prompt to use for the extraction without a schema (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions)

actions

object\[\]

Actions to perform on the page before grabbing the content

- Wait
- Screenshot
- Click
- Write text
- Press a key
- Scroll
- Scrape
- Execute JavaScript

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions-type)

actions.type

enum<string>

required

Wait for a specified amount of milliseconds

Available options:

`wait`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions-milliseconds)

actions.milliseconds

integer

Number of milliseconds to wait

Required range: `x >= 1`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-actions-selector)

actions.selector

string

Query selector to find the element by

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-location)

location

object

Location settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-location-country)

location.country

string

default:

US

ISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-location-languages)

location.languages

string\[\]

Preferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See [https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-remove-base64-images)

removeBase64Images

boolean

Removes all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-block-ads)

blockAds

boolean

default:

true

Enables ad-blocking and cookie popup blocking.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#body-proxy)

proxy

enum<string>

Specifies the type of proxy to use.

- **basic**: Proxies for scraping sites with none to basic anti-bot solutions. Fast and usually works.
- **stealth**: Stealth proxies for scraping sites with advanced anti-bot solutions. Slower, but more reliable on certain sites.

If you do not specify a proxy, Firecrawl will automatically attempt to determine which one you need based on the target site.

Available options:

`basic`,

`stealth`

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data)

data

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-markdown)

data.markdown

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-html)

data.html

string \| null

HTML version of the content on page if `html` is in `formats`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-raw-html)

data.rawHtml

string \| null

Raw HTML content of the page if `rawHtml` is in `formats`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-screenshot)

data.screenshot

string \| null

Screenshot of the page if `screenshot` is in `formats`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-links)

data.links

string\[\]

List of links on the page if `links` is in `formats`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-actions)

data.actions

object \| null

Results of the actions specified in the `actions` parameter. Only present if the `actions` parameter was provided in the request

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-actions-screenshots)

data.actions.screenshots

string\[\]

Screenshot URLs, in the same order as the screenshot actions provided.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata)

data.metadata

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-title)

data.metadata.title

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-description)

data.metadata.description

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-language)

data.metadata.language

string \| null

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-source-url)

data.metadata.sourceURL

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-any-other-metadata)

data.metadata.<any other metadata>

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-status-code)

data.metadata.statusCode

integer

The status code of the page

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-metadata-error)

data.metadata.error

string \| null

The error message of the page

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-llm-extraction)

data.llm\_extraction

object \| null

Displayed when using LLM Extraction. Extracted data from the page following the schema defined.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/scrape#response-data-warning)

data.warning

string \| null

Can be displayed when using LLM Extraction. Warning message will let you know any issues with the extraction.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/scrape)

[Introduction](https://docs.firecrawl.dev/api-reference/introduction) [Batch Scrape](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/scrape \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "formats": [\
    "markdown"\
  ],
  "onlyMainContent": true,
  "includeTags": [\
    "<string>"\
  ],
  "excludeTags": [\
    "<string>"\
  ],
  "headers": {},
  "waitFor": 0,
  "mobile": false,
  "skipTlsVerification": false,
  "timeout": 30000,
  "jsonOptions": {
    "schema": {},
    "systemPrompt": "<string>",
    "prompt": "<string>"
  },
  "actions": [\
    {\
      "type": "wait",\
      "milliseconds": 2,\
      "selector": "#my-element"\
    }\
  ],
  "location": {
    "country": "US",
    "languages": [\
      "en-US"\
    ]
  },
  "removeBase64Images": true,
  "blockAds": true,
  "proxy": "basic"
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "data": {
    "markdown": "<string>",
    "html": "<string>",
    "rawHtml": "<string>",
    "screenshot": "<string>",
    "links": [\
      "<string>"\
    ],
    "actions": {
      "screenshots": [\
        "<string>"\
      ]
    },
    "metadata": {
      "title": "<string>",
      "description": "<string>",
      "language": "<string>",
      "sourceURL": "<string>",
      "<any other metadata> ": "<string>",
      "statusCode": 123,
      "error": "<string>"
    },
    "llm_extraction": {},
    "warning": "<string>"
  }
}
```

## Map Endpoint
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Map Endpoints

Map

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

POST

/

map

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/map \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "search": "<string>",
  "ignoreSitemap": true,
  "sitemapOnly": false,
  "includeSubdomains": false,
  "limit": 5000,
  "timeout": 123
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "links": [\
    "<string>"\
  ]
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-url)

url

string

required

The base URL to start crawling from

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-search)

search

string

Search query to use for mapping. During the Alpha phase, the 'smart' part of the search functionality is limited to 1000 search results. However, if map finds more results, there is no limit applied.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-ignore-sitemap)

ignoreSitemap

boolean

default:

true

Ignore the website sitemap when crawling.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-sitemap-only)

sitemapOnly

boolean

default:

false

Only return links found in the website sitemap

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-include-subdomains)

includeSubdomains

boolean

default:

false

Include subdomains of the website

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-limit)

limit

integer

default:

5000

Maximum number of links to return

Required range: `x <= 5000`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#body-timeout)

timeout

integer

Timeout in milliseconds. There is no timeout by default.

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/map#response-links)

links

string\[\]

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/map.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/map)

[Get Crawl Errors](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get-errors) [Extract](https://docs.firecrawl.dev/api-reference/endpoint/extract)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/map \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "search": "<string>",
  "ignoreSitemap": true,
  "sitemapOnly": false,
  "includeSubdomains": false,
  "limit": 5000,
  "timeout": 123
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "links": [\
    "<string>"\
  ]
}
```

## Firecrawl API Introduction
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Using the API

Introduction

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

## [‚Äã](https://docs.firecrawl.dev/v0/api-reference/introduction\#base-url)  Base URL

All requests contain the following base URL:

Copy

```bash
https://api.firecrawl.dev

```

## [‚Äã](https://docs.firecrawl.dev/v0/api-reference/introduction\#authentication)  Authentication

For authentication, it‚Äôs required to include an Authorization header. The header should contain `Bearer fc_123456789`, where `fc_123456789` represents your API Key.

Copy

```bash
Authorization: Bearer fc_123456789

```

‚Äã

## [‚Äã](https://docs.firecrawl.dev/v0/api-reference/introduction\#response-codes)  Response codes

Firecrawl employs conventional HTTP status codes to signify the outcome of your requests.

Typically, 2xx HTTP status codes denote success, 4xx codes represent failures related to the user, and 5xx codes signal infrastructure problems.

| Status | Description |
| --- | --- |
| 200 | Request was successful. |
| 400 | Verify the correctness of the parameters. |
| 401 | The API key was not provided. |
| 402 | Payment required |
| 404 | The requested resource could not be located. |
| 429 | The rate limit has been surpassed. |
| 5xx | Signifies a server error with Firecrawl. |

Refer to the Error Codes section for a detailed explanation of all potential API errors.

‚Äã

## [‚Äã](https://docs.firecrawl.dev/v0/api-reference/introduction\#rate-limit)  Rate limit

The Firecrawl API has a rate limit to ensure the stability and reliability of the service. The rate limit is applied to all endpoints and is based on the number of requests made within a specific time frame.

When you exceed the rate limit, you will receive a 429 response code.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/introduction.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/introduction)

[Scrape](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape)

On this page

- [Base URL](https://docs.firecrawl.dev/v0/api-reference/introduction#base-url)
- [Authentication](https://docs.firecrawl.dev/v0/api-reference/introduction#authentication)
- [Response codes](https://docs.firecrawl.dev/v0/api-reference/introduction#response-codes)
- [Rate limit](https://docs.firecrawl.dev/v0/api-reference/introduction#rate-limit)

## Crawl API Endpoint
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Endpoints

Crawl

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

POST

/

crawl

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v0/crawl \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "crawlerOptions": {
    "includes": [\
      "<string>"\
    ],
    "excludes": [\
      "<string>"\
    ],
    "generateImgAltText": false,
    "returnOnlyUrls": false,
    "maxDepth": 123,
    "mode": "default",
    "ignoreSitemap": false,
    "limit": 10000,
    "allowBackwardCrawling": false,
    "allowExternalContentLinks": false
  },
  "pageOptions": {
    "headers": {},
    "includeHtml": false,
    "includeRawHtml": false,
    "onlyIncludeTags": [\
      "<string>"\
    ],
    "onlyMainContent": false,
    "removeTags": [\
      "<string>"\
    ],
    "replaceAllPathsWithAbsolutePaths": false,
    "screenshot": false,
    "fullPageScreenshot": false,
    "waitFor": 0
  }
}'
```

200

402

429

500

Copy

```
{
  "jobId": "<string>"
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-url)

url

string

required

The base URL to start crawling from

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options)

crawlerOptions

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-includes)

crawlerOptions.includes

string\[\]

URL patterns to include

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-excludes)

crawlerOptions.excludes

string\[\]

URL patterns to exclude

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-generate-img-alt-text)

crawlerOptions.generateImgAltText

boolean

default:

false

Generate alt text for images using LLMs (must have a paid plan)

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-return-only-urls)

crawlerOptions.returnOnlyUrls

boolean

default:

false

If true, returns only the URLs as a list on the crawl status. Attention: the return response will be a list of URLs inside the data, not a list of documents.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-max-depth)

crawlerOptions.maxDepth

integer

Maximum depth to crawl relative to the entered URL. A maxDepth of 0 scrapes only the entered URL. A maxDepth of 1 scrapes the entered URL and all pages one level deep. A maxDepth of 2 scrapes the entered URL and all pages up to two levels deep. Higher values follow the same pattern.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-mode)

crawlerOptions.mode

enum<string>

default:

default

The crawling mode to use. Fast mode crawls 4x faster websites without sitemap, but may not be as accurate and shouldn't be used in heavy js-rendered websites.

Available options:

`default`,

`fast`

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-ignore-sitemap)

crawlerOptions.ignoreSitemap

boolean

default:

false

Ignore the website sitemap when crawling

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-limit)

crawlerOptions.limit

integer

default:

10000

Maximum number of pages to crawl

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-allow-backward-crawling)

crawlerOptions.allowBackwardCrawling

boolean

default:

false

Enables the crawler to navigate from a specific URL to previously linked pages. For instance, from 'example.com/product/123' back to 'example.com/product'

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-crawler-options-allow-external-content-links)

crawlerOptions.allowExternalContentLinks

boolean

default:

false

Allows the crawler to follow links to external websites.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options)

pageOptions

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-headers)

pageOptions.headers

object

Headers to send with the request. Can be used to send cookies, user-agent, etc.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-include-html)

pageOptions.includeHtml

boolean

default:

false

Include the HTML version of the content on page. Will output a html key in the response.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-include-raw-html)

pageOptions.includeRawHtml

boolean

default:

false

Include the raw HTML content of the page. Will output a rawHtml key in the response.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-only-include-tags)

pageOptions.onlyIncludeTags

string\[\]

Only include tags, classes and ids from the page in the final output. Use comma separated values. Example: 'script, .ad, #footer'

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-only-main-content)

pageOptions.onlyMainContent

boolean

default:

false

Only return the main content of the page excluding headers, navs, footers, etc.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-remove-tags)

pageOptions.removeTags

string\[\]

Tags, classes and ids to remove from the page. Use comma separated values. Example: 'script, .ad, #footer'

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-replace-all-paths-with-absolute-paths)

pageOptions.replaceAllPathsWithAbsolutePaths

boolean

default:

false

Replace all relative paths with absolute paths for images and links

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-screenshot)

pageOptions.screenshot

boolean

default:

false

Include a screenshot of the top of the page that you are scraping.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-full-page-screenshot)

pageOptions.fullPageScreenshot

boolean

default:

false

Include a full page screenshot of the page that you are scraping.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#body-page-options-wait-for)

pageOptions.waitFor

integer

default:

0

Wait x amount of milliseconds for the page to load to fetch content

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl#response-job-id)

jobId

string

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/crawl.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/crawl)

[Search (Beta)](https://docs.firecrawl.dev/v0/api-reference/endpoint/search) [Get Crawl Status](https://docs.firecrawl.dev/v0/api-reference/endpoint/status)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v0/crawl \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "crawlerOptions": {
    "includes": [\
      "<string>"\
    ],
    "excludes": [\
      "<string>"\
    ],
    "generateImgAltText": false,
    "returnOnlyUrls": false,
    "maxDepth": 123,
    "mode": "default",
    "ignoreSitemap": false,
    "limit": 10000,
    "allowBackwardCrawling": false,
    "allowExternalContentLinks": false
  },
  "pageOptions": {
    "headers": {},
    "includeHtml": false,
    "includeRawHtml": false,
    "onlyIncludeTags": [\
      "<string>"\
    ],
    "onlyMainContent": false,
    "removeTags": [\
      "<string>"\
    ],
    "replaceAllPathsWithAbsolutePaths": false,
    "screenshot": false,
    "fullPageScreenshot": false,
    "waitFor": 0
  }
}'
```

200

402

429

500

Copy

```
{
  "jobId": "<string>"
}
```

## Web Scraping API
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Endpoints

Scrape

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

POST

/

scrape

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v0/scrape \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "pageOptions": {
    "headers": {},
    "includeHtml": false,
    "includeRawHtml": false,
    "onlyIncludeTags": [\
      "<string>"\
    ],
    "onlyMainContent": false,
    "removeTags": [\
      "<string>"\
    ],
    "replaceAllPathsWithAbsolutePaths": false,
    "screenshot": false,
    "fullPageScreenshot": false,
    "waitFor": 0
  },
  "extractorOptions": {},
  "timeout": 30000
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "data": {
    "markdown": "<string>",
    "content": "<string>",
    "html": "<string>",
    "rawHtml": "<string>",
    "metadata": {
      "title": "<string>",
      "description": "<string>",
      "language": "<string>",
      "sourceURL": "<string>",
      "<any other metadata> ": "<string>",
      "pageStatusCode": 123,
      "pageError": "<string>"
    },
    "llm_extraction": {},
    "warning": "<string>"
  }
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-url)

url

string

required

The URL to scrape

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options)

pageOptions

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-headers)

pageOptions.headers

object

Headers to send with the request. Can be used to send cookies, user-agent, etc.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-include-html)

pageOptions.includeHtml

boolean

default:

false

Include the HTML version of the content on page. Will output a html key in the response.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-include-raw-html)

pageOptions.includeRawHtml

boolean

default:

false

Include the raw HTML content of the page. Will output a rawHtml key in the response.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-only-include-tags)

pageOptions.onlyIncludeTags

string\[\]

Only include tags, classes and ids from the page in the final output. Use comma separated values. Example: 'script, .ad, #footer'

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-only-main-content)

pageOptions.onlyMainContent

boolean

default:

false

Only return the main content of the page excluding headers, navs, footers, etc.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-remove-tags)

pageOptions.removeTags

string\[\]

Tags, classes and ids to remove from the page. Use comma separated values. Example: 'script, .ad, #footer'

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-replace-all-paths-with-absolute-paths)

pageOptions.replaceAllPathsWithAbsolutePaths

boolean

default:

false

Replace all relative paths with absolute paths for images and links

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-screenshot)

pageOptions.screenshot

boolean

default:

false

Include a screenshot of the top of the page that you are scraping.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-full-page-screenshot)

pageOptions.fullPageScreenshot

boolean

default:

false

Include a full page screenshot of the page that you are scraping.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-page-options-wait-for)

pageOptions.waitFor

integer

default:

0

Wait x amount of milliseconds for the page to load to fetch content

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options)

extractorOptions

object

Options for extraction of structured information from the page content. Note: LLM-based extraction is not performed by default and only occurs when explicitly configured. The 'markdown' mode simply returns the scraped markdown and is the default mode for scraping.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options-mode)

extractorOptions.mode

enum<string>

The extraction mode to use. 'markdown': Returns the scraped markdown content, does not perform LLM extraction. 'llm-extraction': Extracts information from the cleaned and parsed content using LLM. 'llm-extraction-from-raw-html': Extracts information directly from the raw HTML using LLM. 'llm-extraction-from-markdown': Extracts information from the markdown content using LLM.

Available options:

`markdown`,

`llm-extraction`,

`llm-extraction-from-raw-html`,

`llm-extraction-from-markdown`

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options-extraction-prompt)

extractorOptions.extractionPrompt

string

A prompt describing what information to extract from the page, applicable for LLM extraction modes.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-extractor-options-extraction-schema)

extractorOptions.extractionSchema

object

The schema for the data to be extracted, required only for LLM extraction modes.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#body-timeout)

timeout

integer

default:

30000

Timeout in milliseconds for the request

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data)

data

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-markdown)

data.markdown

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-content)

data.content

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-html)

data.html

string \| null

HTML version of the content on page if `includeHtml` is true

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-raw-html)

data.rawHtml

string \| null

Raw HTML content of the page if `includeRawHtml` is true

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata)

data.metadata

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-title)

data.metadata.title

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-description)

data.metadata.description

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-language)

data.metadata.language

string \| null

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-source-url)

data.metadata.sourceURL

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-any-other-metadata)

data.metadata.<any other metadata>

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-page-status-code)

data.metadata.pageStatusCode

integer

The status code of the page

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-metadata-page-error)

data.metadata.pageError

string \| null

The error message of the page

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-llm-extraction)

data.llm\_extraction

object \| null

Displayed when using LLM Extraction. Extracted data from the page following the schema defined.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape#response-data-warning)

data.warning

string \| null

Can be displayed when using LLM Extraction. Warning message will let you know any issues with the extraction.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/scrape)

[Introduction](https://docs.firecrawl.dev/v0/api-reference/introduction) [Search (Beta)](https://docs.firecrawl.dev/v0/api-reference/endpoint/search)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v0/scrape \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "url": "<string>",
  "pageOptions": {
    "headers": {},
    "includeHtml": false,
    "includeRawHtml": false,
    "onlyIncludeTags": [\
      "<string>"\
    ],
    "onlyMainContent": false,
    "removeTags": [\
      "<string>"\
    ],
    "replaceAllPathsWithAbsolutePaths": false,
    "screenshot": false,
    "fullPageScreenshot": false,
    "waitFor": 0
  },
  "extractorOptions": {},
  "timeout": 30000
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "data": {
    "markdown": "<string>",
    "content": "<string>",
    "html": "<string>",
    "rawHtml": "<string>",
    "metadata": {
      "title": "<string>",
      "description": "<string>",
      "language": "<string>",
      "sourceURL": "<string>",
      "<any other metadata> ": "<string>",
      "pageStatusCode": 123,
      "pageError": "<string>"
    },
    "llm_extraction": {},
    "warning": "<string>"
  }
}
```

## Crawl Job Status
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Crawl Endpoints

Get Crawl Status

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

GET

/

crawl

/

{id}

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request GET \
  --url https://api.firecrawl.dev/v1/crawl/{id} \
  --header 'Authorization: Bearer <token>'
```

200

402

429

500

Copy

```
{
  "status": "<string>",
  "total": 123,
  "completed": 123,
  "creditsUsed": 123,
  "expiresAt": "2023-11-07T05:31:56Z",
  "next": "<string>",
  "data": [\
    {\
      "markdown": "<string>",\
      "html": "<string>",\
      "rawHtml": "<string>",\
      "links": [\
        "<string>"\
      ],\
      "screenshot": "<string>",\
      "metadata": {\
        "title": "<string>",\
        "description": "<string>",\
        "language": "<string>",\
        "sourceURL": "<string>",\
        "<any other metadata> ": "<string>",\
        "statusCode": 123,\
        "error": "<string>"\
      }\
    }\
  ]
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Path Parameters

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#parameter-id)

id

string

required

The ID of the crawl job

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-status)

status

string

The current status of the crawl. Can be `scraping`, `completed`, or `failed`.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-total)

total

integer

The total number of pages that were attempted to be crawled.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-completed)

completed

integer

The number of pages that have been successfully crawled.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-credits-used)

creditsUsed

integer

The number of credits used for the crawl.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-expires-at)

expiresAt

string

The date and time when the crawl will expire.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-next)

next

string \| null

The URL to retrieve the next 10MB of data. Returned if the crawl is not completed or if the response is larger than 10MB.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data)

data

object\[\]

The data of the crawl.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-markdown)

data.markdown

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-html)

data.html

string \| null

HTML version of the content on page if `includeHtml` is true

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-raw-html)

data.rawHtml

string \| null

Raw HTML content of the page if `includeRawHtml` is true

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-links)

data.links

string\[\]

List of links on the page if `includeLinks` is true

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-screenshot)

data.screenshot

string \| null

Screenshot of the page if `includeScreenshot` is true

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata)

data.metadata

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-title)

data.metadata.title

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-description)

data.metadata.description

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-language)

data.metadata.language

string \| null

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-source-url)

data.metadata.sourceURL

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-any-other-metadata)

data.metadata.<any other metadata>

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-status-code)

data.metadata.statusCode

integer

The status code of the page

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/crawl-get#response-data-metadata-error)

data.metadata.error

string \| null

The error message of the page

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/crawl-get.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/crawl-get)

[Crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl-post) [Cancel Crawl](https://docs.firecrawl.dev/api-reference/endpoint/crawl-delete)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request GET \
  --url https://api.firecrawl.dev/v1/crawl/{id} \
  --header 'Authorization: Bearer <token>'
```

200

402

429

500

Copy

```
{
  "status": "<string>",
  "total": 123,
  "completed": 123,
  "creditsUsed": 123,
  "expiresAt": "2023-11-07T05:31:56Z",
  "next": "<string>",
  "data": [\
    {\
      "markdown": "<string>",\
      "html": "<string>",\
      "rawHtml": "<string>",\
      "links": [\
        "<string>"\
      ],\
      "screenshot": "<string>",\
      "metadata": {\
        "title": "<string>",\
        "description": "<string>",\
        "language": "<string>",\
        "sourceURL": "<string>",\
        "<any other metadata> ": "<string>",\
        "statusCode": 123,\
        "error": "<string>"\
      }\
    }\
  ]
}
```

## Search API
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v0

Search or ask...

Ctrl K

Search...

Navigation

Endpoints

Search (Beta)

[Documentation](https://docs.firecrawl.dev/v0/introduction) [SDKs](https://docs.firecrawl.dev/v0/sdks/python) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/v0/api-reference/introduction)

POST

/

search

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v0/search \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "query": "<string>",
  "pageOptions": {
    "onlyMainContent": false,
    "fetchPageContent": true,
    "includeHtml": false,
    "includeRawHtml": false
  },
  "searchOptions": {
    "limit": 123
  }
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "data": [\
    {\
      "url": "<string>",\
      "markdown": "<string>",\
      "content": "<string>",\
      "metadata": {\
        "title": "<string>",\
        "description": "<string>",\
        "language": "<string>",\
        "sourceURL": "<string>"\
      }\
    }\
  ]
}
```

The search endpoint combines a search API with the power of Firecrawl to provide a powerful search experience for whatever query.

It automatically searches the web for the query and returns the most relevant results from the top pages in markdown format. The advantage of this endpoint is that it actually scrap each website on the top result so you always get the full content.

This endpoint is currently in beta and is subject to change.

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-query)

query

string

required

The query to search for

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options)

pageOptions

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-only-main-content)

pageOptions.onlyMainContent

boolean

default:

false

Only return the main content of the page excluding headers, navs, footers, etc.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-fetch-page-content)

pageOptions.fetchPageContent

boolean

default:

true

Fetch the content of each page. If false, defaults to a basic fast serp API.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-include-html)

pageOptions.includeHtml

boolean

default:

false

Include the HTML version of the content on page. Will output a html key in the response.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-page-options-include-raw-html)

pageOptions.includeRawHtml

boolean

default:

false

Include the raw HTML content of the page. Will output a rawHtml key in the response.

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-search-options)

searchOptions

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#body-search-options-limit)

searchOptions.limit

integer

Maximum number of results. Max is 20 during beta.

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data)

data

object\[\]

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-url)

data.url

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-markdown)

data.markdown

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-content)

data.content

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata)

data.metadata

object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-title)

data.metadata.title

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-description)

data.metadata.description

string

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-language)

data.metadata.language

string \| null

[‚Äã](https://docs.firecrawl.dev/v0/api-reference/endpoint/search#response-data-metadata-source-url)

data.metadata.sourceURL

string

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/v0/api-reference/endpoint/search.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/v0/api-reference/endpoint/search)

[Scrape](https://docs.firecrawl.dev/v0/api-reference/endpoint/scrape) [Crawl](https://docs.firecrawl.dev/v0/api-reference/endpoint/crawl)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v0/search \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "query": "<string>",
  "pageOptions": {
    "onlyMainContent": false,
    "fetchPageContent": true,
    "includeHtml": false,
    "includeRawHtml": false
  },
  "searchOptions": {
    "limit": 123
  }
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "data": [\
    {\
      "url": "<string>",\
      "markdown": "<string>",\
      "content": "<string>",\
      "metadata": {\
        "title": "<string>",\
        "description": "<string>",\
        "language": "<string>",\
        "sourceURL": "<string>"\
      }\
    }\
  ]
}
```

## Batch Scrape API
[Firecrawl Docs home page![light logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/light.svg)![dark logo](https://mintlify.s3.us-west-1.amazonaws.com/firecrawl/logo/dark.svg)](https://firecrawl.dev/)

v1

Search or ask...

Ctrl K

Search...

Navigation

Scrape Endpoints

Batch Scrape

[Documentation](https://docs.firecrawl.dev/introduction) [SDKs](https://docs.firecrawl.dev/sdks/overview) [Learn](https://www.firecrawl.dev/blog/category/tutorials) [API Reference](https://docs.firecrawl.dev/api-reference/introduction)

POST

/

batch

/

scrape

Try it

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/batch/scrape \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "urls": [\
    "<string>"\
  ],
  "webhook": {
    "url": "<string>",
    "headers": {},
    "metadata": {},
    "events": [\
      "completed"\
    ]
  },
  "ignoreInvalidURLs": false,
  "formats": [\
    "markdown"\
  ],
  "onlyMainContent": true,
  "includeTags": [\
    "<string>"\
  ],
  "excludeTags": [\
    "<string>"\
  ],
  "headers": {},
  "waitFor": 0,
  "mobile": false,
  "skipTlsVerification": false,
  "timeout": 30000,
  "jsonOptions": {
    "schema": {},
    "systemPrompt": "<string>",
    "prompt": "<string>"
  },
  "actions": [\
    {\
      "type": "wait",\
      "milliseconds": 2,\
      "selector": "#my-element"\
    }\
  ],
  "location": {
    "country": "US",
    "languages": [\
      "en-US"\
    ]
  },
  "removeBase64Images": true,
  "blockAds": true,
  "proxy": "basic"
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "id": "<string>",
  "url": "<string>",
  "invalidURLs": [\
    "<string>"\
  ]
}
```

#### Authorizations

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#authorization-authorization)

Authorization

string

header

required

Bearer authentication header of the form `Bearer <token>`, where `<token>` is your auth token.

#### Body

application/json

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-urls)

urls

string\[\]

required

The URL to scrape

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook)

webhook

object

A webhook specification object.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook-url)

webhook.url

string

required

The URL to send the webhook to. This will trigger for batch scrape started (batch\_scrape.started), every page scraped (batch\_scrape.page) and when the batch scrape is completed (batch\_scrape.completed or batch\_scrape.failed). The response will be the same as the `/scrape` endpoint.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook-headers)

webhook.headers

object

Headers to send to the webhook URL.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook-headers-key)

webhook.headers.{key}

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook-metadata)

webhook.metadata

object

Custom metadata that will be included in all webhook payloads for this crawl

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-webhook-events)

webhook.events

enum<string>\[\]

Type of events that should be sent to the webhook URL. (default: all)

Available options:

`completed`,

`page`,

`failed`,

`started`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-ignore-invalid-urls)

ignoreInvalidURLs

boolean

default:

false

If invalid URLs are specified in the urls array, they will be ignored. Instead of them failing the entire request, a batch scrape using the remaining valid URLs will be created, and the invalid URLs will be returned in the invalidURLs field of the response.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-formats)

formats

enum<string>\[\]

Formats to include in the output.

Available options:

`markdown`,

`html`,

`rawHtml`,

`links`,

`screenshot`,

`screenshot@fullPage`,

`json`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-only-main-content)

onlyMainContent

boolean

default:

true

Only return the main content of the page excluding headers, navs, footers, etc.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-include-tags)

includeTags

string\[\]

Tags to include in the output.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-exclude-tags)

excludeTags

string\[\]

Tags to exclude from the output.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-headers)

headers

object

Headers to send with the request. Can be used to send cookies, user-agent, etc.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-wait-for)

waitFor

integer

default:

0

Specify a delay in milliseconds before fetching the content, allowing the page sufficient time to load.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-mobile)

mobile

boolean

default:

false

Set to true if you want to emulate scraping from a mobile device. Useful for testing responsive pages and taking mobile screenshots.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-skip-tls-verification)

skipTlsVerification

boolean

default:

false

Skip TLS certificate verification when making requests

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-timeout)

timeout

integer

default:

30000

Timeout in milliseconds for the request

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options)

jsonOptions

object

Extract object

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options-schema)

jsonOptions.schema

object

The schema to use for the extraction (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options-system-prompt)

jsonOptions.systemPrompt

string

The system prompt to use for the extraction (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-json-options-prompt)

jsonOptions.prompt

string

The prompt to use for the extraction without a schema (Optional)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions)

actions

object\[\]

Actions to perform on the page before grabbing the content

- Wait
- Screenshot
- Click
- Write text
- Press a key
- Scroll
- Scrape
- Execute JavaScript

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions-type)

actions.type

enum<string>

required

Wait for a specified amount of milliseconds

Available options:

`wait`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions-milliseconds)

actions.milliseconds

integer

Number of milliseconds to wait

Required range: `x >= 1`

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-actions-selector)

actions.selector

string

Query selector to find the element by

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-location)

location

object

Location settings for the request. When specified, this will use an appropriate proxy if available and emulate the corresponding language and timezone settings. Defaults to 'US' if not specified.

Showchild attributes

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-location-country)

location.country

string

default:

US

ISO 3166-1 alpha-2 country code (e.g., 'US', 'AU', 'DE', 'JP')

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-location-languages)

location.languages

string\[\]

Preferred languages and locales for the request in order of priority. Defaults to the language of the specified location. See [https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Accept-Language)

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-remove-base64-images)

removeBase64Images

boolean

Removes all base 64 images from the output, which may be overwhelmingly long. The image's alt text remains in the output, but the URL is replaced with a placeholder.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-block-ads)

blockAds

boolean

default:

true

Enables ad-blocking and cookie popup blocking.

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#body-proxy)

proxy

enum<string>

Specifies the type of proxy to use.

- **basic**: Proxies for scraping sites with none to basic anti-bot solutions. Fast and usually works.
- **stealth**: Stealth proxies for scraping sites with advanced anti-bot solutions. Slower, but more reliable on certain sites.

If you do not specify a proxy, Firecrawl will automatically attempt to determine which one you need based on the target site.

Available options:

`basic`,

`stealth`

#### Response

200

200402429500

application/json

Successful response

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-success)

success

boolean

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-id)

id

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-url)

url

string

[‚Äã](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape#response-invalid-urls)

invalidURLs

string\[\] \| null

If ignoreInvalidURLs is true, this is an array containing the invalid URLs that were specified in the request. If there were no invalid URLs, this will be an empty array. If ignoreInvalidURLs is false, this field will be undefined.

[Suggest edits](https://github.com/hellofirecrawl/docs/edit/main/api-reference/endpoint/batch-scrape.mdx) [Raise issue](https://github.com/hellofirecrawl/docs/issues/new?title=Issue%20on%20docs&body=Path:%20/api-reference/endpoint/batch-scrape)

[Scrape](https://docs.firecrawl.dev/api-reference/endpoint/scrape) [Get Batch Scrape Status](https://docs.firecrawl.dev/api-reference/endpoint/batch-scrape-get)

cURL

Python

JavaScript

PHP

Go

Java

Copy

```
curl --request POST \
  --url https://api.firecrawl.dev/v1/batch/scrape \
  --header 'Authorization: Bearer <token>' \
  --header 'Content-Type: application/json' \
  --data '{
  "urls": [\
    "<string>"\
  ],
  "webhook": {
    "url": "<string>",
    "headers": {},
    "metadata": {},
    "events": [\
      "completed"\
    ]
  },
  "ignoreInvalidURLs": false,
  "formats": [\
    "markdown"\
  ],
  "onlyMainContent": true,
  "includeTags": [\
    "<string>"\
  ],
  "excludeTags": [\
    "<string>"\
  ],
  "headers": {},
  "waitFor": 0,
  "mobile": false,
  "skipTlsVerification": false,
  "timeout": 30000,
  "jsonOptions": {
    "schema": {},
    "systemPrompt": "<string>",
    "prompt": "<string>"
  },
  "actions": [\
    {\
      "type": "wait",\
      "milliseconds": 2,\
      "selector": "#my-element"\
    }\
  ],
  "location": {
    "country": "US",
    "languages": [\
      "en-US"\
    ]
  },
  "removeBase64Images": true,
  "blockAds": true,
  "proxy": "basic"
}'
```

200

402

429

500

Copy

```
{
  "success": true,
  "id": "<string>",
  "url": "<string>",
  "invalidURLs": [\
    "<string>"\
  ]
}
```

